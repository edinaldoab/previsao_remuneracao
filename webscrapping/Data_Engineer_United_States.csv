id,company,job title,location,job description,salary estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
0,Wevision LLC,Data Engineer,"Irvine, CA","Job description
We build services, data platform and machine learning based optimization engines for every aspect of advertising, including targeting, decisioning, pricing, personalization, inventory forecasting, attribution and full-funnel measurements. Our tenant is a strong tech team to deliver E2E solutions covering tech areas ranging from research, bigdata, microservices to data applications with front-end tools. Our team is seeking a software developer who will be an outstanding addition and will be responsible for development of high-available and high-concurrent backend services or data solutions. The right person for this role should have experience on either full stack components, or microservices or bigdata platforms. If you are someone who is proactive, hardworking, and enthusiastic in either these domains, this is a phenomenal role for you!
WHAT YOU’LL DO
Build components of large scale data platform for online streaming data and offline batch data from ETL pipelines, data processing, operational data store and AI feature stores.
Continuously improve performance, scalability and availability for microservices of advertising targeting, decisioning and ranking.
Own features of bigdata applications to fit evolving business with realtime metrics, measurable insights and industry leading user experience.
Drive adoption of the best engineering practices, including the use of design patterns, CI/CD, code review and automated integration testing.
Chip in disruptive innovation and apply new ground breaking technologies
As a key member of the team, contribute to all aspects of the software lifecycle: design, experimentation, implementation and testing.
Collaborate with program managers, product managers, and researchers in an open and innovative environment.
WHAT TO BRING
BS in computer science or engineering.
3+ years of professional programming and design experience in Scala, Java, Python, and etc.
2+ years of experience with Hadoop ecosystem e.g. HBase, Hive, Spark/Flink, Impala, Presto, Click House, Druid and etc.
Knowledge of system, application design and architecture
Passion for technology, open to interdisciplinary work
NICE-TO-HAVES
Experience with processing large amount of data at petabyte level.
Experience in digital video advertising or digital marketing domain.
Experience with CRM, DMP, user portrait and audience insights.
Experience with Airflow, Kafka, MemSQL, Docker, AWS, Terraform, Spinnaker, K8S, and etc.
Experience in at least one widely used Web framework (React.js, Vue.js, Angular, etc.) and good knowledge of Web stack HTML, CSS, Webpack.
Job Types: Full-time, Contract
Pay: $55.00 - $85.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Education:
Bachelor's (Required)
Experience:
Python: 1 year (Required)
AWS: 1 year (Preferred)
Scala: 1 year (Required)
Work Location: Hybrid remote in Irvine, CA 92602
Show Less
Report",$55.00 - $85.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
1,"YT Global Network
5.0",Data Engineer- Remote,Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$90.00 - $120.00 Per hour(Employer Est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
2,"Kanini Software Solutions
4.4",Data Engineer,Remote,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Data bricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $65.00 - $70.00 per hour
Experience level:
10 years
Work Location: Remote
Show Less
Report",$65.00 - $70.00 Per hour(Employer Est.),501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2003,$5 to $25 million (USD)
3,"Gridiron IT
4.4",Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
4,"Radiant Systems Inc
4.0",Azure Data Engineer,"Milwaukee, WI","Azure Synapse (no data bricks please)
Some Data Modeling would be helpful
ADF workflow experience
GIT
Python
Spark
MS or Azure SQL
Azure DevOps experience helpful
Datawarehouse and ETL
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: Remote
Show Less
Report",$87T - $1L (Glassdoor Est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1995,$25 to $50 million (USD)
5,"kairos technologies
4.5",Data Engineer,"Boston, MA","Job Title: Data Engineer with AWS – Hybrid
Location: Boston, MA
Duration: 6-12 months
Detail Job Description:
Familiarity with data lake, data warehouse or data lake environments and related topics . Has a proven track record to work with vendors to deploy external SaaS solutions and integrate with existing systems.
In depth with data lakes/ data environments including ETL (PySpark), data Catalogs (Glue, Alation), API interfaces, Cloud data warehouses such as Redshift, Querying engines such as Trino.
Agile approaches to building cloud native solutions using CI/CD, containers, Kubernetes, GitOPS, etc..
Strong automation and development skills in terraform, CloudFormation, and other languages like Python, and bash.
8+ years of total IT experience, with at least 4 years in AWS services such as EMR, EC2, S3, IAM, Glue andRedshift and 2+ years of experience in Infrastructure as code technologies like Terraform, CloudFormation.
4+ years of Python, SQL experience is mandatory.
Job Type: Contract
Salary: $60.00 - $68.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 9725977972
Show Less
Report",$60.00 - $68.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
6,Konnectingtree,Data Engineer,Remote,"Greetings from KonnectingTree!
We are looking for a Data Engineer for one of our clients. This is a remote position with an Implementation partner. AWS Certification Mandatory.
Data Engineer with AWS Experience
Experience with PySpark/Spark
Experience in Python
Able to work independently
Able to work with the business team directly
Interested candidates kindly share your updated resume with mythili.saravanan@konenctingtree.com. Please reach me at 952-679-2916.
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Required)
Python: 5 years (Required)
PySpark: 5 years (Required)
Work Location: Remote
Speak with the employer
+91 952-679-2916
Show Less
Report",$45.00 - $50.00 Per hour(Employer Est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
7,"STAND 8
4.4",DATA ENGINEER,Remote,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!
Looking for a highly motivated and experienced Data Engineer to join our dynamic team. The successful candidate will be responsible for designing, developing and maintaining comprehensive Datalake and warehouse solutions. The candidate should be able to tackle challenges that come with complex large-scale data having different streams of data sources.

Key Responsibilities
Building different types of data lake and warehousing layers based on specific use cases.
Building scalable data infrastructure and understanding distributed systems concepts from a data storage and compute perspective.
Utilizing expertise in SQL and having a strong understanding of ETL (Extract-Transform-Load) and data modeling.
Ensuring the accuracy and availability of data to customers and understanding how technical decisions can impact the business’s analytics and reporting.
Interfacing with other technology teams to extract, transform and load data from a wide variety of data sources.
Design Data models and Data Products to enable advanced analytics for business
Developing scalable engineering solutions and building data solutions that drive real impact at the company
Collaborate with other teams to ensure proper integration of new features and identify areas for improvement.
Maintain a deep understanding of the software architecture and how different components interact.
Ensure quality by performing root cause analysis and troubleshooting of defects.
Stay current with industry trends and advancements in Data Engineering.

Qualifications
Bachelor’s degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in Data Engineering, with a strong focus on batch and stream data processing using distributed computing systems like Spark.
Proficiency in either pySpark or Scala to handle large-volume data processing.
Proficiency in programming languages like Python or Java.
Familiarity with AWS.
Strong SQL skills, including performance tuning.
Excellent written and verbal communication skills.
Experience with Agile software development methodologies.
Ability to work independently and in a team environment.

The US base range for this contract position is $50-$80/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training
Show Less
Report",$50.00 - $80.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
8,"TekWisen Software Pvt. Ltd
4.3",Data Engineer,"Carlsbad, CA","Overview:
TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. The below job opportunity is to one of Our clients who offer biotechnology product development services. The Company focuses on providing laboratory equipment, chemicals, supplies and services used in healthcare, scientific research, safety, and education.
Job Title: Data Engineer
Duration: 6 Months
Work Location: Carlsbad, CA 92008
Pay rate: $59/hr - $62/hr
Job type: Contract
Work type: Onsite
Job Description:
Primary Job Duties:
Maintain Perl and Java code in data processing environment.
Maintain SQL scripts and PL/SQL procedures.
Maintain JSP scripts for basic web services Develop new programs/scripts as needed.
Minimum Qualifications:
A bachelor's degree in computer science, Engineering, Biology, or equivalent experience.
Very good working knowledge of PERL (minimum 3-4 years)
Highly proficient in Oracle SQL and PL/SQL (minimum 3-4 year's experience)
Very good knowledge of Core Java (minimum 3 years working experience)
Good understanding of Java EE (Servlets, JSP).
Good knowledge of XML and JSON data formats.
Good working knowledge of UNIX / Linux systems.
Excellent communication / documentation skills.
Familiarity with Bioinformatics tools is a plus.
Preferably has experience with LSF - Load Sharing Facility.
Experience with Agile Software Development process a plus.
Must be detail oriented and a self-starter.
Multi-tasking with good follow through skills, good communication skills.
Ability to work well in a team environment.
Strong problem solving, debugging and troubleshooting skills using latest tools and technology.
Ability to work alone and accomplish tasks without supervision.
TekWissen® Group is an equal opportunity/affirmative action Employer (m/f/d/v) supporting workforce diversity.
Job Type: Contract
Salary: $59.00 - $62.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Carlsbad, CA 92008: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This is a W2 requirement. Are you available on W2?
Experience:
Perl: 3 years (Preferred)
SQL: 3 years (Preferred)
PL/SQL: 3 years (Preferred)
Work Location: One location
Show Less
Report",$59.00 - $62.00 Per hour(Employer Est.),501 to 1000 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$25 to $50 million (USD)
9,"PCS GLOBAL TECH
4.7",Data Engineer (PAID BOOTCAMP),"Brooklyn, NY","SQL Developer duties and responsibilities
Development of high quality database solutions
Develop, implement and optimize stored procedures and functions using T-SQL
Review and interpret ongoing business report requirements
Research required data
Build appropriate and useful reporting deliverables
Analyze existing SQL queries for performance improvements
Suggest new queries
Develop procedures and scripts for data migration
Provide timely scheduled management reporting
Investigate exceptions with regard to asset movements
SQL Developer requirements and qualifications
1-3 years of experience as a SQL Developer or similar role
Excellent understanding of T-SQL programming
BS/MS degree in Computer Science, Engineering or a related subject
Good knowledge of HTML and JavaScript
1 year of experience with SQL Server Reporting Services and SQL Server Analysis Services
Critical thinker and problem-solving skills
Good time-management skills
Great interpersonal and communication skills
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Compensation package:
Monthly bonus
Experience level:
1 year
Under 1 year
Experience:
SQL: 1 year (Preferred)
Work Location: On the road
Show Less
Report",$60T - $80T (Employer Est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
10,"Glow Networks
3.6",Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour(Employer Est.),51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
11,"Robert W. Baird
4.3",Data & Analytics Engineer,"Milwaukee, WI","As we continue to grow and add top talent to the Baird family of technical associates, we are looking for a Data & Analytics Engineer for our growing data team. This is a key role on our IT Data Team requiring a broad range of skills and the ability to step into different roles depending on the size and scope of the business need. The self-motivated candidate will have proven experience architecting successful data solutions on key projects in a collaborative environment. Success will come from being able to prioritize, deliver value incrementally, problem solve, and manage changing priorities. You will work closely with our business partners and interface with both technical and non-technical colleagues.
This position is hybrid, working a combination of remote and in-office in our new collaborative work space. We offer a collaborative culture with a continuous learning, agile/lean environment and adding value to the Baird business. Learn more about Baird IT here.

As a Data & Analytics Engineer, you will:
Data Architecture
Specialize in data modeling, both 3NF and dimensional, with experience in conceptual, logical, physical, and industry data modeling. Strong knowledge and experience with data architecture methodologies.
Apply the appropriate level of modeling theory, pattern recognition, and abstractions to architect and design a pragmatic solution that functionally meets the business and technical requirements.
Partner with internal business units to define information requirements and translate them into appropriate data solutions.
Collaborate with IT and business partners to lead data discovery, profiling, analysis, and quality assessments in order to obtain clear information requirements.
Develop and validate source to target mappings and transformation logic required to support business needs. Understand the importance of capturing data lineage.
Architect, implement and verify end-to-end data solutions.
Develop test plans needed to ensure a quality deliverable. Participate in validation testing, coordinate user acceptance testing and training to ensure the final implementation enables the user to solve their business problem.
General Data Management
Play a critical role in architecting our data and analytics solution landscape
Demonstrate competence, experience, knowledge, understanding, and advocacy of data management concepts, data warehousing, BI, and analytics.
Demonstrate ability to perform appropriate level of strategic thinking by viewing initiatives both within the immediate project context as well as the overall architectural vision.
Participate and/or Lead in data architectural design and strategy discussions.
Data Delivery
Work with the business users to conduct data discovery engagements and can quickly identify, and prototype, a solution that brings together multiple data sources into one coherent concept and understanding. (data blending)
Leverage existing tools to create data visualizations and mentors the business to be self-sufficient.
Collaborate – build relationships!
Identify and communicate project risks and impediments and proactively work with other members of the Analytics team to complete high-value deliverables as identified by business partners and team leadership.
Partner with Analytics team members to translate business and functional requirements into technical designs
Strive to understand the data consumption needs of the business community, as well as the problems faced by business users involving the access and use of data
Help Analytics teams develop solutions that enable businesses to capitalize on business insights and drive toward gaining a competitive advantage
What makes this opportunity great:
Information technology is a core part of Baird’s business strategy and plays a critical role in the growth and transformation of the firm.
On Computerworld’s ‘Best Company to Work For’ list for five consecutive years with a collaborative culture that values diverse backgrounds and perspectives while emphasizing teamwork and a strong sense of partnership.
Support and flexibility to grow and be your best at work, at home, and in the community.
What we look for:
Minimum of 3-5 years of experience in Data Solution delivery in a complex environment working collaboratively in a team setting
Proficient in Data Solution tools and concepts such as:
Business Intelligence tools: Microsoft tools (SQL Server Management Studio, SSRS, SSAS, Power Pivot, Power Query, PowerBI), Alteryx
Database: SQL Server
Data Query tools: SQL, T-SQL
Data Management and Quality: data mapping, data profiling, metadata repository, relational data modeling, master data management
Data Modeling: ER/Studio Data Architect, 3NF and dimensional modeling
Data Warehousing concepts: Inmon, Kimball, Data Lake
Data Integration concepts and strategies: EII, ETL, EL-T and EAI
#LI-SB1
#LI-Hybrid
Commitment to Inclusion & Diversity
Baird is committed to inclusion & diversity for our clients, our associates and the communities where we live and work. This commitment stems from our culture of integrity, genuine concern for others and respect for the individual. We view inclusion & diversity as an ongoing journey – one of shared responsibility, continuous improvement and a focus on progress. We invite you to join us as we work together to foster an environment where diversity unites rather than divides us.
Show Less
Report",$84T - $1L (Glassdoor Est.),1001 to 5000 Employees,Company - Private,Finance,Investment & Asset Management,1919,$2 to $5 billion (USD)
12,BlueOcean,Data Engineer,Remote,"BlueOcean is a different kind of SaaS company. We are a team of problem solvers fueled by challenging convention, and we are looking for doers, leaders, and dreamers to join us. We are venture-backed by arguably the best SaaS VC in the world Insight Partners, and we are looking to hire those who want to join us as we build this ship.
Our mission is to revolutionize how marketing decisions are made. We are a single pane of glass connecting data and insights to actions and outcomes. Our fundamental vision is to simply Unlock Human Creativity!
You're excited about this opportunity because you will:
Get in at the ground floor of a company fundamentally changing the brand strategy landscape through the use of revolutionary technology
Define and assist in building data solutions to support our data science initiatives
Work with Data Science leaders and stakeholders to ensure data solutions are defined by business requirements and meet stakeholder expectations
Collaborate with Engineering and IT to ensure solutions meet company security and operations requirements
Build database schemas, build, and deploy ETL processes
Assist in build/buy analysis for data management and overlay products
Work with vendors to implement platform and/or product solutions to meet stakeholder requirements
Work with data sources through API or Files
We are excited that you are:
Strongly analytical, with reasoning skills that result in clear technical architectures
A clear thinker, with the ability to translate requirements into clean, efficient, quality code
Driven, proven to prioritize, self-direct and execute at startup velocity
Skilled in communication with both technical and non-technical stakeholders
Qualifications:
Bachelor’s degree in Computer Science or related discipline
4+ years of experience working with large, complex data sets through batch, streaming and shadow copy
Experience deploying and running data pipelines on Amazon Web Services (AWS)
Excellent grasp of Data Warehousing, ETL/ELT and In-Memory/Cubes use cases and patterns
In-depth experience with SQL Server, SSIS and SSAS
Experience with document and graph type NoSQL databases
Experience with REST and GraphQL API
Strong experience working in Python
Compensation:
The salary range for this role is from $135,000 to $155,000. BlueOcean offers a comprehensive benefits package, including healthcare benefits, short-term disability coverage, and 401k. Employees also receive the following: flexible paid time off, 12 paid holidays, company-wide recharge days, and remote working arrangements.
Location:
This is a remote position. BlueOcean is currently able to support employees in the following states: CA, GA, MA, NC, NH, NJ, NY, OR, PA, SC, TX, UT, VA, WA, and WY.
Job Type: Full-time
Pay: $135,000.00 - $155,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
Schedule:
Monday to Friday
Application Question(s):
Do you have python coding experience? If so, how many years?
Do you have experience managing data pipelines in AWS? If so, how much experience do you have?
Do you have experience with AWS services like Glue, Airflow, etc?
Work Location: Remote
Show Less
Report",$1L - $2L (Employer Est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
13,Titan Healthcare Management Solutions,Data Engineer,Remote,"Titan Health is currently hiring for a Data Engineer.
Under the general direction of the Technology Solutions Manager, the Data Engineer is responsible for implementation, configuration, maintenance, and performance of business-critical data infrastructure to deliver data enablement at scale, and power our revenue cycle applications. This role will work with enterprise and client leaders to translate business and functional requirements into technical specifications and solutions within the data architecture strategy.
Essential Job Duties/Responsibilities
Implement efficient and scalable pipelines integrating data from multiple sources to common data models.
Convert raw data into usable information for client and enterprise organizations.
Within an Agile team design, develop, test, implement, and support technical solutions that support full-stack development tools and technologies.
Support data science, data enrichment, research, and data analysis as well as making data operationally able to be consumed by products and services.
Collaborate with product managers, software engineers, security & compliance, and data scientists to enable them with robust data delivery solutions that drive powerful experiences.
Identify and debug issues with code and suggest changes and/or improvements.
Perform unit tests and conduct reviews with the team to ensure code is rigorously designed, elegantly coded, and effectively tuned for performance.
Utilize available technologies to collect and map data to find cost savings and optimization opportunities.
Support and drive a proactive culture of security and compliance.
Bring an agile and engineering mindset to address complex problems, identify opportunities and craft creative solutions.
Leverage best practice coding and engineering standards to support growth and flexibility.
Coordinate with teams across the organization to address incident, change and release management needs/requirements.
Provide input to risk management; report risks as they are identified and participate in prioritization/follow up.
Stay current with emerging technologies and advancements within existing technologies.
Positively and deliberately engage with colleagues – external and internal – to foster collaborative and productive relationships.
Cultivate great teams and lead in alignment with Titan values.
Comply with and hold with utmost regard all compliance requirements to protect patient privacy and confidentiality.
Stay curious, kind and contribute positively to the Titan culture.
Minimum Qualifications
Bachelor’s Degree in Information Technology, Computer Science, Mathematics or related field is preferred but not required.
Experience with Agile methodologies.
Experience working in HIPAA, HITRUST or other advanced compliance environments.
Experience leading the lifecycle management and integrations of enterprise data.
Ability to clearly articulate technology concepts to business leaders and engineers.
Strong understanding of IT Service Management practices.
Strong analytical, problem-solving, and critical thinking skills with excellent attention to detail.
Direct experience with Azure SQL.
Experience with Azure Data Factory preferred.
Prior experience in the facilitating conversations to translate business requirements into the technical requirements needed to develop solutions.
Excellent oral and written communication skills.
Comprehensive knowledge of Microsoft Office applications.
Knowledge of Hospital CMS or billing data structures preferred.
Titan Health offers a robust Health and Welfare benefits program, along with Paid Time Off, 401k plan with company match, and remote working environment.
H9CslyVeWp
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
14,"NucleusTeq
5.0",Data Engineer,Remote,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$5 to $25 million (USD)
15,ALL-INONESOL,Sr. Data Engineer,Remote,"Role: Data Engineer- Front End
Duration: 6 + Months
Location: Remote
Minimum Requirements
10++ years of experience in data engineering- Python/PySpark,Typescript ( or JavaScript).
Python – complete language proficiency
SQL – proficiency in querying
PySpark
Typescript
Job Types: Part-time, Contract, Temporary
Pay: $70.00 - $72.00 per hour
Schedule:
8 hour shift
Experience:
Python: 10 years (Preferred)
Typescript: 4 years (Preferred)
Spark: 4 years (Preferred)
Work Location: Remote
Show Less
Report",$70.00 - $72.00 Per hour(Employer Est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
16,"Holman
3.7",Data Engineer,Remote,"Holman is currently accepting applications for the role of Data Platform Engineer
Principal Purpose of Position:
Design, develop, document and execute data solutions, tools and practices
Analysis of requirements at sufficient level of detail to allow ETL solution to be developed
Development of ETL job flows according to company standards for naming, performance, restartability and performance.
Support testing and remediation of defects in newly-developed/modified ETL workflows
Promote ETL workflows to PROD and provide ongoing support in PRODUCTION, including monitoring and troubleshooting
Ability to create Power BI Datasets to support the Analytic Delivery team
Evaluate emerging data platform technologies
Lead technology implementations
Follow and contribute to best practices for data management and governance
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes)
Performance tune and troubleshoot processes under development and in production as necessary.
Work with the Data Architects to augment ERD’s as changes are developed
Develop, maintain, and extend reusable data components
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.
Required Experience/Skills
2+ years Azure exposure (Any Resources: Databases, Data Factory, Synapse Studio, Storage Account, Power Platform)
2+ years ANSI SQL experience
1+ years data modeling exposure
Advanced problem solving/Critical thinking mindset
Preferred Experience/Skills
Azure connectivity/authentication (service principals, managed identities, certificates)
Power BI Dataset creation/maintenance
Azure Resources: DevOps, Logic Apps, Gen 2 Storage, Purview
SQL Server, Oracle, Python, Spark
Education and/or Training:
Bachelor’s degree in Computer Science or equivalent work experience
Compensation: Starting at $110,00 USD
#LIREMOTE
Job Type: Full-time
Pay: From $110,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Monday to Friday
Experience:
ETL: 2 years (Preferred)
Azure: 2 years (Preferred)
Work Location: Remote
Show Less
Report",$1L (Employer Est.),5001 to 10000 Employees,Company - Private,Retail & Wholesale,Motor Vehicle Dealers,1924,$2 to $5 billion (USD)
17,"ERPMark Inc
4.0",Data Engineer (Scala),"Bentonville, AR","Title: Scala Engineer
Location: Bentonville, AR
Spark Engineer Description:
Client is looking for a highly energetic and collaborative Spark Data Engineer for a 12-month engagement. Responsibilities: As a Senior Spark Data Engineer, you will • Design and develop big data applications using the latest open source technologies. • Desired working in offshore model and Managed outcome • Develop logical and physical data models for big data platforms. • Automate workflows using Apache Airflow. • Create data pipelines using Apache Hive, Apache Spark, Apache Kafka. • Provide ongoing maintenance and enhancements to existing systems and participate in rotational on-call support. • Learn our business domain and technology infrastructure quickly and share your knowledge freely and actively with others in the team. • Mentor junior engineers on the team • Lead daily standups and design reviews • Groom and prioritize backlog using JIRA • Act as the point of contact for your assigned business domain Requirements: GCP Experience • 1+ years of recent GCP/BIG Query experience • Experience building data pipelines in GCP • GCP Dataproc, GCS & BIGQuery experience • 5+ years of hands-on experience with developing data warehouse solutions and data products. • 5+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive or Spark, Airflow or a workflow orchestration solution are required • 2+ years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms. • Experience with programming languages: Python, Java, Scala, etc. • Experience with scripting languages: Perl, Shell, etc. • Practice working with, processing, and managing large data sets (multi TB/PB scale). • Exposure to test driven development and automated testing frameworks. • Background in Scrum/Agile development methodologies. • Capable of delivering on multiple competing priorities with little supervision. • Excellent verbal and written communication skills. • Bachelor's Degree in computer science or equivalent experience. The most successful candidates will also have experience in the following: • Gitflow • Atlassian products - BitBucket, JIRA, Confluence etc. • Continuous Integration tools such as Bamboo, Jenkins, or TFS
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Schedule:
8 hour shift
Work Location: In person
Show Less
Report",$60.00 - $65.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
18,"TikTok
3.6","Data Engineer, E-Commerce","San Jose, CA","Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.

Why Join Us
At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok.

The Global E-Commerce team focuses on building data infrastructure and data product areas to support business engineering teams working directly on TikTok's E-Commerce platform.

As a data engineer in the Global E-Commerce team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.

Responsibilities - What You'll Do
Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis);
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;
Establish solid design and best engineering practice for engineers as well as non-technical people.
Qualifications
BS or MS degree in Computer Science or related technical field or equivalent practical experience;
Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.);
Experience with performing data analysis, data ingestion and data integration;
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems;
Experience with schema design, data modeling and SQL queries;
Passionate and self-motivated about technologies in the Big Data area.
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at Dennis.Chau@tiktok.com
Job Information
The base salary range for this position in the selected city is $136000 - $280000 annually.



Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.



At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees:



We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.



Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.



We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice.
Apply Now: click Apply Now
Show Less
Report",$1L - $3L (Employer Est.),10000+ Employees,Company - Private,Information Technology,Internet & Web Services,2016,Unknown / Non-Applicable
19,"Annexus Health
3.5",Data Engineer,"Raleigh, NC","Diverse experience. A shared passion.
At Annexus Health, we are a team of dedicated professionals with backgrounds in life sciences, healthcare software technology development, and the provider setting. While we approach our work from different angles, we are united by our commitment to reducing the administrative and logistical burdens across the patient journey in order to combat financial toxicity at both the patient level and the practice level.
Senior Data Engineer
We are seeking an energetic and talented Data Engineer to deliver high value, high-quality business capabilities to our data technology platform. You will be an integral member of the engineering team delivering across multiple business functional areas. If you're highly motivated, passionate about technology and data, and eager to join a fast growing start-up focused on making a difference with patients, then we would love to meet you!
What You'll Do:
Code ETL process using Azure Data Factory work flows and other technologies as appropriate.
Build and optimize complex SQL queries
Create data analytics-driven solutions for critical business objectives

About You:
Bachelor's degree in Computer Science, Information Science or related disciplines
In-depth understanding of data management (e.g. permissions, recovery, security and monitoring)
Strong experience with SQL and database management systems, and data modeling (Azure cloud experience preferred)
3+ years of technical expertise in SQL server environment
Knowledge of .NET Framework and ability to work with C#, preferred
Py/Spark and Synapse (Data warehousing) and Data Factory some migration
Standardize, manipulate and process flow - how do you process flow, how do you cleanse the data?
ETL and HL7 a plus
Ability to understand front-end users requirements and a problem-solving attitude
Technical smarts and quick learner
Relentlessly customer-focused attitude
Passion to connect the client with our internal team
Ability/desire to thrive in a service oriented culture
Quick and deep learner
Driven to make a difference
Love to have fun!

Read more about the Annexus culture: https://www.annexushealth.com/about
Show Less
Report",$81T - $1L (Glassdoor Est.),Unknown,Company - Private,Healthcare,Healthcare Services & Hospitals,#N/A,Unknown / Non-Applicable
20,"Zoll Medical Corporation
3.3",Data Engineer,"Broomfield, CO","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote
Show Less
Report",$1L - $1L (Employer Est.),1001 to 5000 Employees,Company - Private,Manufacturing,Health Care Products Manufacturing,1980,$100 to $500 million (USD)
21,"Apple
4.2",Data Engineer,"Cupertino, CA","Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number:200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.
To apply to this job, click Apply Now
Show Less
Report",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD)
22,"Bloom Insurance
3.4",Data Engineer I,Remote,"Data Engineer I
To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.

Essential Functions
Day to day functions include the following:
Design data models and develop database structures in Microsoft SQL server.
Write various database objects like stored procedures, functions, views, triggers for various front end applications.
Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.
Create database deployment packages for deploying changes.
Identify & repair inconsistencies in data, database tuning, query optimization.
Able to generate ad hoc data on demand.
Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise manner
Develop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.
Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.
Documentation
Optimization recommendations
Day to day troubleshooting
.NET Programming as needed

Education/Experience
BA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experience
Solid experience with various versions of MS SQL Server and TSQL programming
Microsoft Certified DBA a plus

Skills/Knowledge
Strong experience in writing efficient SQL code
Working knowledge of SQL Server Management Studio (SSMS)
Knowledge of SQL Server Reporting Services (SSRS)
Knowledge of SQL Server Integration Services (SSIS)
Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plus
Knowledge of data science technologies is a plus
Clear, concise communication skills, excellent organizational skills
Highly self-motivated and directed
Keen attention to detail
High level of work intensity in a team environment
High integrity and values-driven
Eager for professional development
Experience and understanding of source control management a plus
What We Offer
At Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:
Competitive compensation
Comprehensive health benefits
Long-term career growth and mentoring
About Bloom
As an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.
Ascend Technology ™
Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.

Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2006,$25 to $50 million (USD)
23,Adroit Software Inc,Data Engineer,"Smithfield, RI","Role Description
The team is hiring a Lead Developer and you will be responsible for working with Architecture, Data Governance and Business Intelligence teams to support Fidelity’s or vendor Data Management tools and evaluate new technologies. You will possess a passion for continuous learning and upskilling in new technologies and a flair for exploring.
Required Qualifications
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server will be a huge plus
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization
Additional Experience
Experience with Metadata management solutions / Data lineage is a plus
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus
Vendor management is a plus
Some QA/Testing experience is a plus
Some Kubernetes / Docker experience is a plus
Strong communication and presentation skills
COVID Work Policy
Safety is our top priority. Once we can be together in person with fewer safety measures, this role will follow our dynamic working approach. You’ll be spending some of your time onsite depending on the nature and needs of your role.
Dynamic Working – Post Pandemic
Our aim is to combine the best of working offsite with coming together in person. For most teams this means a consistent balance of working from home and office that supports the needs of your role, experience level, and working style.
Your success and growth is important to us, so you’ll want to enjoy the benefits of coming together in person – face to face learning and training, quality time with your manager and teammates, building your career network, making friends, and taking full advantage of cultural and social experiences Fidelity provides for you.
Description
Fidelity TalentSource is your destination for discovering your next temporary role at Fidelity Investments. We are currently sourcing for a Data Engineer to work at Fidelity's location in Smithfield, RI, Durham, NC or Westlake, TX!
Fidelity Brokerage Technology (FBT) enables business partners to win in their respective marketplaces by designing, building and maintaining the technology platforms and products of Fidelity Institutional, Personal Investing and Workplace Investing.
FBT Business Intelligence Team is looking for a Lead Technologist to support our Data Management Tools and evaluating new Technologies in support of Architecture directives!
Role Description
The team is hiring a Lead Developer and you will be responsible for working with Architecture, Data Governance and Business Intelligence teams to support Fidelity’s or vendor Data Management tools and evaluate new technologies. You will possess a passion for continuous learning and upskilling in new technologies and a flair for exploring.
Required Qualifications
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server will be a huge plus
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization
Additional Experience
Experience with Metadata management solutions / Data lineage is a plus
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus
Vendor management is a plus
Some QA/Testing experience is a plus
Some Kubernetes / Docker experience is a plus
Strong communication and presentation skills
COVID Work Policy
Safety is our top priority. Once we can be together in person with fewer safety measures, this role will follow our dynamic working approach. You’ll be spending some of your time onsite depending on the nature and needs of your role.
Dynamic Working – Post Pandemic
Our aim is to combine the best of working offsite with coming together in person. For most teams this means a consistent balance of working from home and office that supports the needs of your role, experience level, and working style.
Your success and growth is important to us, so you’ll want to enjoy the benefits of coming together in person – face to face learning and training, quality time with your manager and teammates, building your career network, making friends, and taking full advantage of cultural and social experiences Fidelity provides for you.
Special Instructions
MUST HAVE: ETL Informatica, SQL, snowflake, basic python
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months
Pre-Screen Questions:
1. What's the difference between a global and local variable? How would you update a global variable in a function?
Global Variable - is defined outside of any function and can be accessed anywhere in the current python file
Local Variable - is defined inside a function and can only be accessed within that same function.
Update global Variable: After declaring the function, you would then use the ""global"" keyword followed by the variable name. This would allow you to edit the global variable inside a function.
Job Type: Contract
Salary: $60.00 - $80.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Parental leave
Vision insurance
Compensation package:
1099 contract
Hourly pay
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Smithfield, RI 02917: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
W2 Only No C2c
Education:
Bachelor's (Preferred)
Experience:
Brokerage (Preferred)
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Snowflake (Preferred)
python (Preferred)
Willingness to travel:
100% (Preferred)
Work Location: One location
Show Less
Report",$60.00 - $80.00 Per hour(Employer Est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
24,"Jetty
4.2",Data Engineer,"New York, NY","Welcome to Jetty, the financial services platform on a mission to make renting a home more affordable and flexible. We've built multiple financial products that benefit both renters and property managers - and we're just getting started.

As a member of the Jetty engineering team, you're passionate about building fintech products that provide value to our customers and to Jetty. You are motivated by designing engineering systems around complex business problems. You love to learn, take on challenges, and are empowered in a fast-paced and transparent culture. You're comfortable finding the right tool or pattern for the job, and advocating for improvements to the way we work.

As a Data Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high level, solving complex problems, and delivering solutions with real business impact - and you're excited by the opportunity to apply those principles to a new, best in class function.

Role & Responsibilities
Build / Support our modern data stack (Snowflake / Fivetran / DBT / Tableau)
Implement the Five Pillars of Data Observability
Write ELT code using modern software engineering practices (Git, automated testing and deployments)
Build and maintain data pipelines to support various business processes and reporting (Fivetran / AWS Lambdas)
Document our data models in a user friendly way for our business stakeholders
Partner with the Product Engineering team to ensure we are capturing the data we need from our applications for analytics and to iterate on our development practices for the data analytics team.
Be an enthusiastic evangelist of our modern data stack (Fivetran / DBT / Snowflake / Tablea)
Be the resident resource on building standard reports and BI dashboards
Experience & Qualifications
4+ years of experience working in a data / analytics engineering role
High proficiency in Snowflake / Fivetran / dbt / Tableau
High proficiency in SQL and Python
Ability to collect, interpret, and synthesize inputs from various parts of the business into data model requirements
Ability to simplify without being simplistic - ability to communicate complex topics and actionable insights in a compelling way that can be understood by a variety of audiences
Inherent curiosity and analytical follow-through — you can't help but ask ""why?"" and love using data and logic to explore potential solutions
Ability to balance ""Rigor"" and ""Scrappiness"" — you know the difference between 80/20 and giving something 110%; as well as when each is appropriate.
Deep understanding of the first and second order effects of reporting — you know the power of presenting the right data to the right people at the right time
Experience in a data/analytics function at a high-growth startup managing multiple stakeholders and delivering actionable insights
About Jetty

At Jetty, we know renting a home can be a financial challenge. That's why we're on a mission to make renting accessible to everyone. Jetty offers four financial products designed to help our members every step of the renting process: Jetty Deposit, a low-cost security deposit product that dramatically reduces move-in costs; Jetty Rent, a flexible rent payment program to eliminate pricey late rent fees; Jetty Credit, a credit building service that helps renters build credit just by paying rent; and Jetty Protect, an affordable renters insurance product that provides comprehensive coverage in just a few clicks.

Jetty has raised multiple rounds of venture capital from investors including Khosla Ventures, Ribbit Capital, Citi, Valar, and strategic investors. We've built a highly collaborative team working remotely around the country, and we believe in finding the best talent—regardless of where they live. To learn more about life at Jetty, visit jetty.com/careers.

Jetty is firmly committed to building a team as diverse as our Members. We are proud to provide equal employment opportunities for all candidates regardless of race, ancestry, citizenship, sex, gender identity or expression, religion, sexual orientation, marital status, age, disability, or veteran status.

Benefits & Perks
Health (with HSA and FSA options), dental, and vision insurance through Aetna & MetLife
401(k) retirement savings program
Optional life and disability coverage
20 days of PTO + 12 holidays, ""Jetty Winter Break,"" and flexible sick days
Generous parental leave policy
Flexible remote work in any US location (keeping east coast hours)
Stipends to cover WFH set-up, childcare, phone/internet bill, and optional co-working space
Show Less
Report",$97T - $1L (Glassdoor Est.),51 to 200 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2015,Unknown / Non-Applicable
25,"DevCare Solutions
3.7",Data Engineer,"Columbus, OH","Identifies data sources, develop and maintain data architectures, constructs data catalog and data decomposition diagrams, provide data flow diagrams and documents the process.
Develop conclusions and articulate implications of advanced data analytics results
Ability to data mining/sourcing to be self-sufficient in the production of valuation and the analysis of business cases and ad-hoc requests.
Ability to extract data from multiple data resources; remove corrupted data and fixing coding errors and related problems
Ability to perform analysis to assess the quality and meaning of data; filter data by reviewing reports and performance indicators to identify and correct ode problems.
ability to create compelling data visualizations and communicate insights effectively to technical and non-technical stakeholders is essential for data scientists.
Using tools to identify, analyze, and interpret patterns and rends in complex data sets could be helpful for the diagnosis and prediction
Preparing reports for the management stating trends, patterns, and predictions using relevant data in visualization.
Work with developer and management to identify process improvement opportunities, propose system modification, and devise data governance strategies.
7+ years of Oracle/SQL Database
7+ years of SQL script
7+ years of API designs in RESTful, XML, XSLT, JSON, jQuery.
5+ years of Python and/or other ETL tools
5+ years of SQL/T-SQL
5+ years of PL/SQL Experience
3+ years of Alteryx, Cloudera, Snoflake
3+ years of PowerBI, Denodo
Excellent analytical skills, attention to detail, and problem-solving skills.
Proven ability to handle multiple tasks and projects simultaneously.
Familiar with the Agile Methodology.
Working experience in Database & application performance tuning.
Must possess excellent written and oral communication skills.
Working experience in delivering expected results in unstructured environments.
Works productively and effectively independently without significant management oversight.
Job Types: Part-time, Contract
Salary: $55.00 - $65.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Columbus, OH 43081
Show Less
Report",$55.00 - $65.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2005,Unknown / Non-Applicable
26,KesarWeb,Data Engineer,"Los Angeles, CA","We are searching for an accountable, multitalented data engineer to facilitate the operations of our data scientists. The data engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.
To ensure success as a data engineer, you should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. A formidable data engineer will demonstrate unsatiated curiosity and outstanding interpersonal skills.
Data Engineer Responsibilities:
Liaising with coworkers and clients to elucidate the requirements for each task.
Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
Reformulating existing frameworks to optimize their functioning.
Testing such structures to ensure that they are fit for use.
Preparing raw data for manipulation by data scientists.
Detecting and correcting errors in your work.
Ensuring that your work remains backed up and readily accessible to relevant coworkers.
Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.
Data Engineer Requirements:
Bachelor's degree in data engineering, big data analytics, computer engineering, or related field.
Master's degree in a relevant field is advantageous.
Proven experience as a data engineer, software developer, or similar.
Expert proficiency in Python, C++, Java, R, and SQL.
Familiarity with Hadoop or suitable equivalent.
Excellent analytical and problem-solving skills.
A knack for independence and group work.
Scrupulous approach to duties.
Capacity to successfully manage a pipeline of duties with minimal supervision.
Job Type: Full-time
Pay: $98,000.00 - $106,000.00 per year
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
Schedule:
Monday to Friday
Work Location: Remote
Show Less
Report",$98T - $1L (Employer Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
27,"HelloFresh
3.7",Data Engineer,"Grand Prairie, TX","Data Engineer, Hello Fresh

This role can be based out of the following facilities:
NYC, Newark, NJ, Totowa, NJ, Phoenix, AZ, Newnan, GA, Aurora, CO, Irving, TX and Grand Prairie, TX

Job Description
As a Data Engineer, you will work with the Fulfillment Planning Technology team to help build the next generation suite of internal tools that enable Planning and Operations teams to see and act quickly to changing business conditions. The Data Engineer will build scalable data pipelines, infrastructure and tools that power our products and services.
You will …
Work with analysts, engineers and planners to design, build, and maintain efficient, scalable and reliable data pipelines to support our business-critical needs in data ingestion, processing, and analysis
Develop and maintain efficient, scalable and reliable code in Python and SQL
Collaborate with other team members to troubleshoot, perform root cause analysis and optimize existing data pipelines and tools
Work with other team members to ensure effective tool integration with other systems and workflows
At a minimum, you have …
Bachelor’s or Master’s degree in Computer Science, Engineering or related field
2+ years of data engineering experience in Fulfillment, Logistics, Supply Chain, Production, or related field working with physical goods
Strong proficiency in Python and SQL
Experience working with distributed systems, building and maintaining pipelines using cloud technologies (AWS, Snowflake)
Experience in containerization and orchestration (Docker, Kubernetes, Airflow, GCP)
Startup experience a plus

You'll get...
Competitive Salary & 401k company match that vests immediately upon participation
Generous parental leave of 4 weeks & PTO policy, as well as paid holidays off
$0 monthly premium and other flexible health plans
Amazing discounts, including up to 75% off HelloFresh subscription
Flexible shift scheduling & advancement opportunities
Emergency child and adult care services
Snacks & monthly catered lunches
Collaborative, dynamic work environment within a fast-paced, mission-driven company
It is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran.

New York Pay Range
$110,000—$137,500 USD
Colorado Pay Range
$99,200—$124,000 USD
Show Less
Report",$1L - $1L (Employer Est.),10000+ Employees,Company - Public,Personal Consumer Services,Beauty & Wellness,2011,$5 to $10 billion (USD)
28,Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour(Employer Est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
29,"Abarca Health
3.6","COOP, Data Engineer","Miami, FL","What you'll do
In a few words…
Abarca is igniting a revolution in healthcare. We built our company on the belief that with smarter technology we are redefining pharmacy benefits, but this is just the beginning…
At Abarca's company culture centers around embracing a growth mindset, theme of inspiring excellence, and encouraging all to bring their best each day! Our Interns/Coops work on real-world projects in collaboration with teams across Abarca Health, while having fun along the way. You'll be empowered to build community, explore your passions, and achieve your goals. The Internship & Coop programs are designed to encourage bringing solutions and ideas to life while working on cutting-edge technology, while experiencing our culture of community. In this role you will be part of the Business Intelligence team where you will be immersed in the world of healthcare technology, which impact millions of lives.
The fundamentals for the job…
Understand the data structure and meaning in both the source and the target Business Intelligence tools, Tableau and Snowflake
Apply business rules as transformations.
Adapt ELT processes to accommodate changes in source systems and new business user requirements.
Building and unit testing data transformation, source data transport, and population processes for environment
Assisting the Business Intelligence Team in delivering information and analytic solutions to the company's key stakeholders, including members of the various business units, internal/external providers, and internal staff.
Report/Query testing, development using dbt and Snowflake SQL.
What we expect of you
The bold requirements…
Be an active student at least in third year and above enrolled in a bachelor's or master's degree in computer engineering, Computer Science, Software Engineering, or related field.
Must have taken and enjoyed the database class.
You should have previous knowledge of SQL, Database Design, and Stored Procedures.
Knowledge of Business Intelligence designs and practices.
Excellent oral and written communication skills.
Physical requirements…
Must be able to access and navigate each department at the organization's facilities.
Sedentary work that primarily involves sitting/standing.
At Abarca we value and celebrate diversity. Diversity, equity, inclusion, and belonging are guiding principles of Abarca and ensure Abarca's workforce reflects the communities it serves. We are proud to provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, medical condition, genetic information, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
Abarca Health LLC is an equal employment opportunity employer and participates in E-Verify. ""Applicant must be a United States' citizen. Abarca Health LLC does not sponsor employment visas at this time""
The above description is not intended to limit the scope of the job or to exclude other duties not mentioned. It is not a final set of specifications for the position. It's simply meant to give readers an idea of what the role entails.
#LI-VP1 #LI-REMOTE
Show Less
Report",$71T - $1L (Glassdoor Est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,2005,Unknown / Non-Applicable
30,"Zillion Technologies
3.8",Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour(Employer Est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
31,Talent Groups,100% REMOTE // GCP Data Engineer,Remote,"100% REMOTE
GCP Data Engineer
Full time
GCP BQ query knowledge with handson skill of Hana (DWH)
Requirement is to migrate HANA Native objects to GCP BQ
DWH handon with details surrogate keys, SCD and various types, referential integrity etc
SQL (Joins, tables, ranks)
Python (file handling (open, read, remove etc), database connections, lists, data type conversions
GCP experience (BigQuery, Pub Sub, GCS), it is a plus with migration knowledge
Job Types: Full-time, Contract
Pay: $140,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Speak with the employer
+91 847-474-3706
Show Less
Report",$1L - $2L (Employer Est.),Unknown,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",#N/A,Unknown / Non-Applicable
32,"Kairos Technologies Inc
4.5",Sr Data Engineer with AWS (Hybrid Onsite),"Boston, MA","Greetings from Kairos Technologies Inc,
Role: Sr Data Engineer with AWS
Location: Boston, MA (Hybrid Onsite)
Duration: 12 months
Experience: 9+ yrs.
Hybrid – Weekly once/ twice need to work from Office
RESPONSIBILITIES
Familiarity with data lake, data warehouse or data lake environments and related topics . Has a proven track record to work with vendors to deploy external SaaS solutions and integrate with existing systems.
In depth with data lakes/ data environments including ETL (PySpark), data Catalogs (Glue, Alation), API interfaces, Cloud data warehouses such as Redshift, Querying engines such as Trino.
Agile approaches to building cloud native solutions using CI/CD, containers, Kubernetes, GitOPS, etc..
Strong automation and development skills in terraform, CloudFormation, and other languages like Python, and bash.
8+ years of total IT experience, with at least 4 years in AWS services such as EMR, EC2, S3, IAM, Glue andRedshift and 2+ years of experience in Infrastructure as code technologies like Terraform, CloudFormation.
4+ years of Python, SQL experience is mandatory.
Thanks& Regards,
K Hemanth Kumar | Sr IT Technical Recruiter | Kairos Technologies Inc
Job Type: Contract
Pay: $65.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Boston, MA 02108
Speak with the employer
+91 9728535149
Show Less
Report",$65.00 - $70.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
33,"Tekrek solutions Inc
1.0",Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
34,"Numentica LLC
4.3",AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L (Glassdoor Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
35,"Violet Ink
3.9",Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour(Employer Est.),1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
36,"Gridiron IT
4.4",Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
37,"Open Systems Technologies Corporation
3.9",Junior Data Engineer,"Arlington, VA","Open Systems Technologies Corporation is looking for a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform.

Responsibilities

As a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members.

Required Skills:
Clearance: Secret
4+ years of experience with SQL
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
4+ years of experience with a modern programming language such as Python or Java
4 years of experience working in a big data and cloud environment
Experience with distributed computer understanding and experience with SQL, Spark, ETL.
Documented experience with AWS, EC2, S3, and/or RDS
Preferred Skills:
2 years of experience working in an agile development environment
Ability to quickly learn technical concepts and communicate with multiple functional groups
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes
BENEFITS

Our company OST has been operating since 1996 and have various contracts with Government agencies. We offer a comprehensive benefit package that includes 3 weeks paid time off, 2 weeks Holiday pay, medical/dental coverage, STD, LTD, Life Insurance, ADD, 401k with up to 4% match, and end of year profit sharing paid out in 401k.
Show Less
Report",$69T - $1L (Glassdoor Est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,1996,Unknown / Non-Applicable
38,"AGM Tech Solutions, LLC
4.8",Data Engineer,"Alpharetta, GA","We have an excellent 6-month contract-to-hire opportunity with our Global Leader client.
Candidate must be local to Alpharetta GA (Hybrid Flexibility) - 3 days a week on-site.
Basic
Work experience with ETL, Data Modeling, and Data Architecture.
Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL.
Experience operating very large data warehouses or data lakes.
Preferred
Experience in designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Comfortable using PySpark APIs to perform advanced data transformations.
Familiarity with implementing classes with Python.
Summary
Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue
Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python.
Design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Job Type: Contract
Pay: $70.00 per hour
Benefits:
401(k)
Flexible schedule
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Machine learning: 2 years (Required)
Work Location: One location
Show Less
Report",$70.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
39,"Together Credit Union
3.2",Data Engineer,Remote,"Position:
Data Engineer (Remote- Nationwide)
Position Summary:
As a Data Engineer at Together Credit Union, you will be responsible for developing and enhancing the various real time flow pipelines as well as enabling sophisticated data analysis from data in our data lake, while also maintaining strict high performance and throughput requirements. You will also work closely with Enterprise Architecture, Business Analysts and Security experts to bring new ideas in data exploration and bi analytics to fruition as deliverables that will enable new ways of furthering the member experience.
Job Description:
Role and Responsibilities:
Work with architects, business analysts, and other technical resources to understand data designs and implement them.
Coordinates, plans, and implements data solutions to meet needs of the business.
Create near real time pipeline with Spark jobs using PySpark scripting.
Build required infrastructure for extract load and transform (ELT) operations from various sources of data to Redshift Serverless.
Develop complex SQL.
Implement JDBC solutions with encryption in transit.
Execute pipelines and Spark jobs in AWS EMR Serverless.
Document pipelines solutions.
Understand the role of an Active Meta Data Repository in driving pipeline solutions.
Create shared and reusable code which can be used across PySpark scripting.
Schedule pipelines which execute in a cohesive manner to ensure timely data delivery and ingestion.
Demonstrate strong technical acumen when representing the data team to the business.
Appropriately escalate and mitigate risks for projects and initiatives to leadership.
Experience, Qualifications, and Skills:
Bachelor’s Degree in Computer Science, Management Information Systems, Information Technology, or related field preferred.
3+ years’ experience in Information Technology.
Expert level Python scripting skills and/or two plus years of PySpark development required.
Minimum of 2 years’ experience with AWS Cloud experience preferred.
Familiarity with additional cloud platforms would be a plus.
3 years of SQL experience required.
Understand relational databases and SQL technologies especially massively parallel solutions like AWS Redshift Serverless or Redshift.
2 years’ experience with ELT required.
2 years’ experience with AWS Redshift preferred.
Previous experience with Snowflake, Synaps, and BigQuery would be a plus.
2 years of AWS EMR experiences preferred.
Previous experience with Job scheduling software.
Possesses and applies a high degree of subject matter expertise; continually strives to build on this knowledge to produce results that meet customer needs and enterprise goals and objectives.
Outstanding customer service skills and demonstrated ability to interact with anticipated audiences in a courteous, service-oriented manner.
Excellent organizational, multi-tasking, and time-management skills.
Collaborative team player, capable of working well with others, but also autonomously with little direction.
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Finance,Investment & Asset Management,1939,$25 to $50 million (USD)
40,"MetroSys
5.0",Data Migration Engineer,Remote,"Job Description
Context
We have a great project and need Migration Engineers to assist with migration of an enterprise client to a new data center. This new IT production environment is based in Redhat Linux, IBM AIX, Sun Solaris, UNIX and HP UX. We are searching for an engineer to support the migration by migrating servers, storage and databases to the new environment. We use a strict step-by-step plan (factory plan) to efficiently execute the migration.

Competence
Bachelor of Science / Master's degree
Minimal 3-5 years of relevant work experience within an enterprise environments
Advanced knowledge of Redhat Linux and UNIX
Advanced knowledge of IBM AIX, Solaris, and HP UX
Some knowledge of IBM XIV, Pure Storage, HPE Nimble and 3PAR storage preferred
Experience with databases (Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
The candidate has a hands-on mindset, a strong customer- and problem-solving orientation, shows fast results, and has demonstrated good communication skills, especially in an international IT organization. To achieve the project goals, the candidate is able to liaise directly with all stakeholders. The candidate has a clear focus on results and quality, and is eager to develop quickly as a project leader, serving customers.

Activities
Intake / analysis of applications for migration
Creation or update of migration run books
Migration of VMware instances to the new platform
Creation of storage disks, virtual storage devices
Reconfiguration of servers, including storage & network
1RfnvbOr2v
Show Less
Report",$50.00 - $70.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
41,"FINSYNC, Inc.
4.2",Data Engineer / Data Analyst,Remote,"FINSYNC is looking for an accomplished Data Engineer/Data Analyst to own data pipeline end to end, including data ingestion, curation, modeling and visualizations. This role will ensure proper data architecture and processes to support data initiatives at scale.
Successful Candidates Have:
5+ years of designing and implementing data solutions
5+ years experience developing data models to support credit and underwriting use cases
Deep understanding of methods to transform source data into refined data catalogs for immediate and seamless consumption in presentation layer
5+ years creating scalable data pipelines all the way to the consumption from customers and internal business partners
Experience with side variety visualizations tools such as Qlik Cloud (preferred), MS PowerBI, Tableau
Experience analyzing both quantitative and qualitative data and understanding best practices for visualization
Experience with SQL and similar programming languages
Strong passion for data quality and data governance
Job Responsibilities:
Maintain and extend data pipeline for all data initiatives
Support data initiatives by developing data models and visualizations
Work collaboratively with stakeholders to ensure data quality and relevance
Make recommendations on changes to source data to more efficiently support business use cases
Support the integration and consumption of data into other platforms and systems
What We Can Offer You for All Your Hard Work?
Competitive compensation package
Full suite of health & welfare benefits including medical, PTO & 401(k)
Workplace flexibility and collaboration with a virtual-first team
ABOUT THE COMPANY
We sync with over 7,000 banks and credit unions to provide the approximately 32 million small businesses in the United States a simpler way to manage finances with the software and services they need, all in one platform. After a business syncs their bank account to FINSYNC, our software helps them send and receive payments, process payroll, automate accounting and manage cash flow with less time and better results working with financial partners (bankers, credit card processors, accountants, and insurance agents) who use our software to deliver unmatched services. The result: financial harmony, peace of mind and the best way to succeed.
We cultivate a culture of:
Teamwork (like a pro sports team) without ego
Constructive communication so we can build transparency and trust
Metrics that matter personally, professionally, and financially
High performance merits high compensation
Self-motivation and self-discipline merits flat organization
Time management and work-life-harmony
Finsync is on a mission to improve the lives of others by helping them succeed in business and in life, doing more business with less time and better results so they can invest their time and successes in other ways meaningful to them.
Show Less
Report",$1L - $2L (Employer Est.),Unknown,Company - Private,Information Technology,Software Development,#N/A,Unknown / Non-Applicable
42,"Xiar tech inc
3.3",Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour(Employer Est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
43,"Orion Innovation
3.8",Data Engineer-Databricks,"Edison, NJ","Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.
Job Description

Designing and implementing highly performant data pipelines from multiple sources using Databricks
Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
Working with other members of the project team to support delivery of additional project components (API interfaces)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Integrate Databricks with other technologies (Ingestion tools, Visualization tools)

Knowledge, Skills, and Abilities:
Proven experience working as a data engineer
Highly proficient in using the spark framework (python and/or scala)
Extensive knowledge of Data Warehousing concepts, strategies, methodologies.
Programming experience in Scala or Python, SQL
Direct experience of building data pipelines using Apache Spark (preferably in Databricks).
Hands on experience designing and delivering solutions using Azure, including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake/ADLS Gen)
Experience with ingestion tools (nifi/azure cloud factory
Experience with big data technologies (hadoop)
Experience with Databricks
Must be team oriented with strong collaboration, prioritization, and adaptability skills required
Skills required:
Data Lake/ ADLS Gen 2
Databricks
Scala, Python, PySpark

Orion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Candidate Privacy Policy
Orion Systems Integrators, LLC and its subsidiaries and its affiliates (collectively, ""Orion,"" ""we"" or ""us"") are committed to protecting your privacy. This Candidate Privacy Policy (orioninc.com) (""Notice"") explains:
What information we collect during our application and recruitment process and why we collect it;
How we handle that information; and
How to access and update that information.
Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.
Show Less
Report",$59T - $88T (Glassdoor Est.),5001 to 10000 Employees,Company - Private,Information Technology,Information Technology Support Services,1993,$100 to $500 million (USD)
44,"Barracuda Networks Inc.
3.8",Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L (Glassdoor Est.),1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
45,"APLOMB Technologies
4.4",Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
46,"I28 Technologies
4.2",Jr.Data Engineer - Python / Hadoop / PySpark,"Honolulu, HI","Banking domain knowledge
knowledge in software development and delivery.
knowledge with Neo4j, GraphQL
Good aptitude, knowledge problem-solving abilities, and analytical skills, ability to take ownership as appropriate
Should be able to do coding, debugging, performance tuning, and deploying the apps to the Production environment.
knowledge working in Agile Methodology
Ability to learn and help the team learn new technologies quickly
Take up the complete planning design implementation/ UAT delivery of the project
knowledge communication and coordination skills
Job Type: Full-time
Salary: $55,000.00 - $60,000.00 per year
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: One location
Show Less
Report",$55T - $60T (Employer Est.),1 to 50 Employees,Company - Private,Finance,Accounting & Tax,#N/A,Less than $1 million (USD)
47,"Pura
3.3",Data Engineer (Mid/Jr),"Pleasant Grove, UT","Data Engineer (Mid/Jr Level)
Pura has been revolutionizing the smart home experience for the past several years. We obsess over providing world-class experiences for our customers, partners, and vendors. We pride ourselves on maintaining a high standard of quality and innovation with our products, and continuous growth and development for our people.
We are looking for a Data Engineer to help business users and analysts throughout the organization access the data they need to operate and grow the business.
What you’ll own:
In this high-impact role, you will:
Work closely with the Data Science team to design and develop scalable data pipelines for processing and analyzing large volumes of data
Build and maintain ETL processes using Python, SQL, Apache Airflow, and other technologies
Develop and deploy data processing jobs on AWS or GCP using Docker and Kubernetes
Write API wrappers to integrate with various external data sources and third-party tools
Implement and maintain best practices for data security, data quality, and data governance
Collaborate with other cross-functional teams to ensure data is available, reliable, and accessible to support business decisions
Write clean, readable, and maintainable code and ensure code is thoroughly tested and documented

Qualifications:
Bachelor's degree in Computer Science, Software Engineering, or related field
1-3 years of experience in data engineering or a related field
Proficiency in Python, SQL, Apache Airflow, and Docker
Experience with AWS or GCP and some Kubernetes experience
Strong analytical and problem-solving skills
Excellent communication and collaboration skills
Ability to work independently and as part of a team
Passion for writing clean, readable code and ensuring code quality

If you are passionate about data engineering and want to join a fast-paced, dynamic team that is making a real impact, we encourage you to apply today!.
Pura’s Story
At Pura, we’re pairing smart tech with premium fragrance to create a perfectly personalized and customized scenting experience for the individual. We partner with brands like Disney, Capri Blue, and Anthropologie to bring original and well-loved fragrances to homes in a modern, convenient, and safe way. We know we’ve only just begun to unlock the possibility of scent, and we’re excited for the opportunities that lie ahead.
We’re quickly turning heads and getting noticed. We raised a seed round of 4.4M in February of 2020, was recognized by Inc. Magazine as a 2021 Best Workplace, won the Silicon Slopes Hall of Fame & Awards Advertising category in 2022, and we’re currently the 6th-fastest growing company in Utah. Check out our Instagram @pura and TikTok @trypura channels for a look into the excited, engaged community we’re building. We pride ourselves on being a human brand and in creating a culture worth talking about, and we have big goals for the future.
Join the Pura Team!
All candidates are subject to a background check.
Pura provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

PSRu221guI
Show Less
Report",$39T - $58T (Glassdoor Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
48,"Infinity Quest
4.0",Sr Big Data/Data Engineer,Remote,"10 Experience
Job Details:
Must Have Skills
Big data
Informatica
Tableau
Detailed Job Description
Design the architecture of big data platformPerform and oversee tasks such as writing scripts, calling APIs, web scraping, and writing SQL queriesDesign and implement data stores that support the scalable processing and storage of our highfrequency dataMaintain our data pipelineCustomize and oversee integration tools, warehouses, databases, and analytical systemsConfigure and provide availability for dataaccess tools used by all data scientists
Job Type: Contract
Salary: From $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 10 years (Preferred)
Tableau: 10 years (Preferred)
Big Data Engineer: 10 years (Preferred)
Work Location: Remote
Speak with the employer
+91 8838059965
Show Less
Report",$65.00 Per hour(Employer Est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
49,Arlensa,"Data Engineer - Azure, Kafka, Snowflake","McKinney, TX","We are modernizing our Data Platform and are seeking a skilled Data Engineer to help maintain/improve existing ETL processes and build new ETL processes and Pipelines in Azure.
Our tech stack includes Azure Data Factory, Azure Data Bricks, SQL, Kafka, Snowflake, Mulesoft, Talend, Azure Analysis Services.
If you have previous work experience in these technologies, and are interested in the Financial Services industry, we'd love to hear from you.
WE CANNOT SPONSOR ANY VISAS
APPLICANTS MUST ALREADY LIVE IN THE NORTH DALLAS AREA
- NO REMOTE WORK OFFERED AND WE ARE NOT CONSIDERING THOSE NOT ALREADY LIVING LOCALLY
Job Type: Full-time
Pay: $120,000.00 - $165,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Ability to commute/relocate:
McKinney, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 2 years (Required)
Kafka: 2 years (Required)
Work Location: In person
Show Less
Report",$1L - $2L (Employer Est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
50,Findability Sciences,Snowflake Data Engineer pipeline,"Houston, TX","Snowflake data engineers will be responsible for architecting and implementing very large scale data intelligence solutions around Snowflake Data Warehouse.
A solid experience and understanding of architecting, designing and operationalization of large scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must.
Developing ETL pipelines in and out of data warehouse using combination of SQL and Snowflakes Snow SQL
Writing SQL queries against Snowflake.
Developing scripts Unix, Python etc. to do Extract, Load and Transform data
Provide production support for Data Warehouse issues such data load problems, transformation translation problems
Translate requirements for BI and Reporting to Database design and reporting design
Understanding data transformation and translation requirements and which tools to leverage to get the job done
Understanding data pipelines and modern ways of automating data pipeline using cloud based
Testing and clearly document implementations, so others can easily understand the requirements, implementation, and test conditions.
Job Types: Full-time, Part-time, Contract, Temporary
Salary: $42.14 - $70.29 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Night shift
Ability to commute/relocate:
Houston, TX 77001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 3 years (Preferred)
Work Location: One location
Show Less
Report",$42.14 - $70.29 Per hour(Employer Est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
51,FocuzMindz,"AWS Data Architect/ Engineer with Redshift, RDS","Alexandria, VA","We are seeking an AWS Data Engineer to join our growing team.
The qualified applicant will play a key role in the data warehouse migration as part of the Enterprise Data Analytic Services program at a federal agency located in Alexandria, VA. Hybrid work options are available.
AWS Data Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premises to cloud environment, evaluation, and optimization.
Full time opportunity
Alexandria VA
Hybrid role -onsite 2 days per week
Responsibilities:
Work on automating migration process for RDS and RedShift scripts in AWS from Dev to Production.
Performance tune RDS/RedShift SQL queries.
Maintain/resize the clusters for RDS and RedShift.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including computing, storage networking, database, management tools, security, identity, and compliance.
Good knowledge of RDS Postgres and AWS Redshift.
5+ years of experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Experience building infrastructure inside of AWS via code. Familiarity with tools such as Terraform or CloudFormation.
5+ years of experience architecting/deploying/operating solutions built on AWS.
Experience using ETL tools such as Alteryx, Snap Logic, Matillion, SAS DI Studio, Informatica, or equivalent tools.
Education Requirements:
Bachelor's degree in engineering, data science, computer science,
AWS and/or Data Science certification is a plus!
Clearance Requirements:
Ability to obtain and hold a Public Trust Clearance.
mary.a@stepstalent.com
Job Type: Contract
Pay: From $159,583.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Alexandria, VA 22301: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 9 years (Required)
AWS: 9 years (Required)
Security clearance:
Confidential (Required)
Work Location: Hybrid remote in Alexandria, VA 22301
Show Less
Report",$2L (Employer Est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
52,"Inteliquet, Inc
3.6",Data Integration Engineer - Remote,"Washington, DC","Are you passionate about driving successful implementations and data integration and to help Cancer Centers adopt a self-service data access and integrated patient matching solution? Then we want you to join our team!

At Inteliquet, we believe every patient is valuable and worthy of our very best efforts.
Our mission is to be a key connector at the intersections of clinical decision support, patient therapy, and therapeutic research and development. Envisioning a world where everyone has timely access to the best and longest-lasting therapies possible, we strive to remove barriers so that clinical trials and research work better for all stakeholders—especially the patient. Through insights, technology, and expertise we help solve one of the most difficult problems in healthcare—harnessing the data needed to quickly and accurately match patients to clinical trials and bring more trials to those patients. We focus on improving the way clinical trials work, so that trial access is available to every patient.
Job Title
Data Integration Engineer
Reports to:
Manager/Director Engineering-Integration
Functional Group:
Engineering
Location:
Remote - Company is 100% remote - always has been
Engagement:
Full-Time Position
Summary:
The Data Integration Engineer plays a vital role in ensuring outstanding project execution, delivery, and support in the implementation of Inteliquet’s software and data integration with Cancer Center oncology data sources. The Data Integration Engineer is responsible for executing project tasks, data management, data discovery, code implementations, data conversion, and on-going support of data ingestion.
Duties and Responsibilities:
This list is not comprehensive but meant to represent the most common or important duties of the position. Other duties are required and/or assigned.
Liaise with Deployment Project Management Lead on client related issues, opportunities and challenges
Participate in new system release testing and evaluation for impact to data integration efforts
Act as technical point of contact for customer integration/data product issues and involve the QA/Engineering team to debug, analyze, and fix software bugs.
Participate in data integration projects:
Develop knowledgebases, templates, and standards to ensure repeatability and quality.
Developing process workflows to increase automation and scalability.
Maintain and enforce best practices to facilitate optimized software implementation and ETL transfer of data.
Works closely with data QA and architects to ensure good data quality and sound data security.
Performing initial data validation before and after implementation.
Implement code necessary to develop data extraction, transformation and loading in SQL, SSIS, C# and other tools as necessary.
Converting and loading client data into the Inteliquet conformed data structures.
Initial integration and configuration of our software from a technical perspective.
Experience with MongoDB and/or Neo4J an advantage.
Serve as company representative to external clients. Inspire confidence in our organization and deliver results.
Contribute to the development of plans, the engagement of and integration of addition data sources with clients
Support transition efforts for clients migrating to a new EMR or other source system
Support Client Daily Needs:
Investigate and resolve data related issues.
Triage and resolve technical issues.
Minimum Requirements:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
4 or more years related application and systems development experience
Knowledge of data processing system, design methods, techniques and standards.
Detail-oriented and proficient in data analysis, with a keen interest in data manipulation and ability to see beyond the data and draw conclusions.
Candidates with experience in medical records, bioinformatics or analytic software will be advantaged. An in-depth knowledge of oncology will be a plus, but we don't expect you to cover all areas of expertise.
Ensure compliance with company policies
Review, understanding, and compliance with HIPAA Security policies and procedures.
Safeguarding the privacy and security of protected health information
Technical Proficiencies:
Fundamental understanding and knowledge of software development environments such as, but not limited to:
MS SQL Server: TSQL, SSIS, SSRS
Languages: PowerShell, C# (optional)
MongoDB (optional)
Neo4j (optional)
Source Code Control: TFS, DevOps
Agile experienced
MS Office: used for documentation
MS Teams
Essential Job Functions:
Critical features of this job are described below. They may be subject to change at any time due to reasonable accommodation or other reasons.
Soft-skills: Must be able to effectively communicate with others; complete and understand complex analysis of numbers; read, analyze and interpret written materials; develop team to meet company standards; ensure compliance with company policies; respond appropriately to feedback to make improvements; maintain positive working relationships; troubleshoot and solve problems.
Physical: Must be able to hear and verbally communicate for hours at a time, use computer equipment. Moderate noise level and limited exposure to physical risk.
Knowledge, Skills, and Abilities Required:
Knowledge of modern business communication, office procedures and methods. Skill to use a personal computer and various software packages such as Microsoft Office Suite. Ability to establish priorities, work independently with minimal supervision, and facilitate teamwork.
Equipment Used:
Headsets, telephones, computer, other office equipment as needed.
Show Less
Report",$77T - $1L (Glassdoor Est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
53,"Mashvisor Inc.
3.7",Data Science Engineer,Remote,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?
Show Less
Report",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,2015,Unknown / Non-Applicable
54,"Narvee Tech
4.1",AWS Data Engineer,Remote,"Its an W2 Position (NO C2C)
Role – AWS Data Engineer
Location – Onisite(NY/TX/NJ)
Need 8-10+ Yrs.
Job Description
Responsibilities:
The Data Engineer is responsible for designing and developing robust, scalable solutions for collecting, analyzing large data sets, creating and maintaining data pipelines, data structures and reports.
Understand business processes, applications and how data is gathered, and tie application telemetry to transactional data model.
Build and manage data marts to satisfy our growing data needs.
Develop and manage data pipelines at enterprise scale
Build data expertise and own data quality for various data flows
Use your coding skills across several languages like SQL and Python to support analysts and data scientists
Interface with internal data consumers to understand data needs
Requirements of the Data Engineer:
Bachelor's degree in Computer Science or Engineering
12+ years of hands-on experience in architecting, crafting, and developing highly scalable distributed data processing systems
10+ years of experience in Data Warehousing, Data mart concepts & implementations especially in relational databases like Oracle, SQL Server, Netezza, Snowflake
10+ years of experience in ETL technologies especially in Informatica
8+ years of experience in SQL, PL/SQL, and performance tuning
5+ years of experience in modern object-oriented programming languages like Python/Spark
2+ years of experience in Snowflake & AWS, cloud technologies
Solid understanding of the SDLC: analysis, design, coding, system and user testing, problem resolution, and planning
Experience with shell scripting, scheduling tool and willing to participate on weekend on call rotation is a must
Benefits:
· H1B Visa sponsorship and assistance in getting CPT, OPT extension.
· STEM extension is done as we are e-verified.
· Green Card sponsorship for qualified candidates
· Attractive Pay Scales · Health Insurance Cover Provided
· 100% Guaranteed successful placement
· We will file H1B whenever it's required
Interested share resume at sara(at)narveetech.com
contact: 614-810-0188
Job Type: Contract
Pay: $40.00 - $60.00 per hour
Work Location: Remote
Show Less
Report",$40.00 - $60.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
55,"Intuceo
3.9",Azure Data Engineer,Remote,"Job Description: Looking for US Citizens only for Federal Client
Bachelor’s degree in science, engineering or technology.
Minimum 8+ years’ experience in designing, delivering and supporting IT solutions.
Experience in Big data analytics projects and products, business processes and architecture.
Experience in Azure Services like ADF, ADLS, Synapse, Databricks, Azure Monitoring.
Experience in Creating Data Pipelines using Apache Hive, Apache Spark, Apache Kafka.
Experience in Programming languages like Python/Java, ETL is required
Experience working with huge datasets, SQL/NO SQL exposure
Azure Data Engineering Certification is plus
Experience in the Insurance industry is preferred.
Please share the resume and reach out to me on 9042041368
Job Type: Full-time
Salary: Up to $180,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Azure Data Engineer: 10 years (Preferred)
SQL: 5 years (Preferred)
Azure: 3 years (Preferred)
License/Certification:
Azure Data Engineer (Preferred)
Work Location: Remote
Show Less
Report",$2L (Employer Est.),51 to 200 Employees,Unknown,#N/A,#N/A,#N/A,Unknown / Non-Applicable
56,"Ascendion
4.5",Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour(Employer Est.),1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
57,"GE Healthcare
4.2",Staff Data & Information Engineer - MRI,"Waukesha, WI","Job Description Summary
The Staff Data & Information Engineer is responsible for supporting the MRI Engineering and Product teams across segments by developing data structures and pipelines to support analytics. In this role you will contribute to the development and deployment of modern machine learning, operational research, and statistical methods on parametric, system log, and device history datasets both on the MRI machine and in the cloud.

At GE Healthcare, our passionate people are creating the products, solutions and services our customers need to deliver the best patient care possible.
Job Description
Essential Responsibilities
Provide architectural expertise to horizontal teams across the MRI Engineering team.
Design, build, and maintain data logging structures on the MRI scanner
Design, build, and maintain data pipelines from a data lake to data exploration platform for engineering use
Collaborate with Software Engineers, Hardware Engineers, Systems Engineers and Product Managers to identify and address gaps in MR system monitoring.
Collaborate and learn from peers across GEHC to ensure we leverage best practice and reduce duplicative work.
Work with business and technical leaders to prioritize data needs and define new process improvement opportunities.
Qualifications/Requirements
Bachelor’s degree in Computer Science or “STEM” Majors (Science, Technology, Engineering, and Math) with 6+ years relevant experience
6+ years of experience in Big Data, data warehouse, data modeling, and/or data analytics projects
Strong knowledge of C++ and Python
Experience with extracting and transforming data from large databases (SQL, NoSQL, Elasticsearch, S3, etc.)
Demonstrated skill in data cleaning, data quality assessment, and using analytics for data assessment
Desired Characteristics
Strong knowledge of Cloud solutions (AWS)
Strong knowledge of microservices, REST, Docker, Kafka
Strong knowledge of ETL processes and relevant tech stack to build and maintain complex queries, streaming, and real-time data pipelines.
Previous experience building data architectures and logging services
Experience with machine data (IoT) logging and analytics
Background in health care, pharma, or diagnostic imaging industry
Knowledge of GE HealthCare MR Products
We expect all employees to live and breathe our behaviors: to act with humility and build trust, lead with transparency, deliver with focus and drive ownership – always with unyielding integrity.

Our total rewards are designed to unlock your ambition by giving you the boost and flexibility you need to turn your ideas into world-changing realities. Our salary and benefits are everything you’d expect from an organization with global strength and scale, and you’ll be surrounded by career opportunities in a culture that fosters care, collaboration and support.

#gehealthcare
#LI-EJ1

While GE Healthcare does not currently require U.S. employees to be vaccinated against COVID-19, some GE Healthcare customers have vaccination mandates that may apply to certain GE Healthcare employees.
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: Yes
Apply Now: click Apply Now
Show Less
Report",$92T - $1L (Glassdoor Est.),5001 to 10000 Employees,Company - Public,Healthcare,Healthcare Services & Hospitals,1892,$10+ billion (USD)
58,"Norwin
2.0",Data Engineer,"South San Francisco, CA","Title: Data Engineer or Data Scientist
Location: South San Francisco, CA 94080(Candidate need to be local and be onsite 3 days a week)
Duration: 6+months(W2 Contract)
Client: Genentech
Description
The systems specialist role will be part of the Workplace Digital Insights team which supports making key data-driven decisions for the Real Estate and Workplace Effectiveness(RE&WE) organization. We help this organization’s forward planning strategies for South San Francisco ranging from developing real estate strategies, to campus planning, building investments, occupancy planning and move management.
Our team designs, builds, integrates and operationalizes technology applications that are customized to meet those organizational business needs. We are responsible for the end to end lifecycle of key applications and data streams that feed our Analytics team and Business Operations teams with high quality, well defined data. These data and data streams are critical to those applications and reporting platforms. Important business data originates from real time sensors, Wi-Fi, other databases, and people, then pipelines into storage, validation and then it is combined with other data in cloud storage, processed by software applications, and ends in visualization and decision support.
This specialized role will have to use a wide range of skills to understand business requests, identify the problem and break it into their business process, functional and system components. A clear and critical thinker is required to then bring new potential solutions to fruition, manage and analyze existing pipelines and data.
This role presents a unique opportunity to work with systems developed and maintained by the group as well as within corporate IT environments, multiple vendor systems, our AWS data lake, and all the pipelines in between. This includes working with data from various Internet of Things (IOT) networks and a great opportunity to collaborate and develop new streams and strategies on our horizon. The position must be filled with someone who has sharp critical thinking, combined with excellent project management and coordination skills, having a strong ability to work with both people representing the business people and those supporting roles within the IT organization. Technical skills are also extremely important, especially skills that support understanding SQL queries, database concepts, API’s, AWS/Cloud services, and developing Tableau as well. You’ll be responsible for understanding how real time data from sensors integrates with corporate systems, combined to cloud based data lake, funnels into data science analytics and will end in visualizations.
Responsibilities
● Understands and writes SQL code in Oracle and Redshift databases to support existing data structures, joins, views. Write and optimize new SQL to support new storage needs, reporting needs, and better performance and efficiency
● Makes decisions and solves problems using sound, inclusive reasoning and judgment. Gathers and analyzes information to fully understand a problem and proactively anticipates needs and prioritize action steps.
● Partners and communicates well with internal stakeholders, business partners, and Global IT to understand, define and help deliver technology needs
● Implements vetted strategies developed by the business to achieve technical roadmaps
● Manages project delivery end-to-end, in other cases helping our IT partner stay on project delivery milestones
● Wrangling structured, unstructured and poorly structured data into appropriate data structures.
● Auditing data in databases and/or reports with a goal of improving data quality from systems or data owners
● Identify opportunities to further build out our IoT strategy
● Develops new and supports existing Tableau dashboard platform, including how to improve sql queries and data management for optimal performance in reporting
Experience/Skills/Abilities:
● At least 3 years of experience working as a Systems Integrator, Data Engineer, Software Engineer or similar position demonstrating the ability to design and implement automation, data modeling, data wrangling, data analysis, and data vision solutions to complex problems, processes, and scenarios. Familiarity with common data structures and languages.
● BS Computer Science, Computer/Electrical Engineering, or Math Degree or relevant experience.
● Experience with IoT, cloud computing, distributed data systems
● Strong capabilities in SQL and Tableau, Excel/Google Suite
● Understands AWS platform (Amazon Redshift, S3, EC2, Glue jobs, etc.)
● Friendly and approachable, with strong communication and presentation skills
● Desire to keep current with a challenging and evolving environment
● Team focused and self-motivated. Able to work as part of a coordinated team, yet independently when necessary
● Proven abilities to take initiative and to be innovative; have an analytical mind with a problem-solving aptitude
● Demonstrated ability in leading technology projects
● Demonstrated experience in bridging business requirements and technical development
● Strong communications skills maintaining ties to product developers and stakeholders
● Demonstrated experience with software lifecycle management
Desirable Experience (but not required):
● Experience working with statistical teams and/or data scientists
● Tools/Programming Experience: Python, R, web services, other languages(e.g. Java, C++, scripting languages)
● AWS Certifications and working experience
● Experience with LEAN/KANBAN/SCRUM development methodologies is desirable
Job Type: Contract
Salary: Up to $90.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 4 years (Required)
SQL: 4 years (Required)
Tableau: 2 years (Required)
AWS: 2 years (Preferred)
Work Location: One location
Show Less
Report",$90.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,Manufacturing,Machinery Manufacturing,#N/A,Unknown / Non-Applicable
59,"Whiz Global LLC
4.0",AWS Data Engineer,"New York, NY","Role: AWS Data Engineer
Duration :Long Term
Location: Lafayette, Louisiana, Connecticut, New York (Day 1 Onsite)
Duties and responsibilities:
1. Candidates would develop applications in AWS - data and analytics technologies including but not limited to Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM.
2. Candidates would need to do development using application development by lifecycles, & continuous integration/deployment practices.
3. Working to integrate open source components into data-analytic solutions.
4. Working with vendors to enhance tool capabilities to meet enterprise needs.
5. Willingness to continuously learn & share learnings with others
Required qualifications to be successful in this role:
- Candidate should have Hands-on experience with AWS services such as Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, and IAM.
- Proficient in at least one programming language (Python, Scala, Java etc).
- Experience in ETL / Data application development and version control systems such as Git.
- Knowledge of application development lifecycles & continuous integration/deployment practices.
- 5 - 7 years experience delivering and operating large scale, highly visible distributed systems.
- Knowledge of IAC using terraform is preferred.
- Snowflake MPP and graph database experience is preferred but not mandatory.
Skill Set Years of Experience Proficiency LevelAWS services such as Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM2+, ExcellentPython/Scala/Java etc Programming
3+, ExcellentTerraform1+, GoodSQL– Simple to Complex SQL analysis
2+, Excellent
Minimum Education Required :
Bachelor's degree in Computer Science or a related discipline, at least eight, typically ten or more years of solid, diverse work experience in IT with a minimum of eight years' experience application program development, or the equivalent in education and work experience.
Job Type: Contract
Salary: $55.00 - $65.00 per hour
Schedule:
8 hour shift
Experience:
AWS: 5 years (Required)
SQL: 3 years (Preferred)
Python: 3 years (Preferred)
Work Location: One location
Show Less
Report",$55.00 - $65.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
60,"Tekrek solutions Inc
1.0",Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
61,"Barracuda Networks Inc.
3.8",Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L (Glassdoor Est.),1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
62,"Arthur Grand Technologies Inc
4.8",Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L (Glassdoor Est.),1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
63,"AGM Tech Solutions, LLC
4.8",Data Engineer,"Alpharetta, GA","We have an excellent 6-month contract-to-hire opportunity with our Global Leader client.
Candidate must be local to Alpharetta GA (Hybrid Flexibility) - 3 days a week on-site.
Basic
Work experience with ETL, Data Modeling, and Data Architecture.
Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL.
Experience operating very large data warehouses or data lakes.
Preferred
Experience in designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Comfortable using PySpark APIs to perform advanced data transformations.
Familiarity with implementing classes with Python.
Summary
Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue
Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python.
Design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Job Type: Contract
Pay: $70.00 per hour
Benefits:
401(k)
Flexible schedule
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Machine learning: 2 years (Required)
Work Location: One location
Show Less
Report",$70.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
64,"Zillion Technologies
3.8",Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour(Employer Est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
65,"Vorys, Sater, Seymour and Pease LLP.
4.1",Senior Data Engineer,"Akron, OH","Precision eControl is all about helping brands better manage their eCommerce presence on Amazon and other online marketplaces. As a Senior Data Engineer, you will be part of a team that creates innovative, cutting edge, one of a kind solutions that are revolutionizing how brands manage online marketplace sales.
Position Summary
The Senior Data Engineer for the Azure infrastructure will be responsible for the day to day operations of a large data warehouse, and will work closely with the business, product team, and the technical staff to ensure alignment to goals and objectives. Utilizing experience with Big Data, this position will drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.
Essential Job Functions
Drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.
Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Design, implement, and document data load processes from disparate data sources into Azure Synapse Pipelines.
Work with Continuous Integration/Delivery using Azure DevOps and Github.
Provide data management, monitoring, troubleshooting and support to client success
Create various triggers to automate the pipeline in Azure Synapse Analytics Pipelines.
Tune SQL queries in Azure SQL DB, Azure Synapse and solve complex data challenges and deliver insights that help our customers achieve their goals.
Self-organize as part of a small-size scrum team and apply data engineering skills.
Follows industry best practices and meets company’s security and performance and requirements
Knowledge, Skills and Abilities
Minimum three (3) years’ experience with MS SQL/T-SQL
Minimum three (3) years’ experience with Azure SQL
Minimum three (3) years’ experience with Apache Spark (PySpark)
Minimum of three (3) years’ experience with Azure Data Factory or Azure Synapse building dynamic ETL pipelines
Minimum three (3) years' experience building dynamic Spark notebooks in Azure Synapse Spark or Azure Databricks
Minimum three (3) years’ experience with Python
Minimum two (2) years’ experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
Experience working with parquet, json , delta, avro and csv files
In-depth understanding of data management (e.g. permissions, recovery, security and monitoring)
Experience with Data Warehouse Architecture
Strong analytic skills related to working with structured, semi-structured and unstructured datasets.
Excellent analytical and organization skills required
Ability to understand user requirements
Client service mindset
Excellent verbal and written communication skills
Excellent problem solving skills
Familiarity with Agile frameworks a plus
Education and Experience
Bachelor's degree in related discipline or combination of equivalent education and experience
7-10 years of experience in similar field
Precision eControl LLC does not discriminate in hiring or terms and conditions of employment because of an individual’s sex, race, age, religion, national origin, ancestry, color, sexual orientation, gender identity, genetic information, marital status, military/veteran status, disability or any other characteristic protected by local, state or federal law. Precision eControl only hires individuals authorized for employment in the United States.
Show Less
Report",$87T - $1L (Glassdoor Est.),501 to 1000 Employees,Company - Private,Legal,Legal,1909,Unknown / Non-Applicable
66,"APLOMB Technologies
4.4",Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
67,"Gopuff
3.0",Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L (Glassdoor Est.),5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
68,"Numentica LLC
4.3",AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L (Glassdoor Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
69,"Innova Solutions Inc.
3.9",Data Engineer/Data Analyst,"Richmond, VA","Position Summary
The person will have a mix of highly technical data quality controls development, data analysis and reporting responsibilities to include writing complex SQL queries, some python code analysis, extensive data analysis, building DQ controls metrics reports, defining tech data controls strategy, working with metadata, architecture and development teams on the resolution to DQ controls issues
Primary Skill
MySQL
Secondary Skill
Tertiary Skill
Required Skills
SQL and DQ Controls Experience
DQ Controls Development and Strategy Skills
Business Analyst
Data Architecture
Desired Skills
Python, metadata, business intelligence reporting, BA
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richmond, VA 23173: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
MySQL: 5 years (Required)
Data Quality: 5 years (Required)
Work Location: Hybrid remote in Richmond, VA 23173
Show Less
Report",$60.00 - $65.00 Per hour(Employer Est.),10000+ Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1998,$2 to $5 billion (USD)
70,"Gridiron IT
4.4",Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
71,"Xiar tech inc
3.3",Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour(Employer Est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
72,DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour(Employer Est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
73,"InfoQuest Consulting Group Inc.
4.5",Data Engineer,"Philadelphia, PA","Duration & Type: 12 months Contract with a media & communications industry client
Location: Philadelphia, PA
No. of Positions: Multiple
Responsibilities:
Develop solutions to big data problems utilizing common tools found in the ecosystem.
Develop solutions to real-time and offline event collecting from various systems.
Develop, maintain, and perform analysis within a real-time architecture supporting large amounts of data from various sources.
Analyze massive amounts of data and help drive prototype ideas for new tools and products.
Design, build and support APIs and services that are exposed to other internal teams
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Required:
5+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in one or more languages: Python, Scala/Java, Spark, Batch, Streaming, ML
Experience with Python unit testing and code coverage frameworks
Experienced in NoSQL / SQL, Microservice, RESTful API development
Strong Experience with AWS Core such as Kinesis, Lambda, API Gateway, CloudFormation, CloudWatch
Experienced with one of the Analytics tools – Presto / Athena, QuickSight, Tableau
Strong Experience with Container technologies and Real-time Streaming (such as Kafka, Kinesis)
Preferred:
Test-driven development/test automation, continuous integration, and deployment automation experience
Experience with Performance tuning at scale
Experience working on big data platforms in the cloud or on traditional Hadoop platforms
Experience working in agile/iterative development and delivery environments
Enjoy working with data – data analysis, data quality, reporting, and visualization
Great design and problem solving skills, with a strong bias for architecting at scale
Excellent communication skills
Experience in software development of large-scale distributed systems
For consideration, please send resume to career@infoquestgroup.com
Show Less
Report",$89T - $1L (Glassdoor Est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
74,"princeton it services
4.2",Data Engineer lead,"Boston, MA","Data Engineer Lead
Job Description:
Position: Data Engineer Lead
Location: Raleigh, NC or Boston, MA
Job Length: Long term
Position Type: C2C/W2
Qualifications:
9+ years Experience in Alation, Collibra, Snowflake
9+ years Experience in Java , Spring boot , spark , Scala.
Stays current with technology trends in order to provide best options for solutions • Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently.
Responsibilities:
Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data management and Stewardship using Collibra.Alation
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required :Alation, Collibra, Snowflake
Job Type: Contract
Salary: From $65.00 per hour
Schedule:
8 hour shift
Experience:
collibra: 5 years (Preferred)
snowflake: 5 years (Preferred)
aliation: 4 years (Preferred)
Work Location: On the road
Speak with the employer
+91 6093006906
Show Less
Report",$65.00 Per hour(Employer Est.),1 to 50 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2008,$1 to $5 million (USD)
75,"ConnectiveRx
3.0",Sr. Data Engineer,"Hanover, NJ","ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.
Show Less
Report",$96T - $1L (Glassdoor Est.),1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable
76,"etrailer.com
4.0",Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L (Employer Est.),501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
77,"Alianza, Inc.
4.4",Data Engineer,"Pleasant Grove, UT","Data Engineer
Alianza is looking for an experienced and results driven Data Engineer. The successful candidate will be the technical engine of the data team, building Python applications to ingest streaming and extracted data and persist to cloud storage. Will use Python and SQL with AWS cloud technologies to automate the generation and delivery of reports. Will utilize CI/CD technologies to fully automate the release of all compute and storage components to the cloud. Work with our data architect and Java developers to design creative, high-quality, data-oriented insights and dashboards. Significant focus of the position will be on streaming data pipelines, distributed datalake architectures, and AWS services. Question the status quo. Write clean, testable, resilient code. Make things go fast and have fun doing it!
Key Duties and Responsibilities:
Participate in the process of designing, data engineering, and developing data services (Streaming, ETL/ELT, Real-time analytics, Reporting) using Python, SQL and AWS services
Adhere to modern methodologies for designing, coding, and testing
Build connected, fully automated data systems and pipeline
Work effectively with remote teams in various remote time zones
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources into usable format
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Identify and implement automatable tasks and reusable frameworks
Participate in sprint planning meetings and provide reasonable estimations
Research and propose new process, techniques, or tools as solutions. Able to produce technical diagrams, explanations, and written documentation to promote proposed solutions
Collaborate with data team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements

Qualifications:
3+ years of Python development experience, preferably writing modules that implement part of a streaming or batch ETL system in a cloud hosted environment
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse / lake designs to stakeholders
Experience designing, building, and maintaining data processing systems
At least 3 years’ experience with modern DevOps automation ecosystems, preferably Git/GitHub/Bitbucket, Buildkite or Ansible (or similar)
Real-world experience handling large data volumes (terabytes of storage and billions of rows)
At least 3 years experience configuring, using, and performance tuning AWS cloud services:preferably S3, Glue, Athena, Kinesis, Firehose, Lambda, Cloudwatch, ECS, API Gateway, RDS (Postgres), SQS, SNS, SES
Experience using AWS Redshift (or similar)
Experience with CloudFormation or TerraForm
Ability to prioritize, learn quickly, and do high-quality work
Demonstrate understanding of modern APIs and endpoints, like REST and GraphQL
Working understanding of Agile dev methodologies, especially Scrum and Kanban
Good listener, communicator, collaborator, and documenter
Proficient with Linux and shell scripting
Experience with data warehouse, data mart, OLAP, dimensional modeling, Kimball method
Good understanding of relational and document database concepts and best practices
Know how to design a clean, performance-optimized relational data model, and reverse engineer existing databases into physical data model diagrams
Experience using C*, Spark, Kafka, KSQL, Confluent, Pulsar and/or Kinesis helpful
Automated testing experience using JUNIT or equivalent
Some experience in software engineering (front, middle, back or all three) and application architecture
Show Less
Report",$73T - $1L (Glassdoor Est.),201 to 500 Employees,Company - Private,Information Technology,Software Development,2009,$25 to $50 million (USD)
78,"Radcube LLC
3.6",Senior Data Engineer,"Cincinnati, OH","Bachelor's degree in Computer Science/Information Systems Understanding of Object-Oriented Programming Languages Understanding of Software Development Lifecyle Strong SQL Skills with ability to perform ETL Familiarity with relational database architecture techniques like EDW, IBM DB2, etc Demonstrated practice for scripting languages, like Python, Java, Powershell Strong Windows Server Experience and/or Application Support Experience Prior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, &amp; Git is a plus Understanding of Agile Software Development methodologies Understanding of data management and info security best practices Demonstrated problem solving skills Demonstrated collaboration skills Excellent verbal and written communication skills
MINIMUM KNOWLEDGE, SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems
Understanding of Object-Oriented Programming Languages
Understanding of Software Development Lifecyle
Strong SQL Skills with ability to perform ETL
Familiarity with relational database architecture techniques like EDW, IBM DB2, etc
Demonstrated practice for scripting languages, like Python, Java, Powershell
Strong Windows Server Experience and/or Application Support Experience
Prior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, & Git is a plus
Understanding of Agile Software Development methodologies
Understanding of data management and info security best practices
Demonstrated problem solving skills
Demonstrated collaboration skills
Excellent verbal and written communication skills
Job Types: Full-time, Contract
Pay: $79,805.61 - $120,633.12 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Cincinnati, OH: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 5 years (Preferred)
Niffi: 4 years (Preferred)
kafka: 3 years (Preferred)
Work Location: In person
Show Less
Report",$80T - $1L (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
79,"Skill Quotient
3.8",SAS/ Data Engineer,"Plano, TX","Job Title: SAS/ Data Engineer
Location: Plano, TX (Hybrid role)
Local candidates required. Will be on site 1 - 2 days per week. Will consider candidates that are committed to relocation from day 1 (EST or CST), nearby states are best for this.
Client: Bank/Financial Services
Duration: 12/29/23, Likely to extend
Required Skills: Please do not submit candidates that do not have these requirements or are not local at this time. If you have someone able to relocate prior to day 1 that is exceedingly good I may be able to consider after some time.
5+ years of experience with SAS Configuration and Integration.
2+ years of experience with Prism (Workday) AND / OR Adenza regulatory reporting tools REQUIRED. (Both would be preferred)
Banking / Financial Services background.
Regulatory reporting and/or Data/Big Data experience (should go along with the Prism and/or Adenza experience).
Agile experience is a big plus
Data / Big Data / SAS Engineer (Regulatory Reporting Project)
Will be part of a new team working on a regulation reporting project (100B Asset Project)
At 100B in assets more governmental / regulatory reporting is required and the client is preparing for this.
Seeking a Data / Big Data / SAS Engineer with some reporting experience.
2 rounds of interviews. Can be local to Detroit area (Auburn Hills, MI) or Dallas area (Plano, TX).
Qualified candidates will have Engineering background or possibly Prism Developers.
Job Type: Contract
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Auburn Hills, MI 48321: Reliably commute or planning to relocate before starting work (Required)
Experience:
SAS: 5 years (Required)
Prism workday: 3 years (Required)
Adenza: 3 years (Required)
Work Location: One location
Show Less
Report",$77T - $1L (Glassdoor Est.),1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2016,Unknown / Non-Applicable
80,"Violet Ink
3.9",Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour(Employer Est.),1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
81,"element technologies
3.9",Senior Data Engineer,"Brooklyn, NY","8+ years of experience in writing SQL.
8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl
Job Types: Full-time, Contract
Salary: $100.00 - $110.00 per hour
Schedule:
Monday to Friday
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
ETL: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$100.00 - $110.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
82,"Mashvisor Inc.
3.7",Data Science Engineer,Remote,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?
Show Less
Report",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,2015,Unknown / Non-Applicable
83,"Ascendion
4.5",Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour(Employer Est.),1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
84,"Reebelo
3.3",Senior Data Engineer (US),"San Francisco, CA","Create The Circular Economy With Us
At Reebelo.com, we empower customers to buy their favourite tech devices in a more sustainable way. Our mission is to refresh the way we all consume tech, delivered through a platform built on sustainable values and quality-assured devices. We are looking for motivated team members like yourself with an innovative mindset. In 2 years we launched 7 countries, raised $21M from top investors and scaled to 8-digit gross sales. Change the world, and supercharge your career with Reebelo!

The Engineering team is focused on building a state-of-the-art web experience for both our customers and vendors. As a Senior Data Engineer in the data platform team, you will have the opportunity to build, optimize and grow the core data platforms in the company. Your work will have a direct and huge impact on the company's core competencies.

Your typical day may include:
Building and maintaining data pipelines to ensure correct data is transferred across platforms
Empowering the external facing data tool by engineering a robust data repository
Working closely with the stakeholders, including the engineering team to ensure accurate release of features
Owning and maintaining the complete infrastructure by ensuring near 100% uptime and proper disaster recovery practices, including but not limited to version control and configuration management

We’d love to have a chat with you if you have:
BS degree in Computer Science/Engineering
5+ years of experience in data engineering
Strong experience in pioneering, scaling, and optimizing data pipelines, data storage, large-scale data processing/analytics platform, dashboards
Expert level in Python, REST APIs
Proven experience in version control and CI/CD tools
In-depth, hands-on experience in AWS cloud infrastructure
Excellent verbal and written communications skills
How we take care of you:
Birthday leave
Competitive salary
Hybrid work environment
Ownership of your own projects
Team events & a great culture!

We understand that experience comes in many forms so if your experience is close to what we’re looking for, please don’t hesitate to apply — we’d love to hear from you!
Show Less
Report",$1L - $2L (Glassdoor Est.),51 to 200 Employees,Company - Private,Retail & Wholesale,Consumer Electronics & Appliances Stores,2019,$100 to $500 million (USD)
85,"Recruiting From Scratch
3.9",Data Engineer,Remote,"Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
From Our Client
Location: New York / Remote
Job Type: Full-time
Experience: 3+ years
About the role
Our mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability. We are growing very fast - in less than five months, we grew to millions in card volume. We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We have raised over $30M from top-tier fintech investors.
WHAT YOU'LL BE WORKING ON
You'll be involved in projects of varying scope and complexity:
Build credit risk models that segment merchants based on their revenue and spend patterns to offer dynamic credit limits that change with business performance given the high seasonality and fast pace of ecommerce
Use machine learning tools to build realtime credit underwriting models leveraging alternative and traditional data sources
WHAT YOU'LL NEED
Passion for, or curiosity to learn, financial technology
Track record of high-quality shipping products and features at scale
Ability to turn business and product ideas into engineering solutions
Desire to work in a fast-paced environment, continuously grow and master your craft




WHAT WE’D LIKE TO SEE
Experience with building out data pipelines (e.g. should know data lakes, data warehouses, ETL, etc.)
Experience working with data analytics, algorithmic decision making, and real-time data systems
Experience with different business requirements on data freshness and retention
PLUSES
Proven experience and subject matter expertise in e-commerce payments (nice to have) or financial services.
Has worked with bank account data (e.g. worked with Plaid), payments data (perhaps Stripe), and/or other fintech data sources




Why you should join us:
Our mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability. We are growing very fast - in less than five months, we have grown to millions in card volume.
We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We recently closed a financing round of $5M from a top-tier fintech investor.
Salary Range: $150,000-$250,000 base.
Show Less
Report",$2L - $2L (Employer Est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2019,$1 to $5 million (USD)
86,"Plaxonic
4.6",Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
87,"Orion Innovation
3.8",Data Engineer-Databricks,"Edison, NJ","Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.
Job Description

Designing and implementing highly performant data pipelines from multiple sources using Databricks
Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
Working with other members of the project team to support delivery of additional project components (API interfaces)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Integrate Databricks with other technologies (Ingestion tools, Visualization tools)

Knowledge, Skills, and Abilities:
Proven experience working as a data engineer
Highly proficient in using the spark framework (python and/or scala)
Extensive knowledge of Data Warehousing concepts, strategies, methodologies.
Programming experience in Scala or Python, SQL
Direct experience of building data pipelines using Apache Spark (preferably in Databricks).
Hands on experience designing and delivering solutions using Azure, including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake/ADLS Gen)
Experience with ingestion tools (nifi/azure cloud factory
Experience with big data technologies (hadoop)
Experience with Databricks
Must be team oriented with strong collaboration, prioritization, and adaptability skills required
Skills required:
Data Lake/ ADLS Gen 2
Databricks
Scala, Python, PySpark

Orion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Candidate Privacy Policy
Orion Systems Integrators, LLC and its subsidiaries and its affiliates (collectively, ""Orion,"" ""we"" or ""us"") are committed to protecting your privacy. This Candidate Privacy Policy (orioninc.com) (""Notice"") explains:
What information we collect during our application and recruitment process and why we collect it;
How we handle that information; and
How to access and update that information.
Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.
Show Less
Report",$59T - $88T (Glassdoor Est.),5001 to 10000 Employees,Company - Private,Information Technology,Information Technology Support Services,1993,$100 to $500 million (USD)
88,"Umanist Staffing
5.0",Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L (Employer Est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
89,"Impact Advisors LLC
4.7",Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
90,"infinity quest
4.0",DATA ENGINEER,"Seattle, WA","At least 3 years of Data Engineer experience is required preferably in a cloud Environment.
You should have at least 4 years of coding experience in python/java/ Scala and open source packages with at least 2 years of experience with Databases(SQL/NOSQL etc).
Experience with large scale Distributed databases like redshift/Snowflake is a big Plus.
You should have Experience with different aspects of data systems including database design, data modeling, performance optimization, SQL etc.
Some Experience with building data pipelines and Orchestration(Airflow ,ADF,glue etc) is required.
Strong communication skills (able to explain concepts to non-technical audiences as well as peers)
Self-starter who is highly organized, communicative, quick learner, and team-oriented
Technology Requirements:
Python/Java or Scala , SQL and Airflow. Cloud experience AWS/Azure
Daily tasks:
Developing, executing, monitoring and troubleshooting Data pipelines and workflows in our cloud environment.
Work on Data Lake/DW/DQ and other framework related items
Team and cross functional collaboration as needed.
Preferred background/prior work experience:
3 years of DE expertise building data pipelines and working in a DW/Data lake Cloud based environment
Job Type: Contract
Salary: $65.00 per hour
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
Day shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road
Show Less
Report",$65.00 Per hour(Employer Est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
91,"Tekrek solutions Inc
1.0",Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
92,"Numentica LLC
4.3",AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L (Glassdoor Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
93,"Zillion Technologies
3.8",Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour(Employer Est.),201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
94,"APLOMB Technologies
4.4",Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
95,"Arthur Grand Technologies Inc
4.8",Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L (Glassdoor Est.),1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
96,"Violet Ink
3.9",Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour(Employer Est.),1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
97,"Gridiron IT
4.4",Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
98,"Impact Advisors LLC
4.7",Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
99,"ConnectiveRx
3.0",Sr. Data Engineer,"Hanover, NJ","ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.
Show Less
Report",$96T - $1L (Glassdoor Est.),1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable
100,"Xiar tech inc
3.3",Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour(Employer Est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
101,"Kreg Tool Company
3.6",Data Engineer / Senior Developer,"Ankeny, IA","Join us at Kreg Tool!

Kreg Tool is proud to be an employee-owned company that manufactures a wide variety of tools, accessories, and equipment for woodworkers and do-it-yourselfers. Guided by a unique purpose and values, we are driven to provide quality products to customers, are fully committed to employees, and are dedicated to the community. We aspire to give employees the tools they need to achieve personal growth and success throughout their career. Each employee is a valuable member of our team, all working together to co-create our futures by serving our customers, community, and one another.
Headquartered in a beautiful new campus in Ankeny, Iowa (just off I-35 north of Des Moines), Kreg creates products that are available throughout North America in the nation’s major home improvement stores, such as Lowe’s, Home Depot, and Menards. We recognize the importance of work-life integration and the health of our employees and acknowledge this through a flexible schedule and competitive benefits package. Beyond medical, vision, and dental benefits, Kreg also offers a “dress for your day” dress code, onsite fitness center, annual bonus opportunity, and volunteer time off, as well as paid parental leave, and tuition and wellness reimbursement.
Data Engineer / Senior Developer
The Data Engineer plays a key role in the execution of Kreg’s data and software engineering team. This person serves as a subject matter expert that can translate key objectives into solutions that drive continuous improvement within the organization. This role will collaborate both within the IT team and cross functionally. To be successful in this role, the right candidate needs to be agile and adaptable.
So, What’s the Gig?
Develop, deliver, and manage robust and high performing ETL/ELT flows and queries that support data movement in data warehouses and between enterprise systems.
Create, maintain, and optimize SQL/Python queries and functions that support data movement.
Coordinate activities with data source owners to capture data integration requirements, conceptualize solutions & build the required data-related technology stack. Ensures quality and accuracy.
Strategize with Company Leadership on the architecture and modernization of legacy integrations and ETL flows.
Assist team members in building, testing, and converting data to modern architecture.
Respond to user ad-hoc requests for data, advising and assisting with options for providing relevant information.
Collaborate with team members to translate best practice information and architecture mockups with user interface design.
Provide ongoing operational support to existing and newly developed processes and systems as necessary.
Support manufacturing and distribution center’s ERP, WMS, and related software systems.
Monitor various system integrations and resolve/maintain transactions as necessary.
Assist with project oversight from design through implementation including identifying, tracking, monitoring, and communicating project-related issues, scope changes, variances, and contingencies that occur during projects.
Write, read, and troubleshoot SQL queries, stored procedures, views, and functions.
Code specifications by converting logical sequence and workflow into program language.
Meet programming standards by following production, productivity, quality, and customer-service standards.
Provide root cause and resolution for persistent system issues.
Serves as the escalation point during incident resolution.
Work with third-party providers and customers to maintain and implement new product configurations/solutions.
What We’re Looking For:
Bachelor’s degree in engineering, Computer Science, Management Information Systems/Science required or similar combination of education and experience.
5+ years’ experience with Python, .NET, and Microsoft SQL
5+ years of data warehousing and analytics experience utilizing Databricks, AWS, Microsoft SQL Server, SQL Server Reporting Services (SSRS) strongly preferred.
5+ years of experience with dimensional data modeling & schema design in Data Warehouses
5+ years of data integration experience for ETL/ELT Solutions and APIs
Experience with a variety of web service technologies (REST, SOAP, XML, JSON).
Experience and understanding of database administration and standardized Data Warehousing Concepts.
Experience with XML, Business Intelligence Tools, Dashboards, analytical manipulation, and interpretation of large databases.
Experience with/knowledge of modern data tools and technologies such as Snowflake, AWS RedShift, AWS S3, Databricks, FiveTran, etc. a plus.
Other Skills and Abilities We’re Looking For:
Research and analytical problem-solving capabilities
Excellent verbal and written communication skills
Able to balances a reactive problem-solving approach with a proactive process improvement mindset
Commitment to Company values
Competencies:
Technical/Professional Knowledge
Customer Focus
Collaboration
Quality Orientation
Planning & Organizing
Decision Making
Continuous Learning
Adaptability
Managing Work
Show Less
Report",$83T - $1L (Glassdoor Est.),51 to 200 Employees,Company - Private,Manufacturing,Consumer Product Manufacturing,1986,$25 to $50 million (USD)
102,"Alianza, Inc.
4.4",Data Engineer,"Pleasant Grove, UT","Data Engineer
Alianza is looking for an experienced and results driven Data Engineer. The successful candidate will be the technical engine of the data team, building Python applications to ingest streaming and extracted data and persist to cloud storage. Will use Python and SQL with AWS cloud technologies to automate the generation and delivery of reports. Will utilize CI/CD technologies to fully automate the release of all compute and storage components to the cloud. Work with our data architect and Java developers to design creative, high-quality, data-oriented insights and dashboards. Significant focus of the position will be on streaming data pipelines, distributed datalake architectures, and AWS services. Question the status quo. Write clean, testable, resilient code. Make things go fast and have fun doing it!
Key Duties and Responsibilities:
Participate in the process of designing, data engineering, and developing data services (Streaming, ETL/ELT, Real-time analytics, Reporting) using Python, SQL and AWS services
Adhere to modern methodologies for designing, coding, and testing
Build connected, fully automated data systems and pipeline
Work effectively with remote teams in various remote time zones
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources into usable format
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Identify and implement automatable tasks and reusable frameworks
Participate in sprint planning meetings and provide reasonable estimations
Research and propose new process, techniques, or tools as solutions. Able to produce technical diagrams, explanations, and written documentation to promote proposed solutions
Collaborate with data team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements

Qualifications:
3+ years of Python development experience, preferably writing modules that implement part of a streaming or batch ETL system in a cloud hosted environment
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse / lake designs to stakeholders
Experience designing, building, and maintaining data processing systems
At least 3 years’ experience with modern DevOps automation ecosystems, preferably Git/GitHub/Bitbucket, Buildkite or Ansible (or similar)
Real-world experience handling large data volumes (terabytes of storage and billions of rows)
At least 3 years experience configuring, using, and performance tuning AWS cloud services:preferably S3, Glue, Athena, Kinesis, Firehose, Lambda, Cloudwatch, ECS, API Gateway, RDS (Postgres), SQS, SNS, SES
Experience using AWS Redshift (or similar)
Experience with CloudFormation or TerraForm
Ability to prioritize, learn quickly, and do high-quality work
Demonstrate understanding of modern APIs and endpoints, like REST and GraphQL
Working understanding of Agile dev methodologies, especially Scrum and Kanban
Good listener, communicator, collaborator, and documenter
Proficient with Linux and shell scripting
Experience with data warehouse, data mart, OLAP, dimensional modeling, Kimball method
Good understanding of relational and document database concepts and best practices
Know how to design a clean, performance-optimized relational data model, and reverse engineer existing databases into physical data model diagrams
Experience using C*, Spark, Kafka, KSQL, Confluent, Pulsar and/or Kinesis helpful
Automated testing experience using JUNIT or equivalent
Some experience in software engineering (front, middle, back or all three) and application architecture
Show Less
Report",$73T - $1L (Glassdoor Est.),201 to 500 Employees,Company - Private,Information Technology,Software Development,2009,$25 to $50 million (USD)
103,"Mashvisor Inc.
3.7",Data Science Engineer,Remote,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?
Show Less
Report",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,2015,Unknown / Non-Applicable
104,"etrailer.com
4.0",Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L (Employer Est.),501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
105,"Jacobs Levy Equity Management
4.5",Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",#N/A,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
106,"Umanist Staffing
5.0",Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L (Employer Est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
107,"Innova Solutions Inc.
3.9",Data Engineer/Data Analyst,"Richmond, VA","Position Summary
The person will have a mix of highly technical data quality controls development, data analysis and reporting responsibilities to include writing complex SQL queries, some python code analysis, extensive data analysis, building DQ controls metrics reports, defining tech data controls strategy, working with metadata, architecture and development teams on the resolution to DQ controls issues
Primary Skill
MySQL
Secondary Skill
Tertiary Skill
Required Skills
SQL and DQ Controls Experience
DQ Controls Development and Strategy Skills
Business Analyst
Data Architecture
Desired Skills
Python, metadata, business intelligence reporting, BA
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richmond, VA 23173: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
MySQL: 5 years (Required)
Data Quality: 5 years (Required)
Work Location: Hybrid remote in Richmond, VA 23173
Show Less
Report",$60.00 - $65.00 Per hour(Employer Est.),10000+ Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1998,$2 to $5 billion (USD)
108,"Gopuff
3.0",Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L (Glassdoor Est.),5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
109,"ApolloMed
4.6",Data Engineer III,Remote,"Job Title: Data Engineer III
Department: Data
About the Role:
We are currently seeking a highly motivated Data Engineer III. This role will report to the Analytics Manager and work closely with data analysts and clinical leaders to produce deliverables for internal and external clients. With over a million managed lives across the country and terabytes of data generated, our teams need to be continuously equipped with the tools and insights to drive strategy and innovation to further our core values of improving patient outcomes and empowering our providers.
What You'll Do:
Design and develop a data reporting environment across organizational data systems
Identify and promote best practices and patterns for data modeling and provide oversight for activities to report migration and data consolidation
Design, code, manage data & analytics product backlog
Identify technical solutions that achieve efficiency and effectiveness and enable data usage as a strategic business asset
Review team members’ codes for quality and correctness
Collaborate with data analysts and other team members to architect data products and services
Integrate the data platform with analytical tools such as Tableau and PowerBI
Analyze and document processes to create a large shared knowledge base
Minimum qualifications:
Bachelor's degree required in healthcare, analytics, statistics, finance, business, or related field; Master’s degree (MBA, MPH) preferred
Experience with SQL Server or similar relational databases.
Strong understanding of database structures, theories, principles, and practices
Working knowledge with programming or scripting languages such as Python, Spark, and SQL
Experience with data profiling.
Familiarity with normalized, dimensional, star schema and snow flake schematic models
Familiarity with business intelligence exploratory or visualization tools (e.g., Tableau, PowerBI.)
Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations.
Strong written and oral communication skills.
Experience with Excel.
You're a great for this role if:
5+ years’ experience developing data pipelines or ETLs
3+ years' experience working with SQL Server Integration Services
3+ years’ experience in managed care or other healthcare data field preferred
Who We Are:
ApolloMed (NASDAQ: AMEH) is a physician-centric, technology-powered healthcare management company. We are building and operating a novel, integrated, value-based healthcare delivery platform to empower our physicians to provide the highest quality of end-to-end care for their patients in a cost-effective manner. Our mission is to combine our clinical experience, best-in-class delivery network, and technological expertise in order to improve patient outcomes, increase access to healthcare, and make the US healthcare system more efficient.
Our platform currently empowers over 10,000 physicians to provide care for over 1.2 million patients nationwide. Our rapid growth and unique position at the intersection of all major healthcare stakeholders (payer, provider, and patient) gives us an unparalleled opportunity to combine clinical and technological expertise in order to improve patient outcomes, increase access to quality healthcare, and reduce the waste in the US healthcare system.
Our Values:
Patients First
Empowering the Independent Provider
Be Innovative
Operate with Integrity & Deliver Excellence
Team of One
Environmental Job Requirements and Working Conditions:
This position is based remotely in the U.S.
The target pay range for this role is: $80,000 - $160,000. The salary range represents our national target range for this role


ApolloMed is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. All employment is decided on the basis of qualifications, merit, and business need. If you require assistance in applying for open positions due to a disability, please email us at humanresourcesdept@networkmedicalmanagement.com to request an accommodation.
Additional Information:
The job description does not constitute an employment agreement between the employer and employee and is subject to change by the employer as the needs of the employer and requirements of the job change.
Show Less
Report",$80T - $2L (Employer Est.),501 to 1000 Employees,Company - Public,Healthcare,Healthcare Services & Hospitals,1994,$500 million to $1 billion (USD)
110,"MetroSys
5.0",Data Migration Engineer,Remote,"Job Description
Context
We have a great project and need Migration Engineers to assist with migration of an enterprise client to a new data center. This new IT production environment is based in Redhat Linux, IBM AIX, Sun Solaris, UNIX and HP UX. We are searching for an engineer to support the migration by migrating servers, storage and databases to the new environment. We use a strict step-by-step plan (factory plan) to efficiently execute the migration.

Competence
Bachelor of Science / Master's degree
Minimal 3-5 years of relevant work experience within an enterprise environments
Advanced knowledge of Redhat Linux and UNIX
Advanced knowledge of IBM AIX, Solaris, and HP UX
Some knowledge of IBM XIV, Pure Storage, HPE Nimble and 3PAR storage preferred
Experience with databases (Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
The candidate has a hands-on mindset, a strong customer- and problem-solving orientation, shows fast results, and has demonstrated good communication skills, especially in an international IT organization. To achieve the project goals, the candidate is able to liaise directly with all stakeholders. The candidate has a clear focus on results and quality, and is eager to develop quickly as a project leader, serving customers.

Activities
Intake / analysis of applications for migration
Creation or update of migration run books
Migration of VMware instances to the new platform
Creation of storage disks, virtual storage devices
Reconfiguration of servers, including storage & network
1RfnvbOr2v
Show Less
Report",$50.00 - $70.00 Per hour(Employer Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
111,"Catalytic Data Science
4.2",Data Engineer,"Boston, MA","Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
Show Less
Report",$87T - $1L (Glassdoor Est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$1 to $5 million (USD)
112,"Ikigai Labs, Inc.
4.9","Software Engineer, Data Engineering","Cambridge, MA","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback
Show Less
Report",$83T - $1L (Glassdoor Est.),1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
113,"Radcube LLC
3.6",Senior Data Engineer,"Cincinnati, OH","Bachelor's degree in Computer Science/Information Systems Understanding of Object-Oriented Programming Languages Understanding of Software Development Lifecyle Strong SQL Skills with ability to perform ETL Familiarity with relational database architecture techniques like EDW, IBM DB2, etc Demonstrated practice for scripting languages, like Python, Java, Powershell Strong Windows Server Experience and/or Application Support Experience Prior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, &amp; Git is a plus Understanding of Agile Software Development methodologies Understanding of data management and info security best practices Demonstrated problem solving skills Demonstrated collaboration skills Excellent verbal and written communication skills
MINIMUM KNOWLEDGE, SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems
Understanding of Object-Oriented Programming Languages
Understanding of Software Development Lifecyle
Strong SQL Skills with ability to perform ETL
Familiarity with relational database architecture techniques like EDW, IBM DB2, etc
Demonstrated practice for scripting languages, like Python, Java, Powershell
Strong Windows Server Experience and/or Application Support Experience
Prior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, & Git is a plus
Understanding of Agile Software Development methodologies
Understanding of data management and info security best practices
Demonstrated problem solving skills
Demonstrated collaboration skills
Excellent verbal and written communication skills
Job Types: Full-time, Contract
Pay: $79,805.61 - $120,633.12 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Cincinnati, OH: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 5 years (Preferred)
Niffi: 4 years (Preferred)
kafka: 3 years (Preferred)
Work Location: In person
Show Less
Report",$80T - $1L (Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
114,"Vorys, Sater, Seymour and Pease LLP.
4.1",Senior Data Engineer,"Akron, OH","Precision eControl is all about helping brands better manage their eCommerce presence on Amazon and other online marketplaces. As a Senior Data Engineer, you will be part of a team that creates innovative, cutting edge, one of a kind solutions that are revolutionizing how brands manage online marketplace sales.
Position Summary
The Senior Data Engineer for the Azure infrastructure will be responsible for the day to day operations of a large data warehouse, and will work closely with the business, product team, and the technical staff to ensure alignment to goals and objectives. Utilizing experience with Big Data, this position will drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.
Essential Job Functions
Drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.
Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Design, implement, and document data load processes from disparate data sources into Azure Synapse Pipelines.
Work with Continuous Integration/Delivery using Azure DevOps and Github.
Provide data management, monitoring, troubleshooting and support to client success
Create various triggers to automate the pipeline in Azure Synapse Analytics Pipelines.
Tune SQL queries in Azure SQL DB, Azure Synapse and solve complex data challenges and deliver insights that help our customers achieve their goals.
Self-organize as part of a small-size scrum team and apply data engineering skills.
Follows industry best practices and meets company’s security and performance and requirements
Knowledge, Skills and Abilities
Minimum three (3) years’ experience with MS SQL/T-SQL
Minimum three (3) years’ experience with Azure SQL
Minimum three (3) years’ experience with Apache Spark (PySpark)
Minimum of three (3) years’ experience with Azure Data Factory or Azure Synapse building dynamic ETL pipelines
Minimum three (3) years' experience building dynamic Spark notebooks in Azure Synapse Spark or Azure Databricks
Minimum three (3) years’ experience with Python
Minimum two (2) years’ experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
Experience working with parquet, json , delta, avro and csv files
In-depth understanding of data management (e.g. permissions, recovery, security and monitoring)
Experience with Data Warehouse Architecture
Strong analytic skills related to working with structured, semi-structured and unstructured datasets.
Excellent analytical and organization skills required
Ability to understand user requirements
Client service mindset
Excellent verbal and written communication skills
Excellent problem solving skills
Familiarity with Agile frameworks a plus
Education and Experience
Bachelor's degree in related discipline or combination of equivalent education and experience
7-10 years of experience in similar field
Precision eControl LLC does not discriminate in hiring or terms and conditions of employment because of an individual’s sex, race, age, religion, national origin, ancestry, color, sexual orientation, gender identity, genetic information, marital status, military/veteran status, disability or any other characteristic protected by local, state or federal law. Precision eControl only hires individuals authorized for employment in the United States.
Show Less
Report",$87T - $1L (Glassdoor Est.),501 to 1000 Employees,Company - Private,Legal,Legal,1909,Unknown / Non-Applicable
115,"CODAMETRIX
4.7",Data Science Engineer III,"Boston, MA","CodaMetrix (CMX) is a multi-specialty coding AI-platform that translates clinical information into accurate sets of medical codes for patient care and revenue cycle processes, from fee-for-service to value-based care models.
We are passionate about getting doctors away from the keyboard and back to clinical care.
This is a remote position.
Job Description
At CodaMetrix, the machine learning (ML) and AI team is responsible for the invention, analysis, and deployment of new ML techniques using healthcare data to improve administrative and clinical medicine. This includes a deep understanding of the data, and any required data normalization processes and techniques.
We are looking for an experienced Data Science Engineer to join our machine learning team and help with translating proof-of-concept ideas to product grade solutions. The Data Science Engineer III will work closely with data scientists, product owners, and backend engineers to gather requirements and understand performance criteria to deliver solutions that bring our AI-driven robust and scalable products to market. The Data Scientist Engineering III primary focus areas will be building CMX infrastructure to run on AWS technologies, data ingestion, data lake utilization, data cleaning and deploying machine learning research experiments into production. This position will help architect, design, and deliver a new machine learning pipeline built on new technologies and as part of that effort work with the team to implement a new data normalization pipeline. The Data Science Engineer III reports to the Chief Data Scientist.
Responsibilities
Establish processes for data and modeling lifecycle through managing the transitions from proof-of-concept to production
Promote for best practice software development principles
Implement and test machine learning and deep learning techniques at scale
Analyze the quality and calibration of predictive models
Collaborate with machine learning, engineering, and product development teams, for deployment of new machine learning techniques and follow deployments, tracking issues, and successes
Work with applied research scientists doing experiments and support those experiments with appropriate data all reviewed/normalized
Provide engineering required to update the CMX Machine Learning pipeline supporting the research experiments move to production
Interface with medical coders, administrators, and physicians to understand the strengths and weaknesses of existing products and to help develop new machine learning-empowered products
Requirements
5+ years of experience in professional software development
3 years experience coding in Java
Heavy AWS experience at multiple companies
Fluent with software development best practices, including version control, documentation, testing and CI/CD
Extensive experience with machine learning approaches and an understanding of the analysis and testing processes of machine learning algorithms
Must have experience with AWS, particularly Spark, Cloud watch, Airflow, cloud storage (S3, Redshift) and computing (EC2, EMR)
Experience with SQL and NoSQL Databases, particularly production database systems (e.g. Postgres) and technologies
Proficiency in Python and Pandas
Experience developing in a Linux environment
Experience working in an active engineering development environment using tools to track code reviews, changes, testing
Experience with data collection, transformation, data cleaning, and working with a data lake
Bachelor's degree in software engineering, computer science or a related field with course or project work in machine learning, AI or data science
Beneficial Experience
Exposure to Natural Language Processing approaches
Experience with Spark and similar technologies from Data Bricks or others
Proficiency in Docker
Familiarity in DataBricks.
Familiarity with deep learning approaches such as CNN, RNN and Reinforcement learning
Familiarity with Elasticsearch a plus
Knowledge of US healthcare systems
Master's degree with significant course or project work in a machine learning-related field

About CodaMetrix
CodaMetrix is a well-capitalized, high-growth company focused on solving some of the more interesting yet challenging problems in clinical and healthcare administrative areas. The company was formed by the Massachusetts General Physicians Organization (MGPO) to commercialize and build upon internally developed, and highly utilized AI-driven solutions. CodaMetrix is led by proven entrepreneurs, technology, and healthcare veterans, whose vision is to create a highly desirable atmosphere for technical talent to flourish and develop innovative, significant, reliable, and broadly utilized solutions.
As a high-growth company, our team and responsibilities are constantly evolving. We therefore prioritize creative thinking, strategic problem solving and an enthusiasm for creating innovative, high-quality products for our customers. Together, we are motivated to achieve our mission of reducing the burden in healthcare administration. Candidates eager to develop and expand their skills, learn from highly technical team members and think differently will thrive at CodaMetrix.
Salary Range
$100,000-$180,000

Full-Time Employee Benefits
Learn more about how we take care of our team.
Insurance: We cover 80% of the cost of medical and dental insurance and offer vision insurance.
Retirement: CMX offers a 401(k) plan that eligible employees can contribute to one month after their first day.
Life: We offer employer-paid life insurance and short-term and long-term disability insurance.
Flexibility: We have an unlimited PTO policy so you can take the time you need to relax and rejuvenate.
Learning: All new hires complete our 7-week Fellowship program to learn about each of our departments.
Development: We provide annual performance evaluations and outline a clear path for promotions.
Engagement: We host recurring events like Meditation Mondays, CMX Connections and Socials.
Recognition: We recognize quarterly You've Been Awesome winners and celebrate our team's service milestones.
Background Check
All candidates will be required to complete a background check upon acceptance of a job offer.
Equal Employment Opportunity
Our company, as well as our products, are made better because we embrace diverse skills, perspectives, and ideas. CodaMetrix is an Equal Employment Opportunity Employer and all qualified applicants will receive consideration for employment.
WqeVk8vO8c
Show Less
Report",$1L - $2L (Employer Est.),51 to 200 Employees,Company - Public,Information Technology,Information Technology Support Services,2018,Unknown / Non-Applicable
116,"Plaxonic
4.6",Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour(Employer Est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
117,"FINSYNC, Inc.
4.2",Data Engineer / Data Analyst,Remote,"FINSYNC is looking for an accomplished Data Engineer/Data Analyst to own data pipeline end to end, including data ingestion, curation, modeling and visualizations. This role will ensure proper data architecture and processes to support data initiatives at scale.
Successful Candidates Have:
5+ years of designing and implementing data solutions
5+ years experience developing data models to support credit and underwriting use cases
Deep understanding of methods to transform source data into refined data catalogs for immediate and seamless consumption in presentation layer
5+ years creating scalable data pipelines all the way to the consumption from customers and internal business partners
Experience with side variety visualizations tools such as Qlik Cloud (preferred), MS PowerBI, Tableau
Experience analyzing both quantitative and qualitative data and understanding best practices for visualization
Experience with SQL and similar programming languages
Strong passion for data quality and data governance
Job Responsibilities:
Maintain and extend data pipeline for all data initiatives
Support data initiatives by developing data models and visualizations
Work collaboratively with stakeholders to ensure data quality and relevance
Make recommendations on changes to source data to more efficiently support business use cases
Support the integration and consumption of data into other platforms and systems
What We Can Offer You for All Your Hard Work?
Competitive compensation package
Full suite of health & welfare benefits including medical, PTO & 401(k)
Workplace flexibility and collaboration with a virtual-first team
ABOUT THE COMPANY
We sync with over 7,000 banks and credit unions to provide the approximately 32 million small businesses in the United States a simpler way to manage finances with the software and services they need, all in one platform. After a business syncs their bank account to FINSYNC, our software helps them send and receive payments, process payroll, automate accounting and manage cash flow with less time and better results working with financial partners (bankers, credit card processors, accountants, and insurance agents) who use our software to deliver unmatched services. The result: financial harmony, peace of mind and the best way to succeed.
We cultivate a culture of:
Teamwork (like a pro sports team) without ego
Constructive communication so we can build transparency and trust
Metrics that matter personally, professionally, and financially
High performance merits high compensation
Self-motivation and self-discipline merits flat organization
Time management and work-life-harmony
Finsync is on a mission to improve the lives of others by helping them succeed in business and in life, doing more business with less time and better results so they can invest their time and successes in other ways meaningful to them.
Show Less
Report",$1L - $2L (Employer Est.),Unknown,Company - Private,Information Technology,Software Development,#N/A,Unknown / Non-Applicable
118,DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour(Employer Est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
119,"Ascendion
4.5",Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour(Employer Est.),1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
