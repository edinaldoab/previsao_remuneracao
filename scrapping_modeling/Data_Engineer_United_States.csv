company,job title,location,job description,salary estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
YT Global Network,Data Engineer- Remote,Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$90.00 - $120.00 Per hour,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Wevision LLC,Data Engineer,"Irvine, CA","Job description
We build services, data platform and machine learning based optimization engines for every aspect of advertising, including targeting, decisioning, pricing, personalization, inventory forecasting, attribution and full-funnel measurements. Our tenant is a strong tech team to deliver E2E solutions covering tech areas ranging from research, bigdata, microservices to data applications with front-end tools. Our team is seeking a software developer who will be an outstanding addition and will be responsible for development of high-available and high-concurrent backend services or data solutions. The right person for this role should have experience on either full stack components, or microservices or bigdata platforms. If you are someone who is proactive, hardworking, and enthusiastic in either these domains, this is a phenomenal role for you!
WHAT YOU’LL DO
Build components of large scale data platform for online streaming data and offline batch data from ETL pipelines, data processing, operational data store and AI feature stores.
Continuously improve performance, scalability and availability for microservices of advertising targeting, decisioning and ranking.
Own features of bigdata applications to fit evolving business with realtime metrics, measurable insights and industry leading user experience.
Drive adoption of the best engineering practices, including the use of design patterns, CI/CD, code review and automated integration testing.
Chip in disruptive innovation and apply new ground breaking technologies
As a key member of the team, contribute to all aspects of the software lifecycle: design, experimentation, implementation and testing.
Collaborate with program managers, product managers, and researchers in an open and innovative environment.
WHAT TO BRING
BS in computer science or engineering.
3+ years of professional programming and design experience in Scala, Java, Python, and etc.
2+ years of experience with Hadoop ecosystem e.g. HBase, Hive, Spark/Flink, Impala, Presto, Click House, Druid and etc.
Knowledge of system, application design and architecture
Passion for technology, open to interdisciplinary work
NICE-TO-HAVES
Experience with processing large amount of data at petabyte level.
Experience in digital video advertising or digital marketing domain.
Experience with CRM, DMP, user portrait and audience insights.
Experience with Airflow, Kafka, MemSQL, Docker, AWS, Terraform, Spinnaker, K8S, and etc.
Experience in at least one widely used Web framework (React.js, Vue.js, Angular, etc.) and good knowledge of Web stack HTML, CSS, Webpack.
Job Types: Full-time, Contract
Pay: $55.00 - $85.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Education:
Bachelor's (Required)
Experience:
Python: 1 year (Required)
AWS: 1 year (Preferred)
Scala: 1 year (Required)
Work Location: Hybrid remote in Irvine, CA 92602
Show Less
Report",$55.00 - $85.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
kairos technologies,Data Engineer,"Boston, MA","Job Title: Data Engineer with AWS – Hybrid
Location: Boston, MA
Duration: 6-12 months
Detail Job Description:
Familiarity with data lake, data warehouse or data lake environments and related topics . Has a proven track record to work with vendors to deploy external SaaS solutions and integrate with existing systems.
In depth with data lakes/ data environments including ETL (PySpark), data Catalogs (Glue, Alation), API interfaces, Cloud data warehouses such as Redshift, Querying engines such as Trino.
Agile approaches to building cloud native solutions using CI/CD, containers, Kubernetes, GitOPS, etc..
Strong automation and development skills in terraform, CloudFormation, and other languages like Python, and bash.
8+ years of total IT experience, with at least 4 years in AWS services such as EMR, EC2, S3, IAM, Glue andRedshift and 2+ years of experience in Infrastructure as code technologies like Terraform, CloudFormation.
4+ years of Python, SQL experience is mandatory.
Job Type: Contract
Salary: $60.00 - $68.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 9725977972
Show Less
Report",$60.00 - $68.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Konnectingtree,Data Engineer,Remote,"Greetings from KonnectingTree!
We are looking for a Data Engineer for one of our clients. This is a remote position with an Implementation partner. AWS Certification Mandatory.
Data Engineer with AWS Experience
Experience with PySpark/Spark
Experience in Python
Able to work independently
Able to work with the business team directly
Interested candidates kindly share your updated resume with mythili.saravanan@konenctingtree.com. Please reach me at 952-679-2916.
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Required)
Python: 5 years (Required)
PySpark: 5 years (Required)
Work Location: Remote
Speak with the employer
+91 952-679-2916
Show Less
Report",$45.00 - $50.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Edmunds.com,Data Engineer,"Santa Monica, CA","Edmunds offers flexibility to work fully remote, from our Edquarters, or a combination of both

At Edmunds we’re driven to make car buying easier. Ever since we began publishing printed car guides in the 60’s, the company has been in the business of trust, innovating ways to empower and support car shoppers. When Edmunds launched the car industry’s first Internet site in 1994, we established a leadership position online and have never looked back. Now, as one of the most trusted review sites on the Internet, millions of visitors use our research, shopping and buying tools every month to make an easy and informed decision on their next car. For consumers, we bring peace of mind. For dealers, we make tools to help them solve their problems and sell more cars. How do we do it, you ask? The key ingredients are our enthusiastic employees, progressive company culture and cutting-edge technology. Want to join the team? Read on to find out how!

What You’re Applying For:
The analytics group at Edmunds.com is looking for an Analytics Development/ Data Engineer who will work with Business Intelligence tools in collaboration with product owners, analysts, marketing and engineering to design and implement Analytics Solutions for Edmunds.
Edmunds.com is all about data, big data! We are fast becoming the de facto standard by which automotive information is defined in the United States. We collect detailed, user behavior for over 18 million unique visitors a month. The BI Engineer is responsible for building BI Solutions, analyzing data, maintaining the platform and answering questions in the analytics realm.
We are looking for a person that is smart, creative, ambitious and most of all passionate about data wrangling and management. Large-scale data analytics should excite you. Edmunds is very much about helping its people grow—meaningfully, effectively and prosperously. We care about what we do and we care about our people. A demonstrated passion for data and analytics will provide incredible growth opportunities and will make this the best job you ever had.

What You’ll Do:
Work with business users directly to gather requirements and provide solutions and insights
Follow Agile project management methodology
Translate functional requirements into technical designs and choose the appropriate BI solutions
Build Data Pipelines to cleanse and store data in databricks
Support infrastructure on AWS
Document functional requirements, functional designs, and all technical designs according to Edmunds technical documentation standards
Apply Minimum Viable Product (MVP) approach to test the solution before building the entire application

What You Need:
Bachelor’s degree in Engineering, Mathematics, Statistics
Minimum 5 Years of Professional development experience
Minimum 2 years experience in deploying & troubleshooting AWS infrastructure. AWS Certification Is preferred
Working Knowledge in Scripting Languages like Linux (Shell Script), Python
Proficient data analysis experience utilizing SQL
Working knowledge in any one of the RDBMS such as Postgres, Redshift
Experience in Business Intelligence tools such Tableau, Microstrategy or equivalent
Experience in Big Data analysis using SparkSQL HDFS, Hive and Scala
Working knowledge in any two of the programming languages Java, C#, or Perl
Working knowledge of statistical analysis using tools such as R
Experience in interacting with cross functional teams
Advanced analytical thinking and problem solving skills with deep technical knowledge of data infrastructure practices and tools
Ability to create and present status reports to senior executives
Experience in configuring and using DevOps tools JIRA, Stash, Confluence

The compensation range for this position is $113,000 - $169,000 per year. The base pay will take into account internal equity as well as job-related knowledge, skills, and experience among other factors. In addition, Edmunds offers full-time employees a comprehensive total rewards package including the benefits listed below.
Edmunds Perks:
Flexible time off
13 Paid Holidays
Comprehensive Health Benefits (medical, dental, vision, life and disability)
Flexible Spending Accounts (Employees) and Health Savings Accounts (Employee and Employer Contributions)
401K Plan with company matching at 100%, up to 6% of eligible salary with immediate vesting
Stock purchase program
CarMax vehicle discount
Up to 4 months Paid Parental Leave
HeartCash matches employee donations to the causes that are important to them
2 Days of Paid Time Off for time to dedicate to social impact causes
FitCash covers a portion of gym or fitness activity fees
Well being sessions and events such as yoga, meditation and walking challenges
On-going career development sessions and an annual learning event
Pet insurance
Sabbatical leave
Education Reimbursement
Pre-tax spending accounts for qualified transportation expenses
Plus a coffee bar, frozen yogurt and more!

Working @ Edmunds.com:
Employees think it’s a pretty great place to work and some pretty impressive publications think it is too: we have been recognized as one of the best places to work by the Fortune Magazine and Great Places to Work, LA Business Journal (for the last 6 years!), Computerworld, Built in LA and Inc. Magazine. We've also been identified as one of the best workplaces specifically in Technology and also for Diversity and Asian Americans. If you’re interested in learning more and joining our mission, we’d love to hear from you!

Edmunds will consider for employment qualified candidates with criminal histories in a manner consistent with the requirements of all applicable laws.
#BI-Remote

This is a remote position.
Show Less
Report",$1L - $2L,201 to 500 Employees,Company - Private,Information Technology,Internet & Web Services,1966,$100 to $500 million (USD)
Kanini Software Solutions,Data Engineer,Remote,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Data bricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $65.00 - $70.00 per hour
Experience level:
10 years
Work Location: Remote
Show Less
Report",$65.00 - $70.00 Per hour,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2003,$5 to $25 million (USD)
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
STAND 8,DATA ENGINEER,Remote,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!
Looking for a highly motivated and experienced Data Engineer to join our dynamic team. The successful candidate will be responsible for designing, developing and maintaining comprehensive Datalake and warehouse solutions. The candidate should be able to tackle challenges that come with complex large-scale data having different streams of data sources.

Key Responsibilities
Building different types of data lake and warehousing layers based on specific use cases.
Building scalable data infrastructure and understanding distributed systems concepts from a data storage and compute perspective.
Utilizing expertise in SQL and having a strong understanding of ETL (Extract-Transform-Load) and data modeling.
Ensuring the accuracy and availability of data to customers and understanding how technical decisions can impact the business’s analytics and reporting.
Interfacing with other technology teams to extract, transform and load data from a wide variety of data sources.
Design Data models and Data Products to enable advanced analytics for business
Developing scalable engineering solutions and building data solutions that drive real impact at the company
Collaborate with other teams to ensure proper integration of new features and identify areas for improvement.
Maintain a deep understanding of the software architecture and how different components interact.
Ensure quality by performing root cause analysis and troubleshooting of defects.
Stay current with industry trends and advancements in Data Engineering.

Qualifications
Bachelor’s degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in Data Engineering, with a strong focus on batch and stream data processing using distributed computing systems like Spark.
Proficiency in either pySpark or Scala to handle large-volume data processing.
Proficiency in programming languages like Python or Java.
Familiarity with AWS.
Strong SQL skills, including performance tuning.
Excellent written and verbal communication skills.
Experience with Agile software development methodologies.
Ability to work independently and in a team environment.

The US base range for this contract position is $50-$80/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training
Show Less
Report",$50.00 - $80.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
Radiant Systems Inc,Azure Data Engineer,"Milwaukee, WI","Azure Synapse (no data bricks please)
Some Data Modeling would be helpful
ADF workflow experience
GIT
Python
Spark
MS or Azure SQL
Azure DevOps experience helpful
Datawarehouse and ETL
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: Remote
Show Less
Report",$87T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1995,$25 to $50 million (USD)
Freemind solutions,Big Data Engineer with Spark and Python,Remote,"Required Skillset
· 5-10 years of experience as a Big Data Developer
· In-depth knowledge of Big Data technologies - Spark, HDFS, Hive, Kudu, Impala · Solid programming experience in Python
· Production experience in core Hadoop technologies including HDFS, Hive and YARN
· Strong working knowledge of SQL and the ability to write, debug, and optimize distributed SQL queries
· Excellent communication skills; previous experience working with internal or external customers
· Strong analytical abilities; ability to translate business requirements and use cases into a Hadoop solution, including ingestion of many data sources, ETL processing, data access, and consumption, as well as custom analytics
· Effective analysis of new and existing applications and platforms
· Experience working with Data Governance tools like Apache Sentry, Kerberos, Atlas, Ranger
· Experience working with streaming data with technologies like Kafka, Spark streaming
· Strong understanding of big data performance tuning
· Experience handling different kinds of structured and unstructured data formats (Parquet/Delta Lake/Avro/XML/JSON/YAML/CSV/Zip/Xlsx/Text etc.)
· Well versed with Software Development Life Cycle Methodologies and Practices · Clear communication and documentation of technical specifications
· Spark Certification is a huge plus Responsibilities
· Integrate data from a variety of data sources (data warehouse, data marts) utilizing on-prem or cloud-based data structures (Azure/AWS); determine new and existing data sources
· Develop, implement and optimize streaming, data lake, and analytics big data solutions
· Create and execute testing strategies including unit, integration, and full end-to-end tests of data pipelines
· Recommend Kudu, HBase, HDFS, and relational databases based on their strengths
· Utilize ETL processes to build data repositories; integrate data into Hadoop data lake using Sqoop (batch ingest), Kafka (streaming), Spark, Hive or Impala (transformation)
· Adapt and learn new technologies in a quickly changing field
· Be creative; evaluate and recommend big data technologies to solve problems and create solutions Recommend and implement best tools to ensure optimized data performance; perform Data Analysis utilizing Spark, Hive, and Impala
Job Type: Contract
Job Type: Part-time
Pay: $60.00 - $65.00 per hour
Experience:
spark: 5 years (Preferred)
python: 5 years (Preferred)
databricks: 3 years (Preferred)
Work Location: Remote
Show Less
Report",$60.00 - $65.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Kenco Management Services LLC,Data Engineer,"Chattanooga, TN","About the Position
The Data Engineer will develop and support next generation architecture for the advanced supply chain analytics offering. The job will design, create, manage, and use of large datasets across a variety of data platforms. The position crafts, implements, and operates stable, scalable, low-cost solutions to replicate data from production systems into the BI data store.

Functions
Partner with key distribution, transportation, and material handling business managers to design and implement efficient and scalable Extract, Transform, Load (ETL) solutions
Assist in developing and improving the current ETL/BI architecture, emphasizing data security, data quality and timeliness, scalability, and extensibility
Design low latency data architectures at scale to enable data driven decision-making
Delivery of performant and scalable data products and platforms, both Online Transactional Processing (OLTP) and Online Analytical Processing (OLAP), to consume, process, analyze and present data from the data ecosystem, while driving data engineering best practices
Ensure the integrity and uninterrupted processing of batch and real-time data pipelines to ingest and process data from various internal data sources and third-party platforms
Work closely with Data Science and Business Intelligence team to develop and maintain company-level Key Performance Indicators (KPIs), accompanying core metrics and develop architecture to drive business growth
Qualifications
Bachelor’s degree in Computer Science. Master’s or PhD degrees preferred
6-8 years Data Engineering experience: proven track record working with large datasets and closely working with data analysts, software, and infrastructure engineers
Expert level skills writing/optimizing complex SQL and schema buildout with structured and unstructured data, including star schemas, constellations, and snowflake schemas
5-6 years of experience with Python
Experience with ETL tool like SSIS etc.
Strong background in working with Big Data technologies like: Spark, Kafka, Hadoop, etc. preferred
Experienced in developing and delivering on technical roadmaps and architectures for platforms or cross-functional problems that impact multiple teams
Experience in data mining, profiling, and analysis
Experience with workflow orchestration tools and rules-driven process automation engine systems
Good understanding of Continuous Integration (CI)/ Continuous Delivery (CD) principles
Demonstrated track record of dealing with ambiguity, prioritizing needs, and delivering results in a dynamic business environment
Proven ability to develop unconventional solutions; Sees opportunities to innovate and leads the way
Experienced in designing data solutions utilizing AWS cloud data platforms and tools

Competencies
Business Acumen - Knowledgeable in current and possible future policies, practices, trends, technology, and information affecting his/her business and organization.
Communicate for Impact - Proactively communicate with all stakeholders throughout the life cycle of programs and projects.
Influencing Others - Can quickly find common ground and can solve problems for the good of the organization with a minimal amount of noise. Authentically gains trust and support of peers.
Managing Transitions/ Change Management - Effectively plans, manages and communicates changes in processes with appropriate stakeholders.
Strategic Agility - Enable Kenco to remain competitive by adjusting and adapting to innovative ideas necessary to support Kenco’s long-term organizational strategy.
#LI-Remote
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
Show Less
Report",$75T - $1L,1001 to 5000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,1950,$500 million to $1 billion (USD)
Titan Healthcare Management Solutions,Data Engineer,Remote,"Titan Health is currently hiring for a Data Engineer.
Under the general direction of the Technology Solutions Manager, the Data Engineer is responsible for implementation, configuration, maintenance, and performance of business-critical data infrastructure to deliver data enablement at scale, and power our revenue cycle applications. This role will work with enterprise and client leaders to translate business and functional requirements into technical specifications and solutions within the data architecture strategy.
Essential Job Duties/Responsibilities
Implement efficient and scalable pipelines integrating data from multiple sources to common data models.
Convert raw data into usable information for client and enterprise organizations.
Within an Agile team design, develop, test, implement, and support technical solutions that support full-stack development tools and technologies.
Support data science, data enrichment, research, and data analysis as well as making data operationally able to be consumed by products and services.
Collaborate with product managers, software engineers, security & compliance, and data scientists to enable them with robust data delivery solutions that drive powerful experiences.
Identify and debug issues with code and suggest changes and/or improvements.
Perform unit tests and conduct reviews with the team to ensure code is rigorously designed, elegantly coded, and effectively tuned for performance.
Utilize available technologies to collect and map data to find cost savings and optimization opportunities.
Support and drive a proactive culture of security and compliance.
Bring an agile and engineering mindset to address complex problems, identify opportunities and craft creative solutions.
Leverage best practice coding and engineering standards to support growth and flexibility.
Coordinate with teams across the organization to address incident, change and release management needs/requirements.
Provide input to risk management; report risks as they are identified and participate in prioritization/follow up.
Stay current with emerging technologies and advancements within existing technologies.
Positively and deliberately engage with colleagues – external and internal – to foster collaborative and productive relationships.
Cultivate great teams and lead in alignment with Titan values.
Comply with and hold with utmost regard all compliance requirements to protect patient privacy and confidentiality.
Stay curious, kind and contribute positively to the Titan culture.
Minimum Qualifications
Bachelor’s Degree in Information Technology, Computer Science, Mathematics or related field is preferred but not required.
Experience with Agile methodologies.
Experience working in HIPAA, HITRUST or other advanced compliance environments.
Experience leading the lifecycle management and integrations of enterprise data.
Ability to clearly articulate technology concepts to business leaders and engineers.
Strong understanding of IT Service Management practices.
Strong analytical, problem-solving, and critical thinking skills with excellent attention to detail.
Direct experience with Azure SQL.
Experience with Azure Data Factory preferred.
Prior experience in the facilitating conversations to translate business requirements into the technical requirements needed to develop solutions.
Excellent oral and written communication skills.
Comprehensive knowledge of Microsoft Office applications.
Knowledge of Hospital CMS or billing data structures preferred.
Titan Health offers a robust Health and Welfare benefits program, along with Paid Time Off, 401k plan with company match, and remote working environment.
H9CslyVeWp
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
ERPMark Inc,Data Engineer (Scala),"Bentonville, AR","Title: Scala Engineer
Location: Bentonville, AR
Spark Engineer Description:
Client is looking for a highly energetic and collaborative Spark Data Engineer for a 12-month engagement. Responsibilities: As a Senior Spark Data Engineer, you will • Design and develop big data applications using the latest open source technologies. • Desired working in offshore model and Managed outcome • Develop logical and physical data models for big data platforms. • Automate workflows using Apache Airflow. • Create data pipelines using Apache Hive, Apache Spark, Apache Kafka. • Provide ongoing maintenance and enhancements to existing systems and participate in rotational on-call support. • Learn our business domain and technology infrastructure quickly and share your knowledge freely and actively with others in the team. • Mentor junior engineers on the team • Lead daily standups and design reviews • Groom and prioritize backlog using JIRA • Act as the point of contact for your assigned business domain Requirements: GCP Experience • 1+ years of recent GCP/BIG Query experience • Experience building data pipelines in GCP • GCP Dataproc, GCS & BIGQuery experience • 5+ years of hands-on experience with developing data warehouse solutions and data products. • 5+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive or Spark, Airflow or a workflow orchestration solution are required • 2+ years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms. • Experience with programming languages: Python, Java, Scala, etc. • Experience with scripting languages: Perl, Shell, etc. • Practice working with, processing, and managing large data sets (multi TB/PB scale). • Exposure to test driven development and automated testing frameworks. • Background in Scrum/Agile development methodologies. • Capable of delivering on multiple competing priorities with little supervision. • Excellent verbal and written communication skills. • Bachelor's Degree in computer science or equivalent experience. The most successful candidates will also have experience in the following: • Gitflow • Atlassian products - BitBucket, JIRA, Confluence etc. • Continuous Integration tools such as Bamboo, Jenkins, or TFS
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Schedule:
8 hour shift
Work Location: In person
Show Less
Report",$60.00 - $65.00 Per hour,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
TikTok,"Data Engineer, E-Commerce","San Jose, CA","Responsibilities
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.

Why Join Us
At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok.

The Global E-Commerce team focuses on building data infrastructure and data product areas to support business engineering teams working directly on TikTok's E-Commerce platform.

As a data engineer in the Global E-Commerce team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.

Responsibilities - What You'll Do
Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis);
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;
Establish solid design and best engineering practice for engineers as well as non-technical people.
Qualifications
BS or MS degree in Computer Science or related technical field or equivalent practical experience;
Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.);
Experience with performing data analysis, data ingestion and data integration;
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems;
Experience with schema design, data modeling and SQL queries;
Passionate and self-motivated about technologies in the Big Data area.
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.

TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at Dennis.Chau@tiktok.com
Job Information
The base salary range for this position in the selected city is $136000 - $280000 annually.



Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.



At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees:



We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.



Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.



We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice.
Start your job application: click Apply Now
Show Less
Report",$1L - $3L,10000+ Employees,Company - Private,Information Technology,Internet & Web Services,2016,Unknown / Non-Applicable
BlueOcean,Data Engineer,Remote,"BlueOcean is a different kind of SaaS company. We are a team of problem solvers fueled by challenging convention, and we are looking for doers, leaders, and dreamers to join us. We are venture-backed by arguably the best SaaS VC in the world Insight Partners, and we are looking to hire those who want to join us as we build this ship.
Our mission is to revolutionize how marketing decisions are made. We are a single pane of glass connecting data and insights to actions and outcomes. Our fundamental vision is to simply Unlock Human Creativity!
You're excited about this opportunity because you will:
Get in at the ground floor of a company fundamentally changing the brand strategy landscape through the use of revolutionary technology
Define and assist in building data solutions to support our data science initiatives
Work with Data Science leaders and stakeholders to ensure data solutions are defined by business requirements and meet stakeholder expectations
Collaborate with Engineering and IT to ensure solutions meet company security and operations requirements
Build database schemas, build, and deploy ETL processes
Assist in build/buy analysis for data management and overlay products
Work with vendors to implement platform and/or product solutions to meet stakeholder requirements
Work with data sources through API or Files
We are excited that you are:
Strongly analytical, with reasoning skills that result in clear technical architectures
A clear thinker, with the ability to translate requirements into clean, efficient, quality code
Driven, proven to prioritize, self-direct and execute at startup velocity
Skilled in communication with both technical and non-technical stakeholders
Qualifications:
Bachelor’s degree in Computer Science or related discipline
4+ years of experience working with large, complex data sets through batch, streaming and shadow copy
Experience deploying and running data pipelines on Amazon Web Services (AWS)
Excellent grasp of Data Warehousing, ETL/ELT and In-Memory/Cubes use cases and patterns
In-depth experience with SQL Server, SSIS and SSAS
Experience with document and graph type NoSQL databases
Experience with REST and GraphQL API
Strong experience working in Python
Compensation:
The salary range for this role is from $135,000 to $155,000. BlueOcean offers a comprehensive benefits package, including healthcare benefits, short-term disability coverage, and 401k. Employees also receive the following: flexible paid time off, 12 paid holidays, company-wide recharge days, and remote working arrangements.
Location:
This is a remote position. BlueOcean is currently able to support employees in the following states: CA, GA, MA, NC, NH, NJ, NY, OR, PA, SC, TX, UT, VA, WA, and WY.
Job Type: Full-time
Pay: $135,000.00 - $155,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
Schedule:
Monday to Friday
Application Question(s):
Do you have python coding experience? If so, how many years?
Do you have experience managing data pipelines in AWS? If so, how much experience do you have?
Do you have experience with AWS services like Glue, Airflow, etc?
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zoll Medical Corporation,Data Engineer,"Broomfield, CO","Data
Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.
Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Other Duties:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
ZOLL Medical Corporation appreciates and values diversity. We are an Equal Opportunity Employer M/F/D/V.
ADA: The employer will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.
Job Type: Full-time
Pay: $125,000.00 - $140,000.00 per year
Work Location: Remote
Show Less
Report",$1L - $1L,1001 to 5000 Employees,Company - Private,Manufacturing,Health Care Products Manufacturing,1980,$100 to $500 million (USD)
Robert W. Baird,Data & Analytics Engineer,"Milwaukee, WI","As we continue to grow and add top talent to the Baird family of technical associates, we are looking for a Data & Analytics Engineer for our growing data team. This is a key role on our IT Data Team requiring a broad range of skills and the ability to step into different roles depending on the size and scope of the business need. The self-motivated candidate will have proven experience architecting successful data solutions on key projects in a collaborative environment. Success will come from being able to prioritize, deliver value incrementally, problem solve, and manage changing priorities. You will work closely with our business partners and interface with both technical and non-technical colleagues.
This position is hybrid, working a combination of remote and in-office in our new collaborative work space. We offer a collaborative culture with a continuous learning, agile/lean environment and adding value to the Baird business. Learn more about Baird IT here.

As a Data & Analytics Engineer, you will:
Data Architecture
Specialize in data modeling, both 3NF and dimensional, with experience in conceptual, logical, physical, and industry data modeling. Strong knowledge and experience with data architecture methodologies.
Apply the appropriate level of modeling theory, pattern recognition, and abstractions to architect and design a pragmatic solution that functionally meets the business and technical requirements.
Partner with internal business units to define information requirements and translate them into appropriate data solutions.
Collaborate with IT and business partners to lead data discovery, profiling, analysis, and quality assessments in order to obtain clear information requirements.
Develop and validate source to target mappings and transformation logic required to support business needs. Understand the importance of capturing data lineage.
Architect, implement and verify end-to-end data solutions.
Develop test plans needed to ensure a quality deliverable. Participate in validation testing, coordinate user acceptance testing and training to ensure the final implementation enables the user to solve their business problem.
General Data Management
Play a critical role in architecting our data and analytics solution landscape
Demonstrate competence, experience, knowledge, understanding, and advocacy of data management concepts, data warehousing, BI, and analytics.
Demonstrate ability to perform appropriate level of strategic thinking by viewing initiatives both within the immediate project context as well as the overall architectural vision.
Participate and/or Lead in data architectural design and strategy discussions.
Data Delivery
Work with the business users to conduct data discovery engagements and can quickly identify, and prototype, a solution that brings together multiple data sources into one coherent concept and understanding. (data blending)
Leverage existing tools to create data visualizations and mentors the business to be self-sufficient.
Collaborate – build relationships!
Identify and communicate project risks and impediments and proactively work with other members of the Analytics team to complete high-value deliverables as identified by business partners and team leadership.
Partner with Analytics team members to translate business and functional requirements into technical designs
Strive to understand the data consumption needs of the business community, as well as the problems faced by business users involving the access and use of data
Help Analytics teams develop solutions that enable businesses to capitalize on business insights and drive toward gaining a competitive advantage
What makes this opportunity great:
Information technology is a core part of Baird’s business strategy and plays a critical role in the growth and transformation of the firm.
On Computerworld’s ‘Best Company to Work For’ list for five consecutive years with a collaborative culture that values diverse backgrounds and perspectives while emphasizing teamwork and a strong sense of partnership.
Support and flexibility to grow and be your best at work, at home, and in the community.
What we look for:
Minimum of 3-5 years of experience in Data Solution delivery in a complex environment working collaboratively in a team setting
Proficient in Data Solution tools and concepts such as:
Business Intelligence tools: Microsoft tools (SQL Server Management Studio, SSRS, SSAS, Power Pivot, Power Query, PowerBI), Alteryx
Database: SQL Server
Data Query tools: SQL, T-SQL
Data Management and Quality: data mapping, data profiling, metadata repository, relational data modeling, master data management
Data Modeling: ER/Studio Data Architect, 3NF and dimensional modeling
Data Warehousing concepts: Inmon, Kimball, Data Lake
Data Integration concepts and strategies: EII, ETL, EL-T and EAI
#LI-SB1
#LI-Hybrid
Commitment to Inclusion & Diversity
Baird is committed to inclusion & diversity for our clients, our associates and the communities where we live and work. This commitment stems from our culture of integrity, genuine concern for others and respect for the individual. We view inclusion & diversity as an ongoing journey – one of shared responsibility, continuous improvement and a focus on progress. We invite you to join us as we work together to foster an environment where diversity unites rather than divides us.
Show Less
Report",$84T - $1L,1001 to 5000 Employees,Company - Private,Finance,Investment & Asset Management,1919,$2 to $5 billion (USD)
Jetty,Data Engineer,"New York, NY","Welcome to Jetty, the financial services platform on a mission to make renting a home more affordable and flexible. We've built multiple financial products that benefit both renters and property managers - and we're just getting started.

As a member of the Jetty engineering team, you're passionate about building fintech products that provide value to our customers and to Jetty. You are motivated by designing engineering systems around complex business problems. You love to learn, take on challenges, and are empowered in a fast-paced and transparent culture. You're comfortable finding the right tool or pattern for the job, and advocating for improvements to the way we work.

As a Data Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high level, solving complex problems, and delivering solutions with real business impact - and you're excited by the opportunity to apply those principles to a new, best in class function.

Role & Responsibilities
Build / Support our modern data stack (Snowflake / Fivetran / DBT / Tableau)
Implement the Five Pillars of Data Observability
Write ELT code using modern software engineering practices (Git, automated testing and deployments)
Build and maintain data pipelines to support various business processes and reporting (Fivetran / AWS Lambdas)
Document our data models in a user friendly way for our business stakeholders
Partner with the Product Engineering team to ensure we are capturing the data we need from our applications for analytics and to iterate on our development practices for the data analytics team.
Be an enthusiastic evangelist of our modern data stack (Fivetran / DBT / Snowflake / Tablea)
Be the resident resource on building standard reports and BI dashboards
Experience & Qualifications
4+ years of experience working in a data / analytics engineering role
High proficiency in Snowflake / Fivetran / dbt / Tableau
High proficiency in SQL and Python
Ability to collect, interpret, and synthesize inputs from various parts of the business into data model requirements
Ability to simplify without being simplistic - ability to communicate complex topics and actionable insights in a compelling way that can be understood by a variety of audiences
Inherent curiosity and analytical follow-through — you can't help but ask ""why?"" and love using data and logic to explore potential solutions
Ability to balance ""Rigor"" and ""Scrappiness"" — you know the difference between 80/20 and giving something 110%; as well as when each is appropriate.
Deep understanding of the first and second order effects of reporting — you know the power of presenting the right data to the right people at the right time
Experience in a data/analytics function at a high-growth startup managing multiple stakeholders and delivering actionable insights
About Jetty

At Jetty, we know renting a home can be a financial challenge. That's why we're on a mission to make renting accessible to everyone. Jetty offers four financial products designed to help our members every step of the renting process: Jetty Deposit, a low-cost security deposit product that dramatically reduces move-in costs; Jetty Rent, a flexible rent payment program to eliminate pricey late rent fees; Jetty Credit, a credit building service that helps renters build credit just by paying rent; and Jetty Protect, an affordable renters insurance product that provides comprehensive coverage in just a few clicks.

Jetty has raised multiple rounds of venture capital from investors including Khosla Ventures, Ribbit Capital, Citi, Valar, and strategic investors. We've built a highly collaborative team working remotely around the country, and we believe in finding the best talent—regardless of where they live. To learn more about life at Jetty, visit jetty.com/careers.

Jetty is firmly committed to building a team as diverse as our Members. We are proud to provide equal employment opportunities for all candidates regardless of race, ancestry, citizenship, sex, gender identity or expression, religion, sexual orientation, marital status, age, disability, or veteran status.

Benefits & Perks
Health (with HSA and FSA options), dental, and vision insurance through Aetna & MetLife
401(k) retirement savings program
Optional life and disability coverage
20 days of PTO + 12 holidays, ""Jetty Winter Break,"" and flexible sick days
Generous parental leave policy
Flexible remote work in any US location (keeping east coast hours)
Stipends to cover WFH set-up, childcare, phone/internet bill, and optional co-working space
Show Less
Report",$97T - $1L,51 to 200 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2015,Unknown / Non-Applicable
DevCare Solutions,Data Engineer,"Columbus, OH","Identifies data sources, develop and maintain data architectures, constructs data catalog and data decomposition diagrams, provide data flow diagrams and documents the process.
Develop conclusions and articulate implications of advanced data analytics results
Ability to data mining/sourcing to be self-sufficient in the production of valuation and the analysis of business cases and ad-hoc requests.
Ability to extract data from multiple data resources; remove corrupted data and fixing coding errors and related problems
Ability to perform analysis to assess the quality and meaning of data; filter data by reviewing reports and performance indicators to identify and correct ode problems.
ability to create compelling data visualizations and communicate insights effectively to technical and non-technical stakeholders is essential for data scientists.
Using tools to identify, analyze, and interpret patterns and rends in complex data sets could be helpful for the diagnosis and prediction
Preparing reports for the management stating trends, patterns, and predictions using relevant data in visualization.
Work with developer and management to identify process improvement opportunities, propose system modification, and devise data governance strategies.
7+ years of Oracle/SQL Database
7+ years of SQL script
7+ years of API designs in RESTful, XML, XSLT, JSON, jQuery.
5+ years of Python and/or other ETL tools
5+ years of SQL/T-SQL
5+ years of PL/SQL Experience
3+ years of Alteryx, Cloudera, Snoflake
3+ years of PowerBI, Denodo
Excellent analytical skills, attention to detail, and problem-solving skills.
Proven ability to handle multiple tasks and projects simultaneously.
Familiar with the Agile Methodology.
Working experience in Database & application performance tuning.
Must possess excellent written and oral communication skills.
Working experience in delivering expected results in unstructured environments.
Works productively and effectively independently without significant management oversight.
Job Types: Part-time, Contract
Salary: $55.00 - $65.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Columbus, OH 43081
Show Less
Report",$55.00 - $65.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2005,Unknown / Non-Applicable
ASA,Data Integration Engineer(Banking),"New York, NY","Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.
Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
SKILLS
10+ years - Enterprise Data Management
10+ years - SQL Server based development of large datasets
5+ years with Data Architecture
3+ years experience in Finance / Banking industry some understanding of Securities and
Banking products and their data footprints.
2+ years Python coding experience
Proficient with Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
Capable of discussing enterprise level services independent of technology stack
Experience with Cloud based data architectures, messaging, analytics
Superior communication skills
Cloud certification(s)
Note
Any experience with Regulatory Reporting is a Plus
Job Type: Contract
Salary: $100.00 - $103.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10020: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL Server: 10 years (Preferred)
python: 2 years (Preferred)
Data management: 10 years (Preferred)
Work Location: One location
Show Less
Report",$100.00 - $103.00 Per hour,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1999,$25 to $50 million (USD)
NucleusTeq,Data Engineer,Remote,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$5 to $25 million (USD)
Apple,Data Engineer,"Cupertino, CA","Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number:200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.
To apply to this job, click Apply Now
Show Less
Report",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD)
Bloom Insurance,Data Engineer I,Remote,"Data Engineer I
To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs.

Essential Functions
Day to day functions include the following:
Design data models and develop database structures in Microsoft SQL server.
Write various database objects like stored procedures, functions, views, triggers for various front end applications.
Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks.
Create database deployment packages for deploying changes.
Identify & repair inconsistencies in data, database tuning, query optimization.
Able to generate ad hoc data on demand.
Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise manner
Develop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting.
Develop & maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio.
Documentation
Optimization recommendations
Day to day troubleshooting
.NET Programming as needed

Education/Experience
BA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experience
Solid experience with various versions of MS SQL Server and TSQL programming
Microsoft Certified DBA a plus

Skills/Knowledge
Strong experience in writing efficient SQL code
Working knowledge of SQL Server Management Studio (SSMS)
Knowledge of SQL Server Reporting Services (SSRS)
Knowledge of SQL Server Integration Services (SSIS)
Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plus
Knowledge of data science technologies is a plus
Clear, concise communication skills, excellent organizational skills
Highly self-motivated and directed
Keen attention to detail
High level of work intensity in a team environment
High integrity and values-driven
Eager for professional development
Experience and understanding of source control management a plus
What We Offer
At Bloom, we offer an engaging, supportive work environment, great benefits, and the opportunity to build the career you always wanted. Benefits of working for Bloom include:
Competitive compensation
Comprehensive health benefits
Long-term career growth and mentoring
About Bloom
As an insurance services company licensed in 48 contiguous U.S. states, Bloom focuses on enabling health plans to increase membership and improve the enrollee experience while reducing costs. We concentrate on two areas of service: technology services and call center services and are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients.
Ascend Technology ™
Bloom provides advanced sales and enrollment automation technology to the insurance industry through our Ascend ™. Our Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership.

Bloom is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2006,$25 to $50 million (USD)
Microsoft,Data Pipelines Engineer,"Redmond, WA","Data Pipelines Engineer

Xbox Game Studios Publishing is expanding to include a dedicated Data Pipelines Engineer for our growing organization. You’ll work closely with internal colleagues and external partner studios to design and implement data pipelines that meet their analytics and business intelligence needs. Data sources include commerce, general platform, and game specific telemetry. Leverage your skills to design and implement solutions that transform and aggregate gaming ecosystem data while providing access and query capability using the latest Azure based technologies. This is an exciting, highly integrated, and collaborative role within our growing team.

This candidate would bring experience, perspective, and expertise from other game studios or complementary industries.

#XGSPublishingjobs
Responsibilities
Responsibilities
Engineer pipelines to procure, clean, shape, validate, and deliver a diverse array of essential facts and dimensions from a variety of data sources on a daily basis.
Collaborate with Program Managers, Engineers, Analysts, and other stakeholders to thoroughly understand and develop solutions for data acquisition, design, and delivery challenges.
Maintain a broad knowledge of our data sources in order to identify opportunities and anticipate changes.
Conform to privacy and security policies that govern data handling, and rigorously advocate for best practices in partner scenarios.
Leverage Azure Data Factory and related internal tools to implement, automate, and maintain game specific data pipelines that transform, aggregate and reliably serve internal and external analytics customers.
Leverage Microsoft/Azure Purview and related internal tools to actively monitor and govern privacy and security policies on the storage and use of our data.
Leverage multiple Azure-based tools and methods to monitor the timeliness and quality of shared data assets, communicate issues to stakeholders, and partner with data source owners to resolve problems.
Develop foundational knowledge and methods that address the unique nature of games data and share them with our internal and external customers.
Apply DevOps and Infrastructure as Code practices to create highly maintainable and reusable data pipelines across our growing array of tenants.
Consistently identify and ship incremental, measurable improvements to tools, instrumentation, models, algorithms, and/or products/services that enable customer/business goals.
Continuously improve game data assets by adding attributes, mitigating data limitations, and using existing sources in ways that enhance results.
Help build and promote an inclusive environment within our organization where people’s lived experiences are embraced and valued.
Other
Embody our culture and values
Qualifications
Required Qualifications:
Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 4+ years experience in business analytics, data science, software development, data modeling or data engineering work
Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 3+ years experience in business analytics, data science, software development, data modeling or data engineering work

OR equivalent experience
4+ years of experience as an engineer in a data-focused role for shipped software products or services, with at least 3+ years working with data from live services, including the continuous management of ETL/data pipelines, processing, storage, and governance
Engineering/DevOps Experience (code reviews, testing, alerting, documenting, retrospectives, incident analysis)
Experience with Azure’s core data and analysis services (Data Factory, Data Lake, Data Explorer, SQL, Power BI)
Preferred Qualifications:
Experience in SQL and KQL (Kusto)
Experience with additional Azure data and analysis services (Databricks, Synapse, Cosmos, Storage/Blob/Table)
Experience in game development or game operations
Experience in SCOPE (Cosmos)
Experience with scripted programming languages, preferably C#
Data Engineering IC4 - The typical base pay range for this role across the U.S. is USD $112,000 - $218,400 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $145,800 - $238,600 per year.

Microsoft has different base pay ranges for different work locations within the United States, which allows us to pay employees competitively and consistently in different geographic markets (see below). The range above reflects the potential base pay across the U.S. for this role (except as noted below); the applicable base pay range will depend on what ultimately is determined to be the candidate’s primary work location. Individual base pay depends on various factors, in addition to primary work location, such as complexity and responsibility of role, job duties/requirements, and relevant experience and skills. Base pay ranges are reviewed and typically updated each year. Offers are made within the base pay range applicable at the time.

At Microsoft certain roles are eligible for additional rewards, including merit increases, annual bonus and stock. These awards are allocated based on individual performance. In addition, certain roles also have the opportunity to earn sales incentives based on revenue or utilization, depending on the terms of the plan and the employee’s role. Benefits/perks listed here may vary depending on the nature of employment with Microsoft and the country work location. U.S.-based employees have access to medical, dental, and vision insurance, a 401(k) plan and company match, short-term and long-term disability coverage, basic life insurance, and wellbeing benefits, among others. U.S.-based employees also receive, per calendar year, up to 10 scheduled paid holidays, and up to 80 hours Holistic Health Time Off. Additionally, hourly/non-exempt employees accrue up to 120 hours paid vacation time, and salaried/exempt employees have Discretionary Time Off (DTO).

Our Commitment to Pay Equity
We are committed to the principle of pay equity – paying employees equitably for substantially similar work. To learn more about pay equity and our other commitments to increase representation and strengthen our culture of inclusion, check out our annual Diversity & Inclusion Report. ( https://www.microsoft.com/en-us/diversity/inside-microsoft/annual-report )

Understanding Roles at Microsoft
The top of this page displays the role for which the base pay ranges apply – Data Engineering IC4.The way we define roles includes two things: discipline (the type of work) and career stage (scope and complexity). The career stage has two parts – the first identifies whether the role is a manager (M), an individual contributor (IC), an admin-technician-retail (ATR) job, or an intern. The second part identifies the relative seniority of the role – a higher number (or later letter alphabetically in the case of ATR) indicates greater scope and complexity.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.
Show Less
Report",$1L - $2L,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1975,$10+ billion (USD)
VirginPulse,Data Engineer I,Remote,"Overview:
Now is the time to join us!
At Virgin Pulse we value and celebrate diversity and we are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Why? Because diversity inspires innovation, collaboration, and challenges us to produce better solutions. But more than this, diversity is our strength, and a catalyst in our ability to #changelivesforgood.
Responsibilities:
Who are you?
Data Engineer I perform development activities with the guidance of another member of the data engineering team. You will work closely with account management, ETL, data warehouse, business intelligence, and reporting teams as you develop data pipelines and enhancements and investigate and troubleshoot issues.

In this role you will wear many hats, but your knowledge will be essential in the following:
Extracting, cleansing, and loading data.
Building data pipelines using SQL, Python, and other technologies.
Triage incoming bugs and incidents.
Perform technical operation tasks.
Investigate and troubleshoot issues with data and data pipelines.
Participation in sprint refinement, planning, and kick-off to help estimate stories, raise awareness and additional implementation details.
Help monitor areas of the data pipeline and raise awareness to team when issues arise.
Performing quality assurance work to verify the accuracy of code and data results
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Qualifications:
What you bring to the Virgin Pulse team
In order to represent the best of what we have to offer you come to us with a multitude of positive attributes including:
1– 2 years or less experience in data engineering
SW certification or degree in IT related field
You also take pride in offering the following Core Skills, Competencies, and Characteristics:
Solid grasp of modern relational and non-relational models and differences between them.
Proficiency in writing SQL, the use of Excel, and some analytical tools.
Understanding of REST API.
Understanding of JSON.
Detail oriented and able to examine data and code for quality and accuracy.
Knowledge of Agile environments, including Scrum and Kanban methodologies
Python / R / programming language experience preferred
ETL experience preferred
AWS Lambda / Console experience preferred
Git experience preferred
No candidate will meet every single desired qualification. If your experience looks a little different from what we’ve identified and you think you can bring value to the role, we’d love to learn more about you!

#LI-REMOTE

Why work at Virgin Pulse?
We believe a career should provide competitive pay and benefits, a collaborative and supportive culture and cutting-edge technology and services. Virgin Pulse is an equal opportunity organization and is committed to diversity, inclusion, equity and social justice. To that end, we make a particular effort to recruit candidates from minoritized backgrounds to apply for open positions.

In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges up to $80,000 annually. Note that salary may vary based on location, skills, and experience. This position is eligible for [10%] target bonus/variable compensation as well as health, dental, vision, mental health and other benefits.
Show Less
Report",$80T,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$25 to $50 million (USD)
Adroit Software Inc,Data Engineer,"Smithfield, RI","Role Description
The team is hiring a Lead Developer and you will be responsible for working with Architecture, Data Governance and Business Intelligence teams to support Fidelity’s or vendor Data Management tools and evaluate new technologies. You will possess a passion for continuous learning and upskilling in new technologies and a flair for exploring.
Required Qualifications
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server will be a huge plus
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization
Additional Experience
Experience with Metadata management solutions / Data lineage is a plus
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus
Vendor management is a plus
Some QA/Testing experience is a plus
Some Kubernetes / Docker experience is a plus
Strong communication and presentation skills
COVID Work Policy
Safety is our top priority. Once we can be together in person with fewer safety measures, this role will follow our dynamic working approach. You’ll be spending some of your time onsite depending on the nature and needs of your role.
Dynamic Working – Post Pandemic
Our aim is to combine the best of working offsite with coming together in person. For most teams this means a consistent balance of working from home and office that supports the needs of your role, experience level, and working style.
Your success and growth is important to us, so you’ll want to enjoy the benefits of coming together in person – face to face learning and training, quality time with your manager and teammates, building your career network, making friends, and taking full advantage of cultural and social experiences Fidelity provides for you.
Description
Fidelity TalentSource is your destination for discovering your next temporary role at Fidelity Investments. We are currently sourcing for a Data Engineer to work at Fidelity's location in Smithfield, RI, Durham, NC or Westlake, TX!
Fidelity Brokerage Technology (FBT) enables business partners to win in their respective marketplaces by designing, building and maintaining the technology platforms and products of Fidelity Institutional, Personal Investing and Workplace Investing.
FBT Business Intelligence Team is looking for a Lead Technologist to support our Data Management Tools and evaluating new Technologies in support of Architecture directives!
Role Description
The team is hiring a Lead Developer and you will be responsible for working with Architecture, Data Governance and Business Intelligence teams to support Fidelity’s or vendor Data Management tools and evaluate new technologies. You will possess a passion for continuous learning and upskilling in new technologies and a flair for exploring.
Required Qualifications
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server will be a huge plus
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization
Additional Experience
Experience with Metadata management solutions / Data lineage is a plus
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus
Vendor management is a plus
Some QA/Testing experience is a plus
Some Kubernetes / Docker experience is a plus
Strong communication and presentation skills
COVID Work Policy
Safety is our top priority. Once we can be together in person with fewer safety measures, this role will follow our dynamic working approach. You’ll be spending some of your time onsite depending on the nature and needs of your role.
Dynamic Working – Post Pandemic
Our aim is to combine the best of working offsite with coming together in person. For most teams this means a consistent balance of working from home and office that supports the needs of your role, experience level, and working style.
Your success and growth is important to us, so you’ll want to enjoy the benefits of coming together in person – face to face learning and training, quality time with your manager and teammates, building your career network, making friends, and taking full advantage of cultural and social experiences Fidelity provides for you.
Special Instructions
MUST HAVE: ETL Informatica, SQL, snowflake, basic python
ETL developer with Informatica
Strong SQL – Snowflake and SQL Server
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required – one week every 3 months
Pre-Screen Questions:
1. What's the difference between a global and local variable? How would you update a global variable in a function?
Global Variable - is defined outside of any function and can be accessed anywhere in the current python file
Local Variable - is defined inside a function and can only be accessed within that same function.
Update global Variable: After declaring the function, you would then use the ""global"" keyword followed by the variable name. This would allow you to edit the global variable inside a function.
Job Type: Contract
Salary: $60.00 - $80.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Parental leave
Vision insurance
Compensation package:
1099 contract
Hourly pay
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Smithfield, RI 02917: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
W2 Only No C2c
Education:
Bachelor's (Preferred)
Experience:
Brokerage (Preferred)
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Snowflake (Preferred)
python (Preferred)
Willingness to travel:
100% (Preferred)
Work Location: One location
Show Less
Report",$60.00 - $80.00 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
KesarWeb,Data Engineer,"Los Angeles, CA","We are searching for an accountable, multitalented data engineer to facilitate the operations of our data scientists. The data engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.
To ensure success as a data engineer, you should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. A formidable data engineer will demonstrate unsatiated curiosity and outstanding interpersonal skills.
Data Engineer Responsibilities:
Liaising with coworkers and clients to elucidate the requirements for each task.
Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
Reformulating existing frameworks to optimize their functioning.
Testing such structures to ensure that they are fit for use.
Preparing raw data for manipulation by data scientists.
Detecting and correcting errors in your work.
Ensuring that your work remains backed up and readily accessible to relevant coworkers.
Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.
Data Engineer Requirements:
Bachelor's degree in data engineering, big data analytics, computer engineering, or related field.
Master's degree in a relevant field is advantageous.
Proven experience as a data engineer, software developer, or similar.
Expert proficiency in Python, C++, Java, R, and SQL.
Familiarity with Hadoop or suitable equivalent.
Excellent analytical and problem-solving skills.
A knack for independence and group work.
Scrupulous approach to duties.
Capacity to successfully manage a pipeline of duties with minimal supervision.
Job Type: Full-time
Pay: $98,000.00 - $106,000.00 per year
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
Schedule:
Monday to Friday
Work Location: Remote
Show Less
Report",$98T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
United Airlines,Data Engineer,"Chicago, IL","Description
There’s never been a more exciting time to join United Airlines. We’re on a path towards becoming the best airline in the history of aviation. Our shared purpose – Connecting People, Uniting the World – is about more than getting people from one place to another. It also means that as a global company that operates in hundreds of locations around the world with millions of customers and tens of thousands of employees, we have a unique responsibility to uplift and provide opportunities in the places where we work, live and fly, and we can only do that with a truly diverse and inclusive workforce. And we’re growing – in the years ahead, we’ll hire tens of thousands of people across every area of the airline. Our careers include a competitive benefits package aimed at keeping you happy, healthy and well-traveled. From employee-run ""Business Resource Group"" communities to world-class benefits like parental leave, 401k and privileges like space available travel, United is truly a one-of-a-kind place to work. Are you ready to travel the world?
Our Marketing and Loyalty team is the strategic force behind United’s industry-leading brand and experience, supporting revenue growth by turning customers into lifelong United flyers. Our marketing communications, market research and brand teams drive travelers’ familiarity and engagement with the United brand. Our product, design and service teams bring the signature United brand to life in our clubs and onboard our aircraft, ensuring a seamless, premier experience. And when customers choose United again and again, that’s because the loyalty team has been hard at work crafting an award-winning program. Our loyalty team manages United MileagePlus®, building travel and lifestyle partnerships that customers can engage with every day, and works with our Star Alliance partners to ensure United can take you anywhere you want to go.
Key Responsibilities:
As a Data Engineer, you will focus on production readiness of unique member transaction and consumption data and elements such as formats, resilience, scaling, storage, and security. In this role, you will prepare the infrastructure for, analyze, and synthesize personal level customer and unique member transaction and computation data within United’s advertising channels. Additionally, you will be responsible for designing, building, testing, integrating, managing, and optimizing customer interaction data from a variety of sources.
Creates and maintains scalable data pipelines
Balance priorities among technology, data science, and business strategy functions
Design and build efficient data structures for both dev and prod environments
Consult on new data streams and troubleshoot existing streams
United values diverse experiences, perspectives, and we encourage everyone who meets the minimum qualifications to apply. While having the “desired” qualifications make for a stronger candidate, we encourage applicants who may not feel they check ALL of those boxes! We are always looking for individuals who will bring something new to the table!
Qualifications
Required
Bachelor's degree or equivalent experience
2+ years of experience building data pipelines via ETL (extract, transform, load)
Experience with cloud computing solutions (AWS, Google, Azure, Databricks, etc.)
Proven proficiency in Python or equivalent
Expertise building data ingestion tools such as APIs
Familiarity with SQL
Demonstrated ability to ingest unstructured data and load into a relational structure
Must be legally authorized to work in the United States for any employer without sponsorship
Preferred
Master's degree or equivalent experience
Computer science, engineering, statistics, or mathematics
5+ years of building complex data architectures with large volumes of data
AWS associate or professional certificate(s)
Experience with customer-centric data (CRM, HR, or other person-centric data structures)
Experience supporting a marketing or other commercial stakeholder group
Experience working with data science teams
Machine learning (mlOps) or reliability (devOps) experience

Equal Opportunity Employer - Minorities/Women/Veterans/Disabled/LGBT
Show Less
Report",$82T - $1L,10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1926,$10+ billion (USD)
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
FocuzMindz,"AWS Data Architect/ Engineer with Redshift, RDS","Alexandria, VA","We are seeking an AWS Data Engineer to join our growing team.
The qualified applicant will play a key role in the data warehouse migration as part of the Enterprise Data Analytic Services program at a federal agency located in Alexandria, VA. Hybrid work options are available.
AWS Data Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premises to cloud environment, evaluation, and optimization.
Full time opportunity
Alexandria VA
Hybrid role -onsite 2 days per week
Responsibilities:
Work on automating migration process for RDS and RedShift scripts in AWS from Dev to Production.
Performance tune RDS/RedShift SQL queries.
Maintain/resize the clusters for RDS and RedShift.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including computing, storage networking, database, management tools, security, identity, and compliance.
Good knowledge of RDS Postgres and AWS Redshift.
5+ years of experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Experience building infrastructure inside of AWS via code. Familiarity with tools such as Terraform or CloudFormation.
5+ years of experience architecting/deploying/operating solutions built on AWS.
Experience using ETL tools such as Alteryx, Snap Logic, Matillion, SAS DI Studio, Informatica, or equivalent tools.
Education Requirements:
Bachelor's degree in engineering, data science, computer science,
AWS and/or Data Science certification is a plus!
Clearance Requirements:
Ability to obtain and hold a Public Trust Clearance.
mary.a@stepstalent.com
Job Type: Contract
Pay: From $159,583.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Alexandria, VA 22301: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 9 years (Required)
AWS: 9 years (Required)
Security clearance:
Confidential (Required)
Work Location: Hybrid remote in Alexandria, VA 22301
Show Less
Report",$2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
AgileEngine,Senior Big Data Engineer,Remote,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Analyze, develop and implement database designs, data models and logical data specifications
Review and improve existing databases/data warehouse designs
Contribute to systems architecture analysis and designProvide comprehensive consultation to infrastructure administrators and business analysts in resolving issues
Perform data modeling studies and develop detailed data models
Work with Business Analysts and staff to establish and maintain consistent data element definitions
Participate in the development and maintenance of corporate data architecture, data management standards and conventions, data dictionaries and data element naming standards
Research and evaluate alternative solutions and recommend the most efficient and cost effective data related solutions for improved data integrity
Performance tune ETL jobs and data models
Migrate ETL code from development to production environments
Assist in the design and development of BI dashboards
Perform DW training as needed
Must haves
Experience in AWS cloud services.
Working in big data projects with 4+ years of experience
Strong working experience Apache Spark
Good experience programming language Scala or Java or both
The benefits of joining us
Professional growth: Accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: We match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: Join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: Tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote
Show Less
Report",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"AGM Tech Solutions, LLC",Data Engineer,"Alpharetta, GA","We have an excellent 6-month contract-to-hire opportunity with our Global Leader client.
Candidate must be local to Alpharetta GA (Hybrid Flexibility) - 3 days a week on-site.
Basic
Work experience with ETL, Data Modeling, and Data Architecture.
Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL.
Experience operating very large data warehouses or data lakes.
Preferred
Experience in designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Comfortable using PySpark APIs to perform advanced data transformations.
Familiarity with implementing classes with Python.
Summary
Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue
Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python.
Design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Job Type: Contract
Pay: $70.00 per hour
Benefits:
401(k)
Flexible schedule
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Machine learning: 2 years (Required)
Work Location: One location
Show Less
Report",$70.00 Per hour,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Innova Solutions Inc.,Data Engineer/Data Analyst,"Richmond, VA","Position Summary
The person will have a mix of highly technical data quality controls development, data analysis and reporting responsibilities to include writing complex SQL queries, some python code analysis, extensive data analysis, building DQ controls metrics reports, defining tech data controls strategy, working with metadata, architecture and development teams on the resolution to DQ controls issues
Primary Skill
MySQL
Secondary Skill
Tertiary Skill
Required Skills
SQL and DQ Controls Experience
DQ Controls Development and Strategy Skills
Business Analyst
Data Architecture
Desired Skills
Python, metadata, business intelligence reporting, BA
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richmond, VA 23173: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
MySQL: 5 years (Required)
Data Quality: 5 years (Required)
Work Location: Hybrid remote in Richmond, VA 23173
Show Less
Report",$60.00 - $65.00 Per hour,10000+ Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1998,$2 to $5 billion (USD)
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Stratford Solutions Inc.,Senior Data Engineer,Remote,"Job title: Senior Data Engineer

Job Location: REMOTE (EST ZONE) M-F 9-5 (35/hrs a week)

Job type: 8 Month Contract

Pay Rate: $100-$125/hr

SCOPE OF SERVICES

Seeking a Data Engineer role to ensure the efficient and successful implementation and support of complex data engineering solutions for City agencies. This resource should demonstrate a solid understanding of industry-standard implementation methodologies using data engineering technologies, tools, and processes.

TASKS:
? Create and maintain optimal data pipeline architecture that is coherent and scalable, based on best practices of integrating data into a consolidated repository.

? Perform the technical design, development, and component testing of repository changes.

? Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics.

? Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, cloud, and big data technologies.

? Develop ETLs to move data securely from source to target systems.

? Create, update, and maintain system documentation.

? Develop new or build against existing APIs for data access or landing data as output for further downstream consumption in the appropriate target data store.

? Perform special projects and initiatives as assigned.

MANDATORY SKILLS/EXPERIENCE Note: Candidates who do not have the mandatory skills will not be considered

8+ years of experience in writing SQL.
8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl
DESIRABLE SKILLS/EXPERIENCE:
Hands on experience developing Microsoft PowerBI solutions.
5+ years hands-on experience in development with the suite of tools from Informatica PowerCenter and B2B Data Transformation.
Experience using Oracle 10g/11g, SQL Server and/or a database appliance.
Knowledge of metadata-driven enterprise reporting platforms.
Show Less
Report",$100.00 - $125.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
I28 Technologies,Jr.Data Engineer - Python / Hadoop / PySpark,"Honolulu, HI","Banking domain knowledge
knowledge in software development and delivery.
knowledge with Neo4j, GraphQL
Good aptitude, knowledge problem-solving abilities, and analytical skills, ability to take ownership as appropriate
Should be able to do coding, debugging, performance tuning, and deploying the apps to the Production environment.
knowledge working in Agile Methodology
Ability to learn and help the team learn new technologies quickly
Take up the complete planning design implementation/ UAT delivery of the project
knowledge communication and coordination skills
Job Type: Full-time
Salary: $55,000.00 - $60,000.00 per year
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: One location
Show Less
Report",$55T - $60T,1 to 50 Employees,Company - Private,Finance,Accounting & Tax,#N/A,Less than $1 million (USD)
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
Gladly,Senior Data Engineer,"San Francisco, CA","Gladly is a Radically Personal Customer Service Platform that puts people at the center of a single, lifelong conversation. We enable companies to talk to their customers they way people talk to their friends: seamlessly across voice, email, SMS, chat, and social media.
Gladly's data products are a foundation for enabling contact center leaders to understand their team's performance and identify opportunities for their company. Because of Gladly's unique approach to customer service, the data we provide is a key differentiator, not an afterthought. Our data warehouse also gives Gladly's customer success team the insights to help customers optimize their use of the product. We create a range of metrics and datasets based on carefully designed events and data models. We are looking for a data engineer to join our small, fast-growing and high impact team.
What you'll do
Own and drive projects, as well as communicate with stakeholders on requirements, progress and delivery.
Teach. Provide technical guidance and mentorship in software engineering best practices while demonstrating these as an individual contributor.
Collaborate. Work closely with small, nimble, cross-functional teams of engineers, product managers, designers, and business teams.
Contribute. Build a best-in-class data pipeline with a few key attributes:
repeatable via infrastructure-as-code
testable, with verification of correctness
reliable and always-on
low latency (on the order of minutes)
observable.
Work with experienced colleagues who will be eager to share their knowledge, provide mentorship and help you grow your career.
Have opportunities to learn and work with technologies used at Gladly like Snowflake, dbt, Debezium, Looker, PostgreSQL, Kafka, Docker, Kubernetes, AWS, Redis, Node.js, Go, Python.
You'll be successful by
Being eager to learn Gladly's business domain and apply this knowledge in building the innovative product.
Self-organizing and prioritizing your work based on the impact to the customer.
Understanding how to balance pragmatic solutions with best practices of data engineering.
Having passion for making the most of our existing technologies and introducing the right tools for problem at hand.
Showing ownership and pride in your work by promoting data best practices and making them easy for engineering teams to adopt as well as providing ongoing maintenance and support.
We're excited about you because you have
5+ years of engineering experience including 2+ years of working with ETL pipelines, data transformation and modeling.
Strong teamwork skills. You love participating with high-performing teams of engineers.
Customer-centricity and product focus. You look at everything you create through the lens of how it improves things for the end-customer. You are comfortable communicating how various technical approaches might impact product behavior (and vice versa).
Learning mentality because nobody checks every box. You aren't intimidated by new domains or technology; you're willing to dive into documentation/videos, talk to your teammates, and experiment to become well-versed.
Willingness to work across the development stack. You're comfortable with working on data pipelines, transformations and implementing insights. You're willing to jump into Gladly backend applications on occasion.
Operational expertise. You value robust observable solutions with actionable monitoring and the importance of tooling for troubleshooting and resolving issues.
Research has shown that individuals from marginalized groups are less likely to apply to jobs where they don't meet 100% of the criteria. Gladly values diversity of experience, so if you believe you have the right skill set, we welcome you to apply - even if you don't check every box in the job description. We're committed to an inclusive workplace and would love to see if you could be the next great addition to our team.
Compensation
$156,000-$215,000 annually.
For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above.
Working at Gladly
People are not just at the heart of our product, they're at the heart of our company.
We value diverse perspectives and hire new people to enrich our mix, not keep it the same.
We believe in open communication and share in an inclusive, open culture.
We have embraced remote work and make it easy for our team to work from anywhere, but we also invest in opportunities to get the teams together in person regularly.
We learn from each other, and we help each other learn.
We provide opportunities to move between teams to learn and contribute to other cool technologies used at Gladly.
We have a strong work ethic, but value life outside of work, too.
Our focus is on people and that starts with our employees. As an employee you can count on:
Competitive salaries, stock options
Medical, Dental, Vision and Life insurance
Generous paid time off
Generous paid Parental Leave
401K
Flexible Spending Accounts
Wellness and home office stipends
Founded in 2014 by a team of repeat entrepreneurs with multiple successful exits, Gladly is reinventing customer service. By focusing on customers instead of tickets, we are disrupting a $70B market and are proud to count Crate and Barrel, Warby Parker and many other innovative brands as customers. Gladly has raised over $110M from Greylock Partners, NEA, GGV Capital, Glynn Capital and JetBlue Tech Ventures.
Gladly has made the decision to become a fully distributed company, allowing employees to live anywhere in the United States, and candidates to come from nearly any geographical region. That said, we also highly value our collaborative and creative culture and commit to meeting in real life as a company at least once per quarter when it is safe to do so.
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2014,Unknown / Non-Applicable
Violet Ink,Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Amazon,Data Engineer - Flink,"Austin, TX","The successful candidate in this role will have:
Experience building and maintaining enterprise-scale (Terabyte - Exabyte) data pipelines.
Experience using modern open-source technologies and cloud services (SNS, SQS, MSK, ECS, EC2, DynamoDB, Kinesis, EMR, Kafka, Flink, Spark)
Experience with modern compression technologies (Orc, Spark)
Experience working with customers to model and onboard datasets to fit customer requirements.
Experience developing data pipeline parsers using Scala.
To follow up with any questions, please contact Ajitabh at # 408-907-2956
Job Type: Contract
Pay: $72.00 - $80.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
LinkedIn profile is required. Please share your LinkedIn profile
Experience:
Apache Flink: 1 year (Required)
Scala: 2 years (Required)
Parsing of Data: 2 years (Required)
AWS tools: 2 years (Required)
Work Location: One location
Show Less
Report",$72.00 - $80.00 Per hour,10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994,$10+ billion (USD)
Kairos Technologies Inc,Sr Data Engineer with AWS (Hybrid Onsite),"Boston, MA","Greetings from Kairos Technologies Inc,
Role: Sr Data Engineer with AWS
Location: Boston, MA (Hybrid Onsite)
Duration: 12 months
Experience: 9+ yrs.
Hybrid – Weekly once/ twice need to work from Office
RESPONSIBILITIES
Familiarity with data lake, data warehouse or data lake environments and related topics . Has a proven track record to work with vendors to deploy external SaaS solutions and integrate with existing systems.
In depth with data lakes/ data environments including ETL (PySpark), data Catalogs (Glue, Alation), API interfaces, Cloud data warehouses such as Redshift, Querying engines such as Trino.
Agile approaches to building cloud native solutions using CI/CD, containers, Kubernetes, GitOPS, etc..
Strong automation and development skills in terraform, CloudFormation, and other languages like Python, and bash.
8+ years of total IT experience, with at least 4 years in AWS services such as EMR, EC2, S3, IAM, Glue andRedshift and 2+ years of experience in Infrastructure as code technologies like Terraform, CloudFormation.
4+ years of Python, SQL experience is mandatory.
Thanks& Regards,
K Hemanth Kumar | Sr IT Technical Recruiter | Kairos Technologies Inc
Job Type: Contract
Pay: $65.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Boston, MA 02108
Speak with the employer
+91 9728535149
Show Less
Report",$65.00 - $70.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Findability Sciences,Snowflake Data Engineer pipeline,"Houston, TX","Snowflake data engineers will be responsible for architecting and implementing very large scale data intelligence solutions around Snowflake Data Warehouse.
A solid experience and understanding of architecting, designing and operationalization of large scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must.
Developing ETL pipelines in and out of data warehouse using combination of SQL and Snowflakes Snow SQL
Writing SQL queries against Snowflake.
Developing scripts Unix, Python etc. to do Extract, Load and Transform data
Provide production support for Data Warehouse issues such data load problems, transformation translation problems
Translate requirements for BI and Reporting to Database design and reporting design
Understanding data transformation and translation requirements and which tools to leverage to get the job done
Understanding data pipelines and modern ways of automating data pipeline using cloud based
Testing and clearly document implementations, so others can easily understand the requirements, implementation, and test conditions.
Job Types: Full-time, Part-time, Contract, Temporary
Salary: $42.14 - $70.29 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Night shift
Ability to commute/relocate:
Houston, TX 77001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 3 years (Preferred)
Work Location: One location
Show Less
Report",$42.14 - $70.29 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
MetroSys,Data Migration Engineer,Remote,"Job Description
Context
We have a great project and need Migration Engineers to assist with migration of an enterprise client to a new data center. This new IT production environment is based in Redhat Linux, IBM AIX, Sun Solaris, UNIX and HP UX. We are searching for an engineer to support the migration by migrating servers, storage and databases to the new environment. We use a strict step-by-step plan (factory plan) to efficiently execute the migration.

Competence
Bachelor of Science / Master's degree
Minimal 3-5 years of relevant work experience within an enterprise environments
Advanced knowledge of Redhat Linux and UNIX
Advanced knowledge of IBM AIX, Solaris, and HP UX
Some knowledge of IBM XIV, Pure Storage, HPE Nimble and 3PAR storage preferred
Experience with databases (Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
The candidate has a hands-on mindset, a strong customer- and problem-solving orientation, shows fast results, and has demonstrated good communication skills, especially in an international IT organization. To achieve the project goals, the candidate is able to liaise directly with all stakeholders. The candidate has a clear focus on results and quality, and is eager to develop quickly as a project leader, serving customers.

Activities
Intake / analysis of applications for migration
Creation or update of migration run books
Migration of VMware instances to the new platform
Creation of storage disks, virtual storage devices
Reconfiguration of servers, including storage & network
1RfnvbOr2v
Show Less
Report",$50.00 - $70.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Publix,Senior Software Engineer – Data Analytics,"Lakeland, FL","Participate in Enterprise Data Warehouse and business application team projects with a focus on gathering requirements, design, development, and implementation of cloud solutions
Work with Data Asset Management team and business users to gather detailed analytics requirements
Analyze the data sources, design, and develop Databricks pipelines to ingest the data
Collaborate with the EDW team and Architecture team to structure the new data within Azure Data Lake
Collaborate with Data Asset Management team to maintain metadata


Required Qualifications:
Bachelor’s degree in Computer Science or other analytical discipline or equivalent experience
Minimum 5 years of experience designing, developing, and supporting applications in an enterprise environment
Minimum one year of experience using Azure cloud computing technologies
Minimum one year of hands-on experience using Databricks, ingesting data using pipelines, and loading delta tables
Experience with Apache Spark and Python
Experience working in a Data Lake environment handling structured and unstructured data
Experience working in a fast-paced, innovative environment
Attention to detail with the ability to produce reliable, effective solutions
Excellent communication skills
Possessing a positive attitude and ability to work in a collaborative and energetic team environment
Address: 321 S. Kentucky Ave
Show Less
Report",$1L - $2L,10000+ Employees,Company - Private,Retail & Wholesale,Grocery Stores,1930,$10+ billion (USD)
Norwin,Data Engineer,"South San Francisco, CA","Title: Data Engineer or Data Scientist
Location: South San Francisco, CA 94080(Candidate need to be local and be onsite 3 days a week)
Duration: 6+months(W2 Contract)
Client: Genentech
Description
The systems specialist role will be part of the Workplace Digital Insights team which supports making key data-driven decisions for the Real Estate and Workplace Effectiveness(RE&WE) organization. We help this organization’s forward planning strategies for South San Francisco ranging from developing real estate strategies, to campus planning, building investments, occupancy planning and move management.
Our team designs, builds, integrates and operationalizes technology applications that are customized to meet those organizational business needs. We are responsible for the end to end lifecycle of key applications and data streams that feed our Analytics team and Business Operations teams with high quality, well defined data. These data and data streams are critical to those applications and reporting platforms. Important business data originates from real time sensors, Wi-Fi, other databases, and people, then pipelines into storage, validation and then it is combined with other data in cloud storage, processed by software applications, and ends in visualization and decision support.
This specialized role will have to use a wide range of skills to understand business requests, identify the problem and break it into their business process, functional and system components. A clear and critical thinker is required to then bring new potential solutions to fruition, manage and analyze existing pipelines and data.
This role presents a unique opportunity to work with systems developed and maintained by the group as well as within corporate IT environments, multiple vendor systems, our AWS data lake, and all the pipelines in between. This includes working with data from various Internet of Things (IOT) networks and a great opportunity to collaborate and develop new streams and strategies on our horizon. The position must be filled with someone who has sharp critical thinking, combined with excellent project management and coordination skills, having a strong ability to work with both people representing the business people and those supporting roles within the IT organization. Technical skills are also extremely important, especially skills that support understanding SQL queries, database concepts, API’s, AWS/Cloud services, and developing Tableau as well. You’ll be responsible for understanding how real time data from sensors integrates with corporate systems, combined to cloud based data lake, funnels into data science analytics and will end in visualizations.
Responsibilities
● Understands and writes SQL code in Oracle and Redshift databases to support existing data structures, joins, views. Write and optimize new SQL to support new storage needs, reporting needs, and better performance and efficiency
● Makes decisions and solves problems using sound, inclusive reasoning and judgment. Gathers and analyzes information to fully understand a problem and proactively anticipates needs and prioritize action steps.
● Partners and communicates well with internal stakeholders, business partners, and Global IT to understand, define and help deliver technology needs
● Implements vetted strategies developed by the business to achieve technical roadmaps
● Manages project delivery end-to-end, in other cases helping our IT partner stay on project delivery milestones
● Wrangling structured, unstructured and poorly structured data into appropriate data structures.
● Auditing data in databases and/or reports with a goal of improving data quality from systems or data owners
● Identify opportunities to further build out our IoT strategy
● Develops new and supports existing Tableau dashboard platform, including how to improve sql queries and data management for optimal performance in reporting
Experience/Skills/Abilities:
● At least 3 years of experience working as a Systems Integrator, Data Engineer, Software Engineer or similar position demonstrating the ability to design and implement automation, data modeling, data wrangling, data analysis, and data vision solutions to complex problems, processes, and scenarios. Familiarity with common data structures and languages.
● BS Computer Science, Computer/Electrical Engineering, or Math Degree or relevant experience.
● Experience with IoT, cloud computing, distributed data systems
● Strong capabilities in SQL and Tableau, Excel/Google Suite
● Understands AWS platform (Amazon Redshift, S3, EC2, Glue jobs, etc.)
● Friendly and approachable, with strong communication and presentation skills
● Desire to keep current with a challenging and evolving environment
● Team focused and self-motivated. Able to work as part of a coordinated team, yet independently when necessary
● Proven abilities to take initiative and to be innovative; have an analytical mind with a problem-solving aptitude
● Demonstrated ability in leading technology projects
● Demonstrated experience in bridging business requirements and technical development
● Strong communications skills maintaining ties to product developers and stakeholders
● Demonstrated experience with software lifecycle management
Desirable Experience (but not required):
● Experience working with statistical teams and/or data scientists
● Tools/Programming Experience: Python, R, web services, other languages(e.g. Java, C++, scripting languages)
● AWS Certifications and working experience
● Experience with LEAN/KANBAN/SCRUM development methodologies is desirable
Job Type: Contract
Salary: Up to $90.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 4 years (Required)
SQL: 4 years (Required)
Tableau: 2 years (Required)
AWS: 2 years (Preferred)
Work Location: One location
Show Less
Report",$90.00 Per hour,1 to 50 Employees,Company - Private,Manufacturing,Machinery Manufacturing,#N/A,Unknown / Non-Applicable
Denodo Technologies,Data Services Engineer,"New York, NY","Company Description

Denodo is a high growth, market leading enterprise software company backed by HGGC. We are recognized as a leader by Gartner and Forrester, and uniquely positioned to address the data fragmentation problems that exist in many enterprises.
We thrive in dynamic environments, and at the risk of sounding cliché, we work hard, and we play hard. People at Denodo are builders at heart. Our global teams are constantly interacting and working together to empower people around the world, build community and connect in meaningful ways.
Denodo's success is founded on being innovative and creative, on delivering the best solutions with the highest levels of customer satisfaction and on having a unique piece of technology. A company can only be as forward-thinking as its people, which explains why we have become the leading developer of Data Virtualization, Data Services and Cloud Data Integration technologies and solutions for the enterprise.
At Denodo, we are like a family and it is of the utmost importance to us that we help support your professional growth every step of the way.

Job Description

The Opportunity
Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization.
Your mission: to help Denodo users achieve and maintain success through accelerated adoption and productive use of Denodo solutions.
In this role you will successfully employ a combination of high technical expertise and client management skills to conduct troubleshooting and issue resolution, provide technical guidance and advice through remote or on-site consulting engagements, deliver timely and complete solutions to customer issues, and being a critical point of contact for getting things done among Denodo, partner and client teams. Our client’s rely on data in their daily operations. You will be called upon to creatively assist our customers in ensuring they are successful in their data processing endeavors utilizing our products.
Location New York, NY
Salary range: $90/yr - $130/yr - Full-time employment
Duties & Responsibilities
As a Data Services Engineer you will successfully employ a combination of high technical expertise, research and investigative know-how, trouble shooting and problem solving techniques, and communication skills between clients and internal Denodo teams to achieve your mission.
Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical discussion, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning.
Constantly learn new things and maintain an overview of modern technologies.
Provide technical consulting, training and support.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention activities to help clients and other technical specialists increase their efficiency when using Denodo products.
Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.
Train and engage clients in the product architecture, configuration, and use of the Denodo Platform.
Promote knowledge and best practices while managing deliverables and timelines.
Capable of building and/or leading the development of custom deployments based on and even beyond client’s requirements.
Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.
Develop white papers, presentations, training materials or documentation on related topics and contribute to knowledge management activities.
Participate in on-call support of Denodo products.
Be willing to travel as necessary to address or service customer needs.

Qualifications

Required Skills
BS or higher degree in Computer Science.
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Good knowledge of JDBC, XML and Web Services APIs.
Excellent verbal and written communication skills to be able to interact with technical and business counterparts.
Active listener.
Strong analytical and problem solving abilities.
Lots of curiosity. You never stop learning new things.
Creativity. We love to be surprised with innovative solutions.
Willingness to travel on occasion.
Be a team worker with positive attitude.
We Value
Experience working with GIT or other version control systems.
Experience working with Big Data and/or noSQL environments like Hadoop, mongoDB, others.
Knowledge and experience with systems and services hosted in the main cloud vendors (AWS, Azure, GCP).
Experience working with caching approaches and technologies such as JCS.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Business software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM).
Integration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, …).
Industry experience in supporting mission critical software components.
Experience in attending customer meetings and writing technical documentation.
Experience in Java software development, especially in the web and database fields.
Foreign language skills are a plus.

Additional Information

Employment Practices
Denodo is an equal opportunity employer and prohibits discrimination and harassment of any kind. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by applicable law. Denodo will provide reasonable accommodation to employees who have protected disabilities in accordance with applicable law.
We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee.
Show Less
Report",$80T - $1L,201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1999,$5 to $25 million (USD)
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
CAPITAL DISTRICT PHYSICIANS HEALTH PLAN INC,ETL Data Engineer,"Albany, NY","CDPHP and its family of companies are mission-driven organizations that support the health and well-being of our customers and the communities we are proud to serve. CDPHP was founded in Albany in 1984 as a physician-guided not-for-profit, and currently offers health plans in 29 counties in New York state. The company values integrity, diversity, and innovation, and its corporate culture supports those values wholeheartedly. At CDPHP, the employees have a voice and are encouraged to make an impact at both the company and community levels through engagement and volunteer opportunities. CDPHP invests in employees who share these values and invites you to be a part of that experience.
The ETL Data Engineer works as part of the Data Management team. ETL Data Engineer will develop data pipelines and ETL solutions; from data sources or endpoints of acquisition, to integrate or consume for specific use cases. These data pipelines are to be created, maintained and optimized as workloads, to move from development to production. Creating, building, and maintaining data pipelines will be the primary responsibility of the ETL Data Engineer. The ETL Data Engineer will collaborate with enterprise architects, data architects, cloud platform engineers, data scientists, and others involved in the product lifecycle to help refine data requirements and to ensure the most appropriate data pipelines and ETL logic are developed.
The ETL Data Engineer will be responsible for using innovative and modern tools, techniques and architectures to enable automation of the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity. The ETL Data Engineer will work with the Analysis team to define and estimate technical solutions, document functional specifications, and prepare test plans/scripts for application testing. In addition, will manage the deployment/CICD process and ensure successful implementation of business functionality being developed. The ETL Data Engineer will be responsible to develop and harden reusable and repeatable patterns and frameworks that can be transferred to and supported by the Operations team. They will also participate in on-call duties through a regular rotation schedule. They must be available 24/7 by cell phone for team support of critical production issues. The ETL Data Engineer will assist with peer code reviews, maintain standards and encourage use of best practices.
QUALIFICATIONS:
Bachelor’s degree required in an Information Management, Information Technology, Engineering or a related field, plus (3) years’ experience or 5 years relevant experience may be substituted for degree.
Minimum of three (3) or more years’ hands-on experience with ETL/BI tools and relational database (normalized and dimensional) within a MS SQL Server or Oracle environment required.
Minimum of one (1) years’ experience with Non-relational data structures or Unstructured Data, Cloud or Big Data related technologies required.
Minimum of three (3) or more years’ experience in developing and/or maintaining enterprise level applications required.
Proficient in ETL tools such as Informatica Powercenter/Cloud, writing stored procedures or creating AWS Lambda functions. Familiar with upcoming ETL variants such as AWS Glue.
Familiarity with AWS cli, Powershell, Shell scripts
Familiarity with AWS technologies like EMR/Lambda/S3/KMS/DynamoDB
Familiarity in version control systems such as BitBucket/git
Can work seamlessly in multiple flavors of OS such as Windows/Linux
Familiarity with Informatica Cloud, Mulesoft Anypoint Platform or any hybrid integration tool
Strong knowledge of systems analysis and writing program specifications required.
Ensure team SLAs are being met and SDLC is followed.
Ability to work with IT and business to formulate business and data requirements.
Oracle, SQL Server, PL/SQL, Linux experience preferred.
A good understanding of Data Warehouse concepts (including star schema) required.
Demonstrated analytical experience and critical thinking skills.
Effectively handles competing priorities.
Knowledge of healthcare processes, rules, regulations, enrollment/eligibility guidelines and reporting requirements for HMO, PPO, ASO, Medicare, Medicaid, Family Health Plus and Child Health Plus preferred.

Annual salary range: $86,000 - $110,000

CDPHP salary ranges are designed to be competitive with room for professional and financial growth. Individual compensation is based on several factors unique to each candidate, such as work experience, qualifications, and skills. In addition to cash compensation, CDPHP employees may be eligible for an incentive payment, a discretionary cash reward based on employee and company performance. Some roles may also be eligible for overtime pay.
CDPHP compensation packages go far beyond just salary. The company offers a comprehensive total rewards package that includes award-winning health care coverage, health care dollars, a generous paid time off allowance, employee assistance programs, flexible work environment, and much more. Learn about all CDPHP employee benefits here.

As an Equal Opportunity / Affirmative Action Employer, CDPHP will not discriminate in its employment practices on the basis of race, color, creed, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity or expression, transgender status, age, national origin, marital status, citizenship, disability, criminal record, genetic information, predisposition or carrier status, status with respect to receiving public assistance, domestic violence victim status, protected veterans status, or any other characteristics protected under applicable law. To that end, all qualified applicants will receive consideration for employment without regard to any such protected status.

CDPHP and its family of companies include subsidiaries Acuitas Health LLC, Strategic Solutions Management Consultants (SSMC), Practice Support Services (PSS), and ConnectRX Services, LLC.
Show Less
Report",$86T - $1L,501 to 1000 Employees,Non-profit Organisation,Insurance,Insurance Carriers,1984,$500 million to $1 billion (USD)
"Bluemont Technology & Research, Inc.",Data Engineer,"Norfolk, VA","NATO Data Engineer
Requirements:
Ts/sci or secret clearance
High proficiency level in English language
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets is required.
Description:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.
Job Type: Full-time
Pay: $90,000.00 - $130,000.00 per year
Experience level:
10 years
11+ years
4 years
5 years
6 years
7 years
8 years
9 years
Ability to commute/relocate:
Norfolk, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you have a secret clearance or TS/SCI?
Work Location: One location
Show Less
Report",$90T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
"AGM Tech Solutions, LLC",Data Engineer,"Alpharetta, GA","We have an excellent 6-month contract-to-hire opportunity with our Global Leader client.
Candidate must be local to Alpharetta GA (Hybrid Flexibility) - 3 days a week on-site.
Basic
Work experience with ETL, Data Modeling, and Data Architecture.
Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL.
Experience operating very large data warehouses or data lakes.
Preferred
Experience in designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Comfortable using PySpark APIs to perform advanced data transformations.
Familiarity with implementing classes with Python.
Summary
Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue
Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python.
Design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Job Type: Contract
Pay: $70.00 per hour
Benefits:
401(k)
Flexible schedule
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Machine learning: 2 years (Required)
Work Location: One location
Show Less
Report",$70.00 Per hour,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Violet Ink,Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
FocuzMindz,"AWS Data Architect/ Engineer with Redshift, RDS","Alexandria, VA","We are seeking an AWS Data Engineer to join our growing team.
The qualified applicant will play a key role in the data warehouse migration as part of the Enterprise Data Analytic Services program at a federal agency located in Alexandria, VA. Hybrid work options are available.
AWS Data Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premises to cloud environment, evaluation, and optimization.
Full time opportunity
Alexandria VA
Hybrid role -onsite 2 days per week
Responsibilities:
Work on automating migration process for RDS and RedShift scripts in AWS from Dev to Production.
Performance tune RDS/RedShift SQL queries.
Maintain/resize the clusters for RDS and RedShift.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including computing, storage networking, database, management tools, security, identity, and compliance.
Good knowledge of RDS Postgres and AWS Redshift.
5+ years of experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Experience building infrastructure inside of AWS via code. Familiarity with tools such as Terraform or CloudFormation.
5+ years of experience architecting/deploying/operating solutions built on AWS.
Experience using ETL tools such as Alteryx, Snap Logic, Matillion, SAS DI Studio, Informatica, or equivalent tools.
Education Requirements:
Bachelor's degree in engineering, data science, computer science,
AWS and/or Data Science certification is a plus!
Clearance Requirements:
Ability to obtain and hold a Public Trust Clearance.
mary.a@stepstalent.com
Job Type: Contract
Pay: From $159,583.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Alexandria, VA 22301: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 9 years (Required)
AWS: 9 years (Required)
Security clearance:
Confidential (Required)
Work Location: Hybrid remote in Alexandria, VA 22301
Show Less
Report",$2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
APLOMB Technologies,Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
"Ikigai Labs, Inc.","Software Engineer, Data Engineering","Cambridge, MA","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Arthur Grand Technologies Inc,Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L,1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
princeton it services,Data Engineer lead,"Boston, MA","Data Engineer Lead
Job Description:
Position: Data Engineer Lead
Location: Raleigh, NC or Boston, MA
Job Length: Long term
Position Type: C2C/W2
Qualifications:
9+ years Experience in Alation, Collibra, Snowflake
9+ years Experience in Java , Spring boot , spark , Scala.
Stays current with technology trends in order to provide best options for solutions • Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently.
Responsibilities:
Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data management and Stewardship using Collibra.Alation
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required :Alation, Collibra, Snowflake
Job Type: Contract
Salary: From $65.00 per hour
Schedule:
8 hour shift
Experience:
collibra: 5 years (Preferred)
snowflake: 5 years (Preferred)
aliation: 4 years (Preferred)
Work Location: On the road
Speak with the employer
+91 6093006906
Show Less
Report",$65.00 Per hour,1 to 50 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2008,$1 to $5 million (USD)
Innova Solutions Inc.,Data Engineer/Data Analyst,"Richmond, VA","Position Summary
The person will have a mix of highly technical data quality controls development, data analysis and reporting responsibilities to include writing complex SQL queries, some python code analysis, extensive data analysis, building DQ controls metrics reports, defining tech data controls strategy, working with metadata, architecture and development teams on the resolution to DQ controls issues
Primary Skill
MySQL
Secondary Skill
Tertiary Skill
Required Skills
SQL and DQ Controls Experience
DQ Controls Development and Strategy Skills
Business Analyst
Data Architecture
Desired Skills
Python, metadata, business intelligence reporting, BA
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richmond, VA 23173: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
MySQL: 5 years (Required)
Data Quality: 5 years (Required)
Work Location: Hybrid remote in Richmond, VA 23173
Show Less
Report",$60.00 - $65.00 Per hour,10000+ Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1998,$2 to $5 billion (USD)
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MeridianSoft, Inc.",GCP Data Engineer Onsite,"Cary, NC","GCP Data Engineer/ Cary,NC Onsite (Full Time)
▪ Data Engineer will be responsible for providing expertise in implementation of data ingestion pipeline using GCP native managed services.
▪ Strong experience with relational and non-relational databases in Cloud with billions of records (structured & unstructured data) ▪ Ability to design & develop data flow pipelines from scratch ▪ Excellent problem solving & debugging skills along with designing skills ▪ Must have experience of GCP services mainly Cloud Storage, Dataflow, BigQuery, Pub/Sub, Cloud Composer etc and BigQuery optimisation techniques ▪ Sound experience on creating data flow scalable modules using Apache Bean Java pipeline ▪ End to end automation experience for pipeline automation ▪ Good to have DataProc, DataFusion experience ▪ Exposure to Bigdata platform ▪ Exposure to processing of Avro and Parquet file formats ( good to have ) using serverless architecture ▪ Good exposure and hands on knowledge on Data Warehouse / Data Lake solutions both on premise and in cloud. ▪ Involvement in cloud migration projects and experience on data platform modernization ▪ Comprehensive experience on programming language - Java ▪ Excellent communication skills / ability to articulate to customers ▪ Experience of CI/CD pipelines (preferred Jenkins, GitHub, GitHub Actions etc.) for automated build, test and deployment of code. ▪ Knowledge of Terraform ( good to have ) ▪ Experience in regulated environments (Banking, Finance, Pharma, etc.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Benefits:
401(k)
Health insurance
Paid time off
Schedule:
8 hour shift
Work Location: On the road
Show Less
Report",$60.00 - $65.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$5 to $25 million (USD)
Recruiting From Scratch,Data Engineer,Remote,"Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
From Our Client
Location: New York / Remote
Job Type: Full-time
Experience: 3+ years
About the role
Our mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability. We are growing very fast - in less than five months, we grew to millions in card volume. We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We have raised over $30M from top-tier fintech investors.
WHAT YOU'LL BE WORKING ON
You'll be involved in projects of varying scope and complexity:
Build credit risk models that segment merchants based on their revenue and spend patterns to offer dynamic credit limits that change with business performance given the high seasonality and fast pace of ecommerce
Use machine learning tools to build realtime credit underwriting models leveraging alternative and traditional data sources
WHAT YOU'LL NEED
Passion for, or curiosity to learn, financial technology
Track record of high-quality shipping products and features at scale
Ability to turn business and product ideas into engineering solutions
Desire to work in a fast-paced environment, continuously grow and master your craft




WHAT WE’D LIKE TO SEE
Experience with building out data pipelines (e.g. should know data lakes, data warehouses, ETL, etc.)
Experience working with data analytics, algorithmic decision making, and real-time data systems
Experience with different business requirements on data freshness and retention
PLUSES
Proven experience and subject matter expertise in e-commerce payments (nice to have) or financial services.
Has worked with bank account data (e.g. worked with Plaid), payments data (perhaps Stripe), and/or other fintech data sources




Why you should join us:
Our mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability. We are growing very fast - in less than five months, we have grown to millions in card volume.
We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We recently closed a financing round of $5M from a top-tier fintech investor.
Salary Range: $150,000-$250,000 base.
Show Less
Report",$2L - $2L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2019,$1 to $5 million (USD)
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Plaxonic Technologies,Sr. Data Engineer,"Dallas, TX","Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Salary: $140,000.00 - $150,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Pyspark: 8 years (Required)
Azure: 8 years (Required)
Work Location: On the road
Speak with the employer
+91 (727) 241- 5640
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
etrailer.com,Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L,501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
"Vorys, Sater, Seymour and Pease LLP.",Senior Data Engineer,"Akron, OH","Precision eControl is all about helping brands better manage their eCommerce presence on Amazon and other online marketplaces. As a Senior Data Engineer, you will be part of a team that creates innovative, cutting edge, one of a kind solutions that are revolutionizing how brands manage online marketplace sales.
Position Summary
The Senior Data Engineer for the Azure infrastructure will be responsible for the day to day operations of a large data warehouse, and will work closely with the business, product team, and the technical staff to ensure alignment to goals and objectives. Utilizing experience with Big Data, this position will drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.
Essential Job Functions
Drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.
Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Design, implement, and document data load processes from disparate data sources into Azure Synapse Pipelines.
Work with Continuous Integration/Delivery using Azure DevOps and Github.
Provide data management, monitoring, troubleshooting and support to client success
Create various triggers to automate the pipeline in Azure Synapse Analytics Pipelines.
Tune SQL queries in Azure SQL DB, Azure Synapse and solve complex data challenges and deliver insights that help our customers achieve their goals.
Self-organize as part of a small-size scrum team and apply data engineering skills.
Follows industry best practices and meets company’s security and performance and requirements
Knowledge, Skills and Abilities
Minimum three (3) years’ experience with MS SQL/T-SQL
Minimum three (3) years’ experience with Azure SQL
Minimum three (3) years’ experience with Apache Spark (PySpark)
Minimum of three (3) years’ experience with Azure Data Factory or Azure Synapse building dynamic ETL pipelines
Minimum three (3) years' experience building dynamic Spark notebooks in Azure Synapse Spark or Azure Databricks
Minimum three (3) years’ experience with Python
Minimum two (2) years’ experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
Experience working with parquet, json , delta, avro and csv files
In-depth understanding of data management (e.g. permissions, recovery, security and monitoring)
Experience with Data Warehouse Architecture
Strong analytic skills related to working with structured, semi-structured and unstructured datasets.
Excellent analytical and organization skills required
Ability to understand user requirements
Client service mindset
Excellent verbal and written communication skills
Excellent problem solving skills
Familiarity with Agile frameworks a plus
Education and Experience
Bachelor's degree in related discipline or combination of equivalent education and experience
7-10 years of experience in similar field
Precision eControl LLC does not discriminate in hiring or terms and conditions of employment because of an individual’s sex, race, age, religion, national origin, ancestry, color, sexual orientation, gender identity, genetic information, marital status, military/veteran status, disability or any other characteristic protected by local, state or federal law. Precision eControl only hires individuals authorized for employment in the United States.
Show Less
Report",$87T - $1L,501 to 1000 Employees,Company - Private,Legal,Legal,1909,Unknown / Non-Applicable
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
InfoQuest Consulting Group Inc.,Data Engineer,"Philadelphia, PA","Duration & Type: 12 months Contract with a media & communications industry client
Location: Philadelphia, PA
No. of Positions: Multiple
Responsibilities:
Develop solutions to big data problems utilizing common tools found in the ecosystem.
Develop solutions to real-time and offline event collecting from various systems.
Develop, maintain, and perform analysis within a real-time architecture supporting large amounts of data from various sources.
Analyze massive amounts of data and help drive prototype ideas for new tools and products.
Design, build and support APIs and services that are exposed to other internal teams
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Required:
5+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in one or more languages: Python, Scala/Java, Spark, Batch, Streaming, ML
Experience with Python unit testing and code coverage frameworks
Experienced in NoSQL / SQL, Microservice, RESTful API development
Strong Experience with AWS Core such as Kinesis, Lambda, API Gateway, CloudFormation, CloudWatch
Experienced with one of the Analytics tools – Presto / Athena, QuickSight, Tableau
Strong Experience with Container technologies and Real-time Streaming (such as Kafka, Kinesis)
Preferred:
Test-driven development/test automation, continuous integration, and deployment automation experience
Experience with Performance tuning at scale
Experience working on big data platforms in the cloud or on traditional Hadoop platforms
Experience working in agile/iterative development and delivery environments
Enjoy working with data – data analysis, data quality, reporting, and visualization
Great design and problem solving skills, with a strong bias for architecting at scale
Excellent communication skills
Experience in software development of large-scale distributed systems
For consideration, please send resume to career@infoquestgroup.com
Show Less
Report",$89T - $1L,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Orion Innovation,Data Engineer-Databricks,"Edison, NJ","Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.
Job Description

Designing and implementing highly performant data pipelines from multiple sources using Databricks
Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
Working with other members of the project team to support delivery of additional project components (API interfaces)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Integrate Databricks with other technologies (Ingestion tools, Visualization tools)

Knowledge, Skills, and Abilities:
Proven experience working as a data engineer
Highly proficient in using the spark framework (python and/or scala)
Extensive knowledge of Data Warehousing concepts, strategies, methodologies.
Programming experience in Scala or Python, SQL
Direct experience of building data pipelines using Apache Spark (preferably in Databricks).
Hands on experience designing and delivering solutions using Azure, including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake/ADLS Gen)
Experience with ingestion tools (nifi/azure cloud factory
Experience with big data technologies (hadoop)
Experience with Databricks
Must be team oriented with strong collaboration, prioritization, and adaptability skills required
Skills required:
Data Lake/ ADLS Gen 2
Databricks
Scala, Python, PySpark

Orion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Candidate Privacy Policy
Orion Systems Integrators, LLC and its subsidiaries and its affiliates (collectively, ""Orion,"" ""we"" or ""us"") are committed to protecting your privacy. This Candidate Privacy Policy (orioninc.com) (""Notice"") explains:
What information we collect during our application and recruitment process and why we collect it;
How we handle that information; and
How to access and update that information.
Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.
Show Less
Report",$59T - $88T,5001 to 10000 Employees,Company - Private,Information Technology,Information Technology Support Services,1993,$100 to $500 million (USD)
CODAMETRIX,Data Science Engineer III,"Boston, MA","CodaMetrix (CMX) is a multi-specialty coding AI-platform that translates clinical information into accurate sets of medical codes for patient care and revenue cycle processes, from fee-for-service to value-based care models.
We are passionate about getting doctors away from the keyboard and back to clinical care.
This is a remote position.
Job Description
At CodaMetrix, the machine learning (ML) and AI team is responsible for the invention, analysis, and deployment of new ML techniques using healthcare data to improve administrative and clinical medicine. This includes a deep understanding of the data, and any required data normalization processes and techniques.
We are looking for an experienced Data Science Engineer to join our machine learning team and help with translating proof-of-concept ideas to product grade solutions. The Data Science Engineer III will work closely with data scientists, product owners, and backend engineers to gather requirements and understand performance criteria to deliver solutions that bring our AI-driven robust and scalable products to market. The Data Scientist Engineering III primary focus areas will be building CMX infrastructure to run on AWS technologies, data ingestion, data lake utilization, data cleaning and deploying machine learning research experiments into production. This position will help architect, design, and deliver a new machine learning pipeline built on new technologies and as part of that effort work with the team to implement a new data normalization pipeline. The Data Science Engineer III reports to the Chief Data Scientist.
Responsibilities
Establish processes for data and modeling lifecycle through managing the transitions from proof-of-concept to production
Promote for best practice software development principles
Implement and test machine learning and deep learning techniques at scale
Analyze the quality and calibration of predictive models
Collaborate with machine learning, engineering, and product development teams, for deployment of new machine learning techniques and follow deployments, tracking issues, and successes
Work with applied research scientists doing experiments and support those experiments with appropriate data all reviewed/normalized
Provide engineering required to update the CMX Machine Learning pipeline supporting the research experiments move to production
Interface with medical coders, administrators, and physicians to understand the strengths and weaknesses of existing products and to help develop new machine learning-empowered products
Requirements
5+ years of experience in professional software development
3 years experience coding in Java
Heavy AWS experience at multiple companies
Fluent with software development best practices, including version control, documentation, testing and CI/CD
Extensive experience with machine learning approaches and an understanding of the analysis and testing processes of machine learning algorithms
Must have experience with AWS, particularly Spark, Cloud watch, Airflow, cloud storage (S3, Redshift) and computing (EC2, EMR)
Experience with SQL and NoSQL Databases, particularly production database systems (e.g. Postgres) and technologies
Proficiency in Python and Pandas
Experience developing in a Linux environment
Experience working in an active engineering development environment using tools to track code reviews, changes, testing
Experience with data collection, transformation, data cleaning, and working with a data lake
Bachelor's degree in software engineering, computer science or a related field with course or project work in machine learning, AI or data science
Beneficial Experience
Exposure to Natural Language Processing approaches
Experience with Spark and similar technologies from Data Bricks or others
Proficiency in Docker
Familiarity in DataBricks.
Familiarity with deep learning approaches such as CNN, RNN and Reinforcement learning
Familiarity with Elasticsearch a plus
Knowledge of US healthcare systems
Master's degree with significant course or project work in a machine learning-related field

About CodaMetrix
CodaMetrix is a well-capitalized, high-growth company focused on solving some of the more interesting yet challenging problems in clinical and healthcare administrative areas. The company was formed by the Massachusetts General Physicians Organization (MGPO) to commercialize and build upon internally developed, and highly utilized AI-driven solutions. CodaMetrix is led by proven entrepreneurs, technology, and healthcare veterans, whose vision is to create a highly desirable atmosphere for technical talent to flourish and develop innovative, significant, reliable, and broadly utilized solutions.
As a high-growth company, our team and responsibilities are constantly evolving. We therefore prioritize creative thinking, strategic problem solving and an enthusiasm for creating innovative, high-quality products for our customers. Together, we are motivated to achieve our mission of reducing the burden in healthcare administration. Candidates eager to develop and expand their skills, learn from highly technical team members and think differently will thrive at CodaMetrix.
Salary Range
$100,000-$180,000

Full-Time Employee Benefits
Learn more about how we take care of our team.
Insurance: We cover 80% of the cost of medical and dental insurance and offer vision insurance.
Retirement: CMX offers a 401(k) plan that eligible employees can contribute to one month after their first day.
Life: We offer employer-paid life insurance and short-term and long-term disability insurance.
Flexibility: We have an unlimited PTO policy so you can take the time you need to relax and rejuvenate.
Learning: All new hires complete our 7-week Fellowship program to learn about each of our departments.
Development: We provide annual performance evaluations and outline a clear path for promotions.
Engagement: We host recurring events like Meditation Mondays, CMX Connections and Socials.
Recognition: We recognize quarterly You've Been Awesome winners and celebrate our team's service milestones.
Background Check
All candidates will be required to complete a background check upon acceptance of a job offer.
Equal Employment Opportunity
Our company, as well as our products, are made better because we embrace diverse skills, perspectives, and ideas. CodaMetrix is an Equal Employment Opportunity Employer and all qualified applicants will receive consideration for employment.
WqeVk8vO8c
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Public,Information Technology,Information Technology Support Services,2018,Unknown / Non-Applicable
Skill Quotient,SAS/ Data Engineer,"Auburn Hills, MI","Job Title: SAS/ Data Engineer
Location: Auburn Hills, MI Hybrid role)
Local candidates required. Will be on site 1 - 2 days per week. Will consider candidates that are committed to relocation from day 1 (EST or CST), nearby states are best for this.
Client: Bank/Financial Services
Duration: 12/29/23, Likely to extend
Required Skills: Please do not submit candidates that do not have these requirements or are not local at this time. If you have someone able to relocate prior to day 1 that is exceedingly good I may be able to consider after some time.
5+ years of experience with SAS Configuration and Integration.
2+ years of experience with Prism (Workday) AND / OR Adenza regulatory reporting tools REQUIRED. (Both would be preferred)
Banking / Financial Services background.
Regulatory reporting and/or Data/Big Data experience (should go along with the Prism and/or Adenza experience).
Agile experience is a big plus
Data / Big Data / SAS Engineer (Regulatory Reporting Project)
Will be part of a new team working on a regulation reporting project (100B Asset Project)
At 100B in assets more governmental / regulatory reporting is required and the client is preparing for this.
Seeking a Data / Big Data / SAS Engineer with some reporting experience.
2 rounds of interviews. Can be local to Detroit area (Auburn Hills, MI) or Dallas area (Plano, TX).
Qualified candidates will have Engineering background or possibly Prism Developers.
Job Type: Contract
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Auburn Hills, MI 48321: Reliably commute or planning to relocate before starting work (Required)
Experience:
SAS: 5 years (Required)
Prism workday: 3 years (Required)
Adenza: 3 years (Required)
Work Location: One location
Show Less
Report",$73T - $1L,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2016,Unknown / Non-Applicable
Mashvisor Inc.,Data Science Engineer,Remote,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?
Show Less
Report",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,2015,Unknown / Non-Applicable
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Impact Advisors LLC,Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
APLOMB Technologies,Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Arthur Grand Technologies Inc,Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L,1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Violet Ink,"Data Engineer (STRONG SQL,PYTHON,HIVE) NO- H1B",Remote,"Summary:
We are looking for an experienced data engineer (contract) to join our team. The data engineer will support our ongoing initiatives within capacity engineering automation projects. The engineer with work closely with a Sr. engineer, a data scientist and capacity planners to deliver applications mainly in big data and infrastructure metrics domains. You will pull data from various data sources, transform and apply algorithms to produce results which will have a high business impact.
Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Develop analytical tools and programs, and build dashboards
· Collaborate with data scientists/planners and architects on several projects
· Assembling large, complex sets of data that meet business requirements
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
· Building ETL jobs for optimal extraction, transformation and loading of data from various data sources using AWS, Airflow and SQL technologies
· Building analytical tools to utilize the data pipeline, providing actionable insights into key infrastructure metrics and performance
Requirements and skills
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Fluent of python and other programming languages
· Hands-on experience with SQL database design and experienced to work with hive or other data warehouses
· Have experiences to build dashboards on Tableau/Grafana/Superset
· Experiences on building data pipelines on Airflow is a plus
· Experiences on ReactJs/Typescript/Javascript is a plus
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field
Notes from manager:
HM is Shaohui (sean) Liu . he is based out of Canada. His boss is Harish Pushparaj, Engineering Manager.
Harish heads the capacity engineering team. They manage public and private infrastructure. The focus is on capacity planning, rightsizing capacity, data sorting, improving the capacity
This person will be helping with 2 projects and the role has to start Jan 10th. One project is focused on storage model automation. Pull data from the hive, calculate storage usage, and build multiple dashboards. Don’t need a customized UI.
The second project is based on reducing costs from K8 clusters, again will involve creating multiple dashboards.
Need someone very fluent in python and sql. Databases – hive and mysql
Write data pipelines on airflow. Not a must have but a great plus, if he/she doesn't have that experience – they can teach how to write code on airflow.
Mid level – 9+ yrs of experience.
This a 1 year contract
H1 B okay
Very important – need this person to be on PST .
Urgency in closing the role by next week. shortlisted candidates will go through 3 interviews. 2 technical and one from management/behavioral perspective. Last one will be with Harish, Sean’s boss
*
Job Type: Contract
Pay: From $70.00 per hour
Schedule:
8 hour shift
Experience:
Python: 5 years (Required)
SQL: 5 years (Required)
Hive: 5 years (Required)
Work Location: Remote
Show Less
Report",$70.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Together Credit Union,Data Engineer,Remote,"Position:
Data Engineer (Remote- Nationwide)
Position Summary:
As a Data Engineer at Together Credit Union, you will be responsible for developing and enhancing the various real time flow pipelines as well as enabling sophisticated data analysis from data in our data lake, while also maintaining strict high performance and throughput requirements. You will also work closely with Enterprise Architecture, Business Analysts and Security experts to bring new ideas in data exploration and bi analytics to fruition as deliverables that will enable new ways of furthering the member experience.
Job Description:
Role and Responsibilities:
Work with architects, business analysts, and other technical resources to understand data designs and implement them.
Coordinates, plans, and implements data solutions to meet needs of the business.
Create near real time pipeline with Spark jobs using PySpark scripting.
Build required infrastructure for extract load and transform (ELT) operations from various sources of data to Redshift Serverless.
Develop complex SQL.
Implement JDBC solutions with encryption in transit.
Execute pipelines and Spark jobs in AWS EMR Serverless.
Document pipelines solutions.
Understand the role of an Active Meta Data Repository in driving pipeline solutions.
Create shared and reusable code which can be used across PySpark scripting.
Schedule pipelines which execute in a cohesive manner to ensure timely data delivery and ingestion.
Demonstrate strong technical acumen when representing the data team to the business.
Appropriately escalate and mitigate risks for projects and initiatives to leadership.
Experience, Qualifications, and Skills:
Bachelor’s Degree in Computer Science, Management Information Systems, Information Technology, or related field preferred.
3+ years’ experience in Information Technology.
Expert level Python scripting skills and/or two plus years of PySpark development required.
Minimum of 2 years’ experience with AWS Cloud experience preferred.
Familiarity with additional cloud platforms would be a plus.
3 years of SQL experience required.
Understand relational databases and SQL technologies especially massively parallel solutions like AWS Redshift Serverless or Redshift.
2 years’ experience with ELT required.
2 years’ experience with AWS Redshift preferred.
Previous experience with Snowflake, Synaps, and BigQuery would be a plus.
2 years of AWS EMR experiences preferred.
Previous experience with Job scheduling software.
Possesses and applies a high degree of subject matter expertise; continually strives to build on this knowledge to produce results that meet customer needs and enterprise goals and objectives.
Outstanding customer service skills and demonstrated ability to interact with anticipated audiences in a courteous, service-oriented manner.
Excellent organizational, multi-tasking, and time-management skills.
Collaborative team player, capable of working well with others, but also autonomously with little direction.
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Finance,Investment & Asset Management,1939,$25 to $50 million (USD)
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
etrailer.com,Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L,501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
Stytch,Data Engineer,"New York, NY","What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!
Show Less
Report",$2L - $3L,1 to 50 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2020,Unknown / Non-Applicable
MetroSys,Data Migration Engineer,Remote,"Job Description
Context
We have a great project and need Migration Engineers to assist with migration of an enterprise client to a new data center. This new IT production environment is based in Redhat Linux, IBM AIX, Sun Solaris, UNIX and HP UX. We are searching for an engineer to support the migration by migrating servers, storage and databases to the new environment. We use a strict step-by-step plan (factory plan) to efficiently execute the migration.

Competence
Bachelor of Science / Master's degree
Minimal 3-5 years of relevant work experience within an enterprise environments
Advanced knowledge of Redhat Linux and UNIX
Advanced knowledge of IBM AIX, Solaris, and HP UX
Some knowledge of IBM XIV, Pure Storage, HPE Nimble and 3PAR storage preferred
Experience with databases (Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
The candidate has a hands-on mindset, a strong customer- and problem-solving orientation, shows fast results, and has demonstrated good communication skills, especially in an international IT organization. To achieve the project goals, the candidate is able to liaise directly with all stakeholders. The candidate has a clear focus on results and quality, and is eager to develop quickly as a project leader, serving customers.

Activities
Intake / analysis of applications for migration
Creation or update of migration run books
Migration of VMware instances to the new platform
Creation of storage disks, virtual storage devices
Reconfiguration of servers, including storage & network
1RfnvbOr2v
Show Less
Report",$50.00 - $70.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Plaxonic Technologies,Sr. Data Engineer,"Dallas, TX","Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Salary: $140,000.00 - $150,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Pyspark: 8 years (Required)
Azure: 8 years (Required)
Work Location: On the road
Speak with the employer
+91 (727) 241- 5640
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Amaze Systems,Azure Data Engineer,"Washington, DC","Position: Azure Data Engineer
Location : Washington DC(Day 1 Onsite)
W2
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable to support in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role
Role requires interplay between data and business process. Candidates who have performed this Role would be preferred.
Experience in the design of reporting & data visualization solutions such as Power BI or Tableau is a plus
Knowledge and implementation in financial institutions is a plus
Knowledge in Statistical Programming languages is a plus
Knowledge in Machine Learning is a plus.
Microsoft Data Certification is a plus.
Thanks
Job Type: Contract
Experience level:
7 years
8 years
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 7 years (Required)
Data warehouse: 7 years (Required)
SQL: 6 years (Required)
Work Location: One location
Show Less
Report",$1L - $1L,201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Alianza, Inc.",Data Engineer,"Pleasant Grove, UT","Data Engineer
Alianza is looking for an experienced and results driven Data Engineer. The successful candidate will be the technical engine of the data team, building Python applications to ingest streaming and extracted data and persist to cloud storage. Will use Python and SQL with AWS cloud technologies to automate the generation and delivery of reports. Will utilize CI/CD technologies to fully automate the release of all compute and storage components to the cloud. Work with our data architect and Java developers to design creative, high-quality, data-oriented insights and dashboards. Significant focus of the position will be on streaming data pipelines, distributed datalake architectures, and AWS services. Question the status quo. Write clean, testable, resilient code. Make things go fast and have fun doing it!
Key Duties and Responsibilities:
Participate in the process of designing, data engineering, and developing data services (Streaming, ETL/ELT, Real-time analytics, Reporting) using Python, SQL and AWS services
Adhere to modern methodologies for designing, coding, and testing
Build connected, fully automated data systems and pipeline
Work effectively with remote teams in various remote time zones
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources into usable format
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Identify and implement automatable tasks and reusable frameworks
Participate in sprint planning meetings and provide reasonable estimations
Research and propose new process, techniques, or tools as solutions. Able to produce technical diagrams, explanations, and written documentation to promote proposed solutions
Collaborate with data team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements

Qualifications:
3+ years of Python development experience, preferably writing modules that implement part of a streaming or batch ETL system in a cloud hosted environment
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse / lake designs to stakeholders
Experience designing, building, and maintaining data processing systems
At least 3 years’ experience with modern DevOps automation ecosystems, preferably Git/GitHub/Bitbucket, Buildkite or Ansible (or similar)
Real-world experience handling large data volumes (terabytes of storage and billions of rows)
At least 3 years experience configuring, using, and performance tuning AWS cloud services:preferably S3, Glue, Athena, Kinesis, Firehose, Lambda, Cloudwatch, ECS, API Gateway, RDS (Postgres), SQS, SNS, SES
Experience using AWS Redshift (or similar)
Experience with CloudFormation or TerraForm
Ability to prioritize, learn quickly, and do high-quality work
Demonstrate understanding of modern APIs and endpoints, like REST and GraphQL
Working understanding of Agile dev methodologies, especially Scrum and Kanban
Good listener, communicator, collaborator, and documenter
Proficient with Linux and shell scripting
Experience with data warehouse, data mart, OLAP, dimensional modeling, Kimball method
Good understanding of relational and document database concepts and best practices
Know how to design a clean, performance-optimized relational data model, and reverse engineer existing databases into physical data model diagrams
Experience using C*, Spark, Kafka, KSQL, Confluent, Pulsar and/or Kinesis helpful
Automated testing experience using JUNIT or equivalent
Some experience in software engineering (front, middle, back or all three) and application architecture
Show Less
Report",$73T - $1L,201 to 500 Employees,Company - Private,Information Technology,Software Development,2009,$25 to $50 million (USD)
Innova Solutions Inc.,Data Engineer/Data Analyst,"Richmond, VA","Position Summary
The person will have a mix of highly technical data quality controls development, data analysis and reporting responsibilities to include writing complex SQL queries, some python code analysis, extensive data analysis, building DQ controls metrics reports, defining tech data controls strategy, working with metadata, architecture and development teams on the resolution to DQ controls issues
Primary Skill
MySQL
Secondary Skill
Tertiary Skill
Required Skills
SQL and DQ Controls Experience
DQ Controls Development and Strategy Skills
Business Analyst
Data Architecture
Desired Skills
Python, metadata, business intelligence reporting, BA
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richmond, VA 23173: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
MySQL: 5 years (Required)
Data Quality: 5 years (Required)
Work Location: Hybrid remote in Richmond, VA 23173
Show Less
Report",$60.00 - $65.00 Per hour,10000+ Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1998,$2 to $5 billion (USD)
Denodo Technologies,Data Services Engineer,"New York, NY","Company Description

Denodo is a high growth, market leading enterprise software company backed by HGGC. We are recognized as a leader by Gartner and Forrester, and uniquely positioned to address the data fragmentation problems that exist in many enterprises.
We thrive in dynamic environments, and at the risk of sounding cliché, we work hard, and we play hard. People at Denodo are builders at heart. Our global teams are constantly interacting and working together to empower people around the world, build community and connect in meaningful ways.
Denodo's success is founded on being innovative and creative, on delivering the best solutions with the highest levels of customer satisfaction and on having a unique piece of technology. A company can only be as forward-thinking as its people, which explains why we have become the leading developer of Data Virtualization, Data Services and Cloud Data Integration technologies and solutions for the enterprise.
At Denodo, we are like a family and it is of the utmost importance to us that we help support your professional growth every step of the way.

Job Description

The Opportunity
Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization.
Your mission: to help Denodo users achieve and maintain success through accelerated adoption and productive use of Denodo solutions.
In this role you will successfully employ a combination of high technical expertise and client management skills to conduct troubleshooting and issue resolution, provide technical guidance and advice through remote or on-site consulting engagements, deliver timely and complete solutions to customer issues, and being a critical point of contact for getting things done among Denodo, partner and client teams. Our client’s rely on data in their daily operations. You will be called upon to creatively assist our customers in ensuring they are successful in their data processing endeavors utilizing our products.
Location New York, NY
Salary range: $90/yr - $130/yr - Full-time employment
Duties & Responsibilities
As a Data Services Engineer you will successfully employ a combination of high technical expertise, research and investigative know-how, trouble shooting and problem solving techniques, and communication skills between clients and internal Denodo teams to achieve your mission.
Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical discussion, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning.
Constantly learn new things and maintain an overview of modern technologies.
Provide technical consulting, training and support.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention activities to help clients and other technical specialists increase their efficiency when using Denodo products.
Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.
Train and engage clients in the product architecture, configuration, and use of the Denodo Platform.
Promote knowledge and best practices while managing deliverables and timelines.
Capable of building and/or leading the development of custom deployments based on and even beyond client’s requirements.
Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.
Develop white papers, presentations, training materials or documentation on related topics and contribute to knowledge management activities.
Participate in on-call support of Denodo products.
Be willing to travel as necessary to address or service customer needs.

Qualifications

Required Skills
BS or higher degree in Computer Science.
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Good knowledge of JDBC, XML and Web Services APIs.
Excellent verbal and written communication skills to be able to interact with technical and business counterparts.
Active listener.
Strong analytical and problem solving abilities.
Lots of curiosity. You never stop learning new things.
Creativity. We love to be surprised with innovative solutions.
Willingness to travel on occasion.
Be a team worker with positive attitude.
We Value
Experience working with GIT or other version control systems.
Experience working with Big Data and/or noSQL environments like Hadoop, mongoDB, others.
Knowledge and experience with systems and services hosted in the main cloud vendors (AWS, Azure, GCP).
Experience working with caching approaches and technologies such as JCS.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Business software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM).
Integration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, …).
Industry experience in supporting mission critical software components.
Experience in attending customer meetings and writing technical documentation.
Experience in Java software development, especially in the web and database fields.
Foreign language skills are a plus.

Additional Information

Employment Practices
Denodo is an equal opportunity employer and prohibits discrimination and harassment of any kind. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by applicable law. Denodo will provide reasonable accommodation to employees who have protected disabilities in accordance with applicable law.
We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee.
Show Less
Report",$80T - $1L,201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1999,$5 to $25 million (USD)
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Transcarent,Senior Data Engineer,"San Francisco, CA","Who we are
Healthcare is more confusing, more costly, and more complex than ever. Transcarent is a health and care experience company on a mission to empower Members to stay healthy by providing them with unbiased information, trusted guidance, and easy access to high value care where and when they need it. You will be part of a world-class team, supported by top tier investors like 7wireVentures and General Catalyst, and founded by a mission-driven team committed to transforming the health and care experience for all. We closed on our Series C funding in January 2022, raising our total funding to $298 million and enabling us to respond to the demand for rapid expansion of our offering.
Transcarent is committed to growing and empowering a diverse and inclusive community within our company. We believe that a team with diverse lived experiences, working together will strengthen our organization, and our ability to deliver ""not just better but different"" experiences for our members.
What we look for in our teammates
We are looking for teammates to join us in building our company, culture, and Member experience who:
Put people first, and make decisions with the Member's best interests in mind
Are active learners, constantly looking to improve and grow
Are driven by our mission to measurably improve health and care each day
Bring the energy needed to transform health and care, and move and adapt rapidly
Are laser focused on delivering results for Members, and proactively problem solving to get there
We are looking for data engineers excited to create a single source of truth that will power the decisions for the company for other developers, products, clinical teams, data analysts, data scientists, member experience, and more. They will need to be able to ingest various sorts of application and health data, leverage our Snowflake data warehouse to load it, and transform the data into various data sets to offer it self-serve in a secure, compliant manner. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

In this role, you will...
Be a data champion and seek to empower others to leverage the data to its full potential.
Create and maintain optimal data pipeline architecture with high observability and robust operational characteristics.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal data extraction, transformation, and loading using SQL, python, and dbt from various sources.
Work with stakeholders, including the Executive, Product, Clinical, Data, and Design teams, to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
About You
You are entrepreneurial and mission-driven and can present your ideas with clarity and confidence.
You are a high-agency person. You refuse to accept undue constraints and the status quo and will not rest until you figure things out.
Advanced expertise in python and dbt for data pipelines
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing big data pipelines, architectures, and data sets. A definite plus with healthcare experience
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Good to have healthcare domain experience.
Tech Stack
The following is a list of software/tools that would be nice to have but not required:
Experience with cloud-based data warehouse: Snowflake
Experience with relational SQL and NoSQL databases
Experience with object-oriented/object function scripting languages: Golang, Python, Java, C++, Scala, etc.
Experience with big data tools: Spark, Kafka, etc.
Experience with data pipeline and workflow management tools like Airflow
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Total Rewards
As a remote position, the salary range for this role is $140,000 – $170,000.
Individual compensation packages are based on a few different factors unique to each candidate, including primary work location and an evaluation of a candidate's skills, experience, market demands, and internal equity.
Salary is just one component of Transcarent's total package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock options.
Our benefits and perks programs include, but are not limited to:
Competitive medical, dental, and vision coverage
Competitive 401(k) Plan with a generous company match
Flexible Time Off/Paid Time Off, 12 paid holidays
Protection Plans including Life Insurance, Disability Insurance, and Supplemental Insurance
Mental Health and Wellness benefits
Location
You must be authorized to work in the United States. Depending on the position we may have a preference to a specific location, but are generally open to remote work anywhere in the US.
Transcarent is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. If you are a person with a disability and require assistance during the application process, please don't hesitate to reach out!
Research shows that candidates from underrepresented backgrounds often don't apply unless they meet 100% of the job criteria. While we have worked to consolidate the minimum qualifications for each role, we aren't looking for someone who checks each box on a page; we're looking for active learners and people who care about disrupting the current health and care with their unique experiences.
Show Less
Report",$1L - $2L,201 to 500 Employees,Company - Private,Healthcare,Healthcare Services & Hospitals,2020,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Concurrency,Data Engineer,United States,"Who We Are
We are change agents. We are inspired technologists. We are unlike any other technology consulting firm. Our team fearlessly challenges the status quo, relentlessly pursues what’s next and pushes the limits of what’s possible. A Microsoft Gold Partner and multiple time Partner of the Year award recipient, Concurrency is renowned for its ability to turn unmatched technology expertise into client outcomes. Have we inspired the technologist in you? Come be a change agent at Concurrency.
Who We’re Looking For
We’re excited to add a Data Engineer to our Data & AI team. In this role, you’ll work with a team of customer-focused professionals who are committed to defining technical strategy, architecting, designing, and delivering end-to-end digital transformation. you'll demonstrate strong technical competence and business acumen through engaging in senior-level technology decision-making discussions related to agility, business value, data warehousing, and cloud-oriented data solutions. You’ll empower other consultants by sharing subject matter expertise in large enterprise implementations, as well as overseeing the delivery of large, complex, and strategic projects for enterprise customers.
Position Responsibilities
Data Engineers for various and unanticipated worksites throughout the U.S. (HQ: Brookfield, WI).
Lead requirements and design sessions with customers and internal teams.
Author functional requirements and technical design documentation.
Build, automate, and modify ADF pipelines.
Create or modify ELT/ETL procedures and scripts in T-SQL.
Create or modify Python, Scala, and SQL programs.
Develop Power BI Tabular Models, Reports and Dashboards.
Work with the solution team to help set standard architectures, processes, and best practices.
Technical Environment: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas).
POSITION REQUIREMENTS:
Bachelor’s degree in Computer Science, Management Information Systems, or a related field plus 3 years of experience in the job offered or in data analytics required.
Required skills: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas). 100% telecommuting permitted.
Concurrency takes pride in bringing a different mindset to consulting—that takes a diversity of thought, collaboration and resilience. We are an innovation-obsessed yet a fun and progressive place to work. We offer flexible work schedules, competitive compensation, and great benefits for our people and their families.
In addition, all employees are eligible for several rewards and recognition programs, excellent training programs, and bonus opportunities to encourage our people to be the best versions of themselves in and out of work.
Show Less
Report",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1989,$25 to $50 million (USD)
CentralSquare Technologies,Data Conversion Engineer I,"Lake Mary, FL","What We’re About
At CentralSquare, you’ll get the opportunity to work in a collaborative environment within a company that builds complex web-based enterprise applications for our Public Servants across North America. As over 250 Million citizens in the US are impacted by CentralSquare Software, we are on a trajectory to revolutionize the way agencies address citizens’ needs by improving quality of life and building safer, smarter communities...and we need great candidates to do it!
Looking to grow your career? That’s great! Hard work should be rewarded, and we are committed to cultivating careers while providing competitive compensation and a great benefits package, including tuition reimbursement, parental leave, paid volunteer hours, and unlimited PTO. Our flexible work environment also enables you to take advantage of an excellent work-life balance whether you are in office or working remotely.
Job Summary
This role is responsible for extracting and transforming data from our client's legacy database systems and loading that data into their new CentralSquare software system. This role works with minimal supervision with some latitude for independent judgment and quality of life improvements (automate scripting when possible). We work closely with our clients to effectively shape the data into a structure they can trust moving forward using their CentralSquare product in a way that works best for them. The work performed in this role will directly impact the productivity and effectiveness of the overall project and lead to customer satisfaction.


Job Duties Include:
Write, modify, and troubleshoot complex SQL queries, ETL (Extract, Transform, Load) processes, and Python scripts.
Work collaboratively with customer agencies undergoing project implementation to obtain data from their legacy system, understand what it means and how it is used, and determine how best to translate it into the destination system.
Communicate directly with customer agencies and Project Managers throughout implementation projects to clarify questions that arise regarding data mapping, demonstrate converted data in the target system, understand any modifications needed, and verify results of modifications.
Analyses long-running conversion processes and tune for optimum performance.
Develop and maintain an understanding of application software including functionality and database schema; use this knowledge to make informed decisions regarding data mapping and to review data within the target system.
Create and maintain documentation and wikis for data conversion processes; seek out process improvements.
Manipulate and load data such as code tables from secondary additional sources to assist in the configuration of agencies’ CentralSquare systems.
Perform tests and execute the final data conversion at the time of system go-live.
Identify deficiencies in converted data and work to resolve them.
Analyses data on legacy systems to estimate project scope and length.
Requires frequent guidance from more experienced staff
Performs all other duties as assigned
Requirements:
0-2 years of experience
Associate's or Bachelor's Degree in a technology-related field or related experience
Ability to continually pass background check requirements for working on and connecting to public safety and/or public administration information systems (if required)
Proficient in SQL
No Python/C# experience
Possesses the ability to make a technical problem understandable to non/technical customers
Show Less
Report",$54T - $72T,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1981,$100 to $500 million (USD)
Route,Sr. Data Engineer (Utah),"Lehi, UT","We are Route
Buying stuff online can get messy once you hit that ""order"" button. Managing dozens carrier tracking links, dealing with lost or damaged packages, and resolving issues with customer support can feel like a wild goose chase. That's why we created Route — to make the post-purchase experience for consumers like you, and the brands you love, as seamless as possible.
Route is on a mission to connect the world's commerce. Through our network of +5 million Route App users and 15,000 merchants, we're making it easier than ever for consumers to track, insure, and discover their favorite products in one place — which connects the world's coolest direct-to-consumer brands to happy repeat customers.
Since Route launched in 2018, we've been on a journey to build innovative products that empower our customers, all while fostering a people-first, values-driven company culture. We're looking for talented people across the ecommerce space to join us on the next steps of this adventure.
Don't just take our word for it! Discover what life at Route has to offer.
The Team
Route's Data Engineering team is at the cornerstone of the organization mission to mature the self-service/GitOps mantra for our processes with tools to scale and build value enhancing insights and opportunities for our most priceless asset, our data. This team has the amazing opportunity to create and innovate with an immediate impact with some of the most cutting-edge solutions in the market, as well as, embracing the open-source community, whichever we believe continues us on our exciting journey. As a member of our team, you will be working to build a best-in-class data platform, with some of the brightest engineers, to provide analytics to our customers and merchants, while developing internal software solutions for our teams.

The Opportunity
We are looking for a senior backend software engineer to join our team, focused on building a large distributed data workflow platform. We want a master builder that has an affinity for problem-solving and values the typical services' SLA and SLO expectations. You love developing with large scale orchestration, scheduling, distributed services, or even building out development tools for our engineers, data scientists, and business analysts as their audience.
We value team over ego and are passionate about simplicity, self-service, automation, and of course, data! We are traditionally a Golang shop developing in ECS in AWS, and currently playing with a plethora of tools in our stack, such as Airflow, DBT, Databricks, Snowflake and Tableau.
What you'll do
Design and optimize data structures in our data management system (i.e. S3, Snowflake, DBT, Databricks)
Drive efficiencies and reliability through design, automation, observability, and performance testing
Build and maintain scalable self-service solutions to our pipelines in our data ecosystem
Partner with Business analysts to understand their data challenges and crafting scalable solutions with Product alignment
Serve as a pragmatic data stewards as we iterate through evolving our data catalog while strengthening our data retention strategy and adoption
Contributing towards our documentation, runbook and architecture diagrams to keep them up to date as the frameworks evolve
What we are looking for
5+ years of relevant experience
Bachelor's degree in Engineering, Computer Science, Information Systems or related field, preferred
Experience in Data Modeling, Data architecture, Data Quality, ETL and Data Warehouse methodologies and technologies.
Experience with any combination of the following technologies: Databricks, Snowflake, Tableau, DBT, Airflow
3+ years with experience in one of the following: Python, Go, Spark, Scala
3+ years experience in working in AWS, Gitlab, Terraform
3+ years in SQL
1+ year designing, building and managing ETL Pipelines
Practical understanding of designing and building a Data Lake architecture
Pay Transparency
Salary for this role: $164,000 - $177,000 DOE
The cash compensation above includes base salary, and is not reflective of potential commission for employees in eligible roles, or annual bonus targets under Route's bonus plan for eligible roles. In addition to cash compensation, all Route employees are eligible to participate in Routes equity incentive plan to receive stock options per the terms of the agreement. Some roles may also be eligible for overtime pay. Individual compensation packages are based on a few different factors unique to each candidate, including their career level, skills, experience, specific geographic location qualifications and other job-related reasons.
Total Rewards:
We know our team works best when everyone feels happy, healthy, and supported. We offer to pay 100% of your health insurance premiums on a $0 deductible plan for you and your family, remote or hybrid work arrangements, unlimited PTO, 401k matching, formalized growth opportunities, learning & development, DEI programs & events, and so much more.
Equal opportunity for all:
Route is an Equal Opportunity Employer. We embrace diversity and equal opportunity in a serious way. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be.
Show Less
Report",$2L - $2L,201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,2018,Unknown / Non-Applicable
"Bluemont Technology & Research, Inc.",Data Engineer,"Norfolk, VA","NATO Data Engineer
Requirements:
Ts/sci or secret clearance
High proficiency level in English language
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets is required.
Description:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.
Job Type: Full-time
Pay: $90,000.00 - $130,000.00 per year
Experience level:
10 years
11+ years
4 years
5 years
6 years
7 years
8 years
9 years
Ability to commute/relocate:
Norfolk, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you have a secret clearance or TS/SCI?
Work Location: One location
Show Less
Report",$90T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
infinity quest,DATA ENGINEER,"Seattle, WA","At least 3 years of Data Engineer experience is required preferably in a cloud Environment.
You should have at least 4 years of coding experience in python/java/ Scala and open source packages with at least 2 years of experience with Databases(SQL/NOSQL etc).
Experience with large scale Distributed databases like redshift/Snowflake is a big Plus.
You should have Experience with different aspects of data systems including database design, data modeling, performance optimization, SQL etc.
Some Experience with building data pipelines and Orchestration(Airflow ,ADF,glue etc) is required.
Strong communication skills (able to explain concepts to non-technical audiences as well as peers)
Self-starter who is highly organized, communicative, quick learner, and team-oriented
Technology Requirements:
Python/Java or Scala , SQL and Airflow. Cloud experience AWS/Azure
Daily tasks:
Developing, executing, monitoring and troubleshooting Data pipelines and workflows in our cloud environment.
Work on Data Lake/DW/DQ and other framework related items
Team and cross functional collaboration as needed.
Preferred background/prior work experience:
3 years of DE expertise building data pipelines and working in a DW/Data lake Cloud based environment
Job Type: Contract
Salary: $65.00 per hour
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
Day shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road
Show Less
Report",$65.00 Per hour,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Impact Advisors LLC,Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
princeton it services,Data Engineer lead,"Boston, MA","Data Engineer Lead
Job Description:
Position: Data Engineer Lead
Location: Raleigh, NC or Boston, MA
Job Length: Long term
Position Type: C2C/W2
Qualifications:
9+ years Experience in Alation, Collibra, Snowflake
9+ years Experience in Java , Spring boot , spark , Scala.
Stays current with technology trends in order to provide best options for solutions • Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently.
Responsibilities:
Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data management and Stewardship using Collibra.Alation
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required :Alation, Collibra, Snowflake
Job Type: Contract
Salary: From $65.00 per hour
Schedule:
8 hour shift
Experience:
collibra: 5 years (Preferred)
snowflake: 5 years (Preferred)
aliation: 4 years (Preferred)
Work Location: On the road
Speak with the employer
+91 6093006906
Show Less
Report",$65.00 Per hour,1 to 50 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2008,$1 to $5 million (USD)
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
Arthur Grand Technologies Inc,Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L,1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
"Alianza, Inc.",Data Engineer,"Pleasant Grove, UT","Data Engineer
Alianza is looking for an experienced and results driven Data Engineer. The successful candidate will be the technical engine of the data team, building Python applications to ingest streaming and extracted data and persist to cloud storage. Will use Python and SQL with AWS cloud technologies to automate the generation and delivery of reports. Will utilize CI/CD technologies to fully automate the release of all compute and storage components to the cloud. Work with our data architect and Java developers to design creative, high-quality, data-oriented insights and dashboards. Significant focus of the position will be on streaming data pipelines, distributed datalake architectures, and AWS services. Question the status quo. Write clean, testable, resilient code. Make things go fast and have fun doing it!
Key Duties and Responsibilities:
Participate in the process of designing, data engineering, and developing data services (Streaming, ETL/ELT, Real-time analytics, Reporting) using Python, SQL and AWS services
Adhere to modern methodologies for designing, coding, and testing
Build connected, fully automated data systems and pipeline
Work effectively with remote teams in various remote time zones
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources into usable format
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Identify and implement automatable tasks and reusable frameworks
Participate in sprint planning meetings and provide reasonable estimations
Research and propose new process, techniques, or tools as solutions. Able to produce technical diagrams, explanations, and written documentation to promote proposed solutions
Collaborate with data team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements

Qualifications:
3+ years of Python development experience, preferably writing modules that implement part of a streaming or batch ETL system in a cloud hosted environment
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse / lake designs to stakeholders
Experience designing, building, and maintaining data processing systems
At least 3 years’ experience with modern DevOps automation ecosystems, preferably Git/GitHub/Bitbucket, Buildkite or Ansible (or similar)
Real-world experience handling large data volumes (terabytes of storage and billions of rows)
At least 3 years experience configuring, using, and performance tuning AWS cloud services:preferably S3, Glue, Athena, Kinesis, Firehose, Lambda, Cloudwatch, ECS, API Gateway, RDS (Postgres), SQS, SNS, SES
Experience using AWS Redshift (or similar)
Experience with CloudFormation or TerraForm
Ability to prioritize, learn quickly, and do high-quality work
Demonstrate understanding of modern APIs and endpoints, like REST and GraphQL
Working understanding of Agile dev methodologies, especially Scrum and Kanban
Good listener, communicator, collaborator, and documenter
Proficient with Linux and shell scripting
Experience with data warehouse, data mart, OLAP, dimensional modeling, Kimball method
Good understanding of relational and document database concepts and best practices
Know how to design a clean, performance-optimized relational data model, and reverse engineer existing databases into physical data model diagrams
Experience using C*, Spark, Kafka, KSQL, Confluent, Pulsar and/or Kinesis helpful
Automated testing experience using JUNIT or equivalent
Some experience in software engineering (front, middle, back or all three) and application architecture
Show Less
Report",$73T - $1L,201 to 500 Employees,Company - Private,Information Technology,Software Development,2009,$25 to $50 million (USD)
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Plaxonic,Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
APLOMB Technologies,Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Catalytic Data Science,Data Engineer,"Boston, MA","Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
Show Less
Report",$87T - $1L,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$1 to $5 million (USD)
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
Reebelo,Senior Data Engineer (US),"San Francisco, CA","Create The Circular Economy With Us
At Reebelo.com, we empower customers to buy their favourite tech devices in a more sustainable way. Our mission is to refresh the way we all consume tech, delivered through a platform built on sustainable values and quality-assured devices. We are looking for motivated team members like yourself with an innovative mindset. In 2 years we launched 7 countries, raised $21M from top investors and scaled to 8-digit gross sales. Change the world, and supercharge your career with Reebelo!

The Engineering team is focused on building a state-of-the-art web experience for both our customers and vendors. As a Senior Data Engineer in the data platform team, you will have the opportunity to build, optimize and grow the core data platforms in the company. Your work will have a direct and huge impact on the company's core competencies.

Your typical day may include:
Building and maintaining data pipelines to ensure correct data is transferred across platforms
Empowering the external facing data tool by engineering a robust data repository
Working closely with the stakeholders, including the engineering team to ensure accurate release of features
Owning and maintaining the complete infrastructure by ensuring near 100% uptime and proper disaster recovery practices, including but not limited to version control and configuration management

We’d love to have a chat with you if you have:
BS degree in Computer Science/Engineering
5+ years of experience in data engineering
Strong experience in pioneering, scaling, and optimizing data pipelines, data storage, large-scale data processing/analytics platform, dashboards
Expert level in Python, REST APIs
Proven experience in version control and CI/CD tools
In-depth, hands-on experience in AWS cloud infrastructure
Excellent verbal and written communications skills
How we take care of you:
Birthday leave
Competitive salary
Hybrid work environment
Ownership of your own projects
Team events & a great culture!

We understand that experience comes in many forms so if your experience is close to what we’re looking for, please don’t hesitate to apply — we’d love to hear from you!
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Retail & Wholesale,Consumer Electronics & Appliances Stores,2019,$100 to $500 million (USD)
ConnectiveRx,Sr. Data Engineer,"Hanover, NJ","ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.
Show Less
Report",$96T - $1L,1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Concurrency,Data Engineer,United States,"Who We Are
We are change agents. We are inspired technologists. We are unlike any other technology consulting firm. Our team fearlessly challenges the status quo, relentlessly pursues what’s next and pushes the limits of what’s possible. A Microsoft Gold Partner and multiple time Partner of the Year award recipient, Concurrency is renowned for its ability to turn unmatched technology expertise into client outcomes. Have we inspired the technologist in you? Come be a change agent at Concurrency.
Who We’re Looking For
We’re excited to add a Data Engineer to our Data & AI team. In this role, you’ll work with a team of customer-focused professionals who are committed to defining technical strategy, architecting, designing, and delivering end-to-end digital transformation. you'll demonstrate strong technical competence and business acumen through engaging in senior-level technology decision-making discussions related to agility, business value, data warehousing, and cloud-oriented data solutions. You’ll empower other consultants by sharing subject matter expertise in large enterprise implementations, as well as overseeing the delivery of large, complex, and strategic projects for enterprise customers.
Position Responsibilities
Data Engineers for various and unanticipated worksites throughout the U.S. (HQ: Brookfield, WI).
Lead requirements and design sessions with customers and internal teams.
Author functional requirements and technical design documentation.
Build, automate, and modify ADF pipelines.
Create or modify ELT/ETL procedures and scripts in T-SQL.
Create or modify Python, Scala, and SQL programs.
Develop Power BI Tabular Models, Reports and Dashboards.
Work with the solution team to help set standard architectures, processes, and best practices.
Technical Environment: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas).
POSITION REQUIREMENTS:
Bachelor’s degree in Computer Science, Management Information Systems, or a related field plus 3 years of experience in the job offered or in data analytics required.
Required skills: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas). 100% telecommuting permitted.
Concurrency takes pride in bringing a different mindset to consulting—that takes a diversity of thought, collaboration and resilience. We are an innovation-obsessed yet a fun and progressive place to work. We offer flexible work schedules, competitive compensation, and great benefits for our people and their families.
In addition, all employees are eligible for several rewards and recognition programs, excellent training programs, and bonus opportunities to encourage our people to be the best versions of themselves in and out of work.
Show Less
Report",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1989,$25 to $50 million (USD)
Stytch,Data Engineer,"New York, NY","What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!
Show Less
Report",$2L - $3L,1 to 50 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2020,Unknown / Non-Applicable
Vizva Technologies,Senior Azure Data Bricks Engineer,"Jersey City, NJ","Roles & Responsibilities
AREAS OF RESPONSIBILITY
· As a Data Engineer, you will work with multiple teams to deliver solutions on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies). In addition to building the next generation of application data platforms (not infrastructure) and/or improving recent implementations. Note: This is a data engineer from the application side. Must be able to analyze data and develop strategies for populating data lakes. This is not an infrastructure position. This person may be called upon to do complex coding using U-SQL, Scala or Python and T-SQL.
·
Generic Managerial Skills
Professional Skill Requirements
Work as part of a team to develop Cloud Data and Analytics solutions
Participate in development of cloud data warehouses, data as a service, business intelligence solutions
Data wrangling of heterogeneous data
Ability to provide solutions that are forward-thinking in data and analytics
Deliver a quality product
Coding complex U-SQL, Spark (Scala or Python). T-SQL
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)
Skills/Qualifications
Expertise in Any ETL tool i.e. (SSIS, Informatica, Data Stage)
Expertise to Implementing Data warehousing Solutions
experience as Data Engineer in Azure Big Data Environment
Programming experience in Scala or Python, SQL
Hands-on experience in Azure stack (Azure Data Lake, Azure Data Factory, Azure Databricks) -- Mandatory
Good understanding of other Azure services like Azure Data Lake Analytics & U-SQL, Azure SQL DW
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance & scale
Demonstrated analytical and problem-solving skills, particularly those that apply to a big data environment
Good Understanding of Modern Data Warehouse/Lambda Architecture, Data warehousing concepts
Proficient in a source code control system such as GIT
Knowledge of C#
Ability to code
Ability to use word to create required technical documentation
Soft Skills
Excellent written and verbal skills (English)
Flexible
Self-Starter
Team Player or individual contributor
Job Type: Contract
Pay: $52.06 - $69.28 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Bricks Engineer: 10 years (Preferred)
like Azure Data Lake Analytics & U-SQL, Azure SQL DW: 10 years (Preferred)
Scala or Python, SQL: 10 years (Preferred)
Work Location: In person
Show Less
Report",$52.06 - $69.28 Per hour,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
InfoQuest Consulting Group Inc.,Data Engineer,"Philadelphia, PA","Duration & Type: 12 months Contract with a media & communications industry client
Location: Philadelphia, PA
No. of Positions: Multiple
Responsibilities:
Develop solutions to big data problems utilizing common tools found in the ecosystem.
Develop solutions to real-time and offline event collecting from various systems.
Develop, maintain, and perform analysis within a real-time architecture supporting large amounts of data from various sources.
Analyze massive amounts of data and help drive prototype ideas for new tools and products.
Design, build and support APIs and services that are exposed to other internal teams
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Required:
5+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in one or more languages: Python, Scala/Java, Spark, Batch, Streaming, ML
Experience with Python unit testing and code coverage frameworks
Experienced in NoSQL / SQL, Microservice, RESTful API development
Strong Experience with AWS Core such as Kinesis, Lambda, API Gateway, CloudFormation, CloudWatch
Experienced with one of the Analytics tools – Presto / Athena, QuickSight, Tableau
Strong Experience with Container technologies and Real-time Streaming (such as Kafka, Kinesis)
Preferred:
Test-driven development/test automation, continuous integration, and deployment automation experience
Experience with Performance tuning at scale
Experience working on big data platforms in the cloud or on traditional Hadoop platforms
Experience working in agile/iterative development and delivery environments
Enjoy working with data – data analysis, data quality, reporting, and visualization
Great design and problem solving skills, with a strong bias for architecting at scale
Excellent communication skills
Experience in software development of large-scale distributed systems
For consideration, please send resume to career@infoquestgroup.com
Show Less
Report",$89T - $1L,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Trans Techno Infoway,SENIOR DATA MINING ENGINEER,"Cypress, CA","The TTI Software team is driven by the need to exceed both our own expectations and the expectations of our clients. As a result, we consider talent and dedication our greatest resources and challenge each other to grow and improve through constant collaboration. The team environment at TTI creates a positive and fulfilling atmosphere.

Working at TTI means being part of an organization that is well-respected, innovative, fun, and satisfying. The attraction of a TTI career is the opportunity for learning, career development, and to be a part of a diversified team of highly skilled professionals.

The TTI Software environment is charged with the spirit of insight, innovation, and creativity. Team members, the management and executive teams know that they impact the outcome of the products delivered to clients and as a result they are dedicated to everything they do and treat each other with the utmost respect. We believe that success comes through perseverance and every project should be exciting and fulfilling.
Benefits
TTI Software offers competitive and comprehensive benefits. We provide comfortable office facilities and amenities. Our office environment is informal and exciting, our team represents a diversity of talents and backgrounds, and our work atmosphere is collaborative, challenging, and fulfilling.

Now TTI has the following Requirements in the field of IT. TTI Software offers competitive and comprehensive benefits. We provide comfortable office facilities and amenities. Our office environment is informal and exciting, our team represents a diversity of talents and backgrounds, and our work atmosphere is collaborative, challenging, and fulfilling.

Benefits include:
A comprehensive Medical Dental and a Life Insurance Program
Paid Vacations, Holidays and Sick Leave
Stock Option Plan
Tax Deferred Savings Plan - 401(k)
Paid and Secured Parking

How to Apply
Any applicant who is intetested in this position may send resume to the

Following Address:
Trans Techno Infoway, Inc.
8555 Moody Street,
Cypress, CA 90630
Fax:714-952-3091
Email: hr@ttechno.com

The
SENIOR DATA MINING ENGINEER
Description:
This person will design and implement the decision support system backend systems. Several areas are open including data transformations, data mining algorithms, reporting, and campaign management.

Requirements:
Excellent design and programming knowledge of VC++, SQL Server 7.0/2000, SQL, COM and ASP. BS or MS in Computer Science or equivalent work experience. Prior experience with data transformations, machine learning, statistics, or campaign management is preferred. This person must have the passion to develop code that is high quality, scalable, and fast.
Show Less
Report",$86T - $1L,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
MetroSys,Data Migration Engineer,Remote,"Job Description
Context
We have a great project and need Migration Engineers to assist with migration of an enterprise client to a new data center. This new IT production environment is based in Redhat Linux, IBM AIX, Sun Solaris, UNIX and HP UX. We are searching for an engineer to support the migration by migrating servers, storage and databases to the new environment. We use a strict step-by-step plan (factory plan) to efficiently execute the migration.

Competence
Bachelor of Science / Master's degree
Minimal 3-5 years of relevant work experience within an enterprise environments
Advanced knowledge of Redhat Linux and UNIX
Advanced knowledge of IBM AIX, Solaris, and HP UX
Some knowledge of IBM XIV, Pure Storage, HPE Nimble and 3PAR storage preferred
Experience with databases (Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
The candidate has a hands-on mindset, a strong customer- and problem-solving orientation, shows fast results, and has demonstrated good communication skills, especially in an international IT organization. To achieve the project goals, the candidate is able to liaise directly with all stakeholders. The candidate has a clear focus on results and quality, and is eager to develop quickly as a project leader, serving customers.

Activities
Intake / analysis of applications for migration
Creation or update of migration run books
Migration of VMware instances to the new platform
Creation of storage disks, virtual storage devices
Reconfiguration of servers, including storage & network
1RfnvbOr2v
Show Less
Report",$50.00 - $70.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
etrailer.com,Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L,501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
Royal Credit Union,Data Engineer,"Eau Claire, WI","Team Members are at the core of who we are; living our core values of making a difference, doing the right thing, making it easy, being caring, being friendly, nice, and respectful, and taking ownership. Perhaps Royal is the change you’re looking for. Royal Credit Union is looking for caring, energetic team members who want to create a positive impact in the lives of our Members.

Is this you? If so, let us know by submitting an application!

Description:
The Data Engineer will work in a team that is building the Business Intelligence platform at Royal. This includes the use of various methods to analyze and transform raw data into useful cloud-based data systems. Involving increasingly complex level of data and statistical analysis in support of creating and shaping data systems, pipelines, and other business-related activities. Resolutions may require more in-depth quantitative analysis with non-routine solutions. Support line of business for all quantitative needs to include leading mid-size initiatives.

Work Schedule:
This is a full-time, exempt level position. Hours will generally be Monday through Friday, 8:30am – 5:00pm with occasional evening and weekend hours, as needed.

Required Experience/Education/Skills:
Bachelor’s Degree in software engineering, data engineering, data science, information systems, computer science, mathematics, machine learning, or related field of study or the equivalent in relevant work experience.
Four (4) years of experience in data engineering, data science, or software engineering experience, including knowledge of extensive data ecosystem.
Preferred Experience/Education/Skills:
Experienced in implementing Data and Advanced Analytic solutions, or related experience in the Cloud (AWS, Azure, etc.).
Experience implementing an end-to-end Cloud native platform for Datawarehouse (Snowflake, Databricks, Big Query, Redshift, etc.).
Experience with Data connectivity methodologies such as APIs (Rest/Soap), ODBC/JDBC, HTTP web hooks, Json, etc.
Intermediate experience with data management principles.
We will be communicating with you by email and/or text on the status of your application. Please monitor your email inbox and junk folder for status updates and additional instructions.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, protected veteran status or status as an individual with disability.
Show Less
Report",$92T - $1L,501 to 1000 Employees,Company - Private,Finance,Banking & Lending,1964,$2 to $5 billion (USD)
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
APLOMB Technologies,Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"VISUAL SOFT, INC",Data Engineer - Active TOP SECRET - REMOTE-ONSITE,"Washington, DC","Visual Soft, Inc is seeking qualified candidates to work on our efforts with a Prime for their end customer, a federal agency.

Position: Data Engineer - (50% REMOTE and 50% ONSITE)
Location: Washington, DC or Crystal City, Arlington, VA
Shift time: 8 am to 5 pm

JOB DESCRIPTION:
As a Data Engineer, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. You will collaborate and work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.
**Desirable skills include, Spark, Databricks, Data Lakes, Bigdata Tools and Technologies and AWS
Years of Experience:: 5+ years of experience
Education Requirement: BS degree preferred
Clearance requirement: Top SECRET is a MUST

Standard Benefits:
Our standard benefits include: Our standard benefits include 3 weeks of Paid time off (PTO that includes sick leave). Any unused PTO will be issued as a check at the end of an employee's anniversary with us. we also provide 2 floating and 8 public holidays. Floating and holidays expire at the end of every year of service of an employee. In addition, company will cover 50% of health and dental insurances only for all full time employees, however, dependents can be added at extra cost. Employee's health and dental coverage becomes effective after 30 days or first of the month after an employee completes initial 30 working days, we cover 50% for the employee's health and dental insurances. Dependents coverage for health and dental insurances is available as an out of pocket expense for employees. An employee has to finish all of your paper work for health and dental in the first 30 days of your employment with us. We provide STD, LTD and one time salary equivalent of life insurance at NO cost to all full time employees. All full time employees or w-2 employees with no benefits will be eligible to participate in company's 401k program after 90 days of employment with a company match of 4%, immediate vesting. In addition, all w-2 employees are eligible to be part of company's profit sharing, no employee contributions required. No commuting and/or parking expenses provided.
Show Less
Report",#N/A,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Arthur Grand Technologies Inc,Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L,1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
etrailer.com,Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L,501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
Plaxonic,Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Findability Sciences,Snowflake Data Engineer pipeline,"Houston, TX","Snowflake data engineers will be responsible for architecting and implementing very large scale data intelligence solutions around Snowflake Data Warehouse.
A solid experience and understanding of architecting, designing and operationalization of large scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must.
Developing ETL pipelines in and out of data warehouse using combination of SQL and Snowflakes Snow SQL
Writing SQL queries against Snowflake.
Developing scripts Unix, Python etc. to do Extract, Load and Transform data
Provide production support for Data Warehouse issues such data load problems, transformation translation problems
Translate requirements for BI and Reporting to Database design and reporting design
Understanding data transformation and translation requirements and which tools to leverage to get the job done
Understanding data pipelines and modern ways of automating data pipeline using cloud based
Testing and clearly document implementations, so others can easily understand the requirements, implementation, and test conditions.
Job Types: Full-time, Part-time, Contract, Temporary
Salary: $42.14 - $70.29 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Night shift
Ability to commute/relocate:
Houston, TX 77001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 3 years (Preferred)
Work Location: One location
Show Less
Report",$42.14 - $70.29 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
MetroSys,Data Migration Engineer,Remote,"Job Description
Context
We have a great project and need Migration Engineers to assist with migration of an enterprise client to a new data center. This new IT production environment is based in Redhat Linux, IBM AIX, Sun Solaris, UNIX and HP UX. We are searching for an engineer to support the migration by migrating servers, storage and databases to the new environment. We use a strict step-by-step plan (factory plan) to efficiently execute the migration.

Competence
Bachelor of Science / Master's degree
Minimal 3-5 years of relevant work experience within an enterprise environments
Advanced knowledge of Redhat Linux and UNIX
Advanced knowledge of IBM AIX, Solaris, and HP UX
Some knowledge of IBM XIV, Pure Storage, HPE Nimble and 3PAR storage preferred
Experience with databases (Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
The candidate has a hands-on mindset, a strong customer- and problem-solving orientation, shows fast results, and has demonstrated good communication skills, especially in an international IT organization. To achieve the project goals, the candidate is able to liaise directly with all stakeholders. The candidate has a clear focus on results and quality, and is eager to develop quickly as a project leader, serving customers.

Activities
Intake / analysis of applications for migration
Creation or update of migration run books
Migration of VMware instances to the new platform
Creation of storage disks, virtual storage devices
Reconfiguration of servers, including storage & network
1RfnvbOr2v
Show Less
Report",$50.00 - $70.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Amaze Systems,Azure Data Engineer,"Washington, DC","Position: Azure Data Engineer
Location : Washington DC(Day 1 Onsite)
W2
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable to support in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role
Role requires interplay between data and business process. Candidates who have performed this Role would be preferred.
Experience in the design of reporting & data visualization solutions such as Power BI or Tableau is a plus
Knowledge and implementation in financial institutions is a plus
Knowledge in Statistical Programming languages is a plus
Knowledge in Machine Learning is a plus.
Microsoft Data Certification is a plus.
Thanks
Job Type: Contract
Experience level:
7 years
8 years
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 7 years (Required)
Data warehouse: 7 years (Required)
SQL: 6 years (Required)
Work Location: One location
Show Less
Report",$1L - $1L,201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Impact Advisors LLC,Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
ConnectiveRx,Sr. Data Engineer,"Hanover, NJ","ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.
Show Less
Report",$96T - $1L,1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable
"DiNi Communications, Inc.",Big Data Engineer II,"Brooklyn, NY","DiNi Communications, Inc. is seeking a Big Data Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
This role will be on-site in Brooklyn.
Required skills:
5+ years of experience in writing SQL.
5+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power.
Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl.
Preferred Skills:
Hands on experience developing Microsoft PowerBI solutions.
5+ years hands-on experience in development with the suite of tools from Informatica.
PowerCenter and B2B Data Transformation.
Experience using Oracle 10g/11g, SQL Server and/or a database appliance.
Knowledge of metadata-driven enterprise reporting platforms.
Job Responsibilties including, but not limited to:
Ensure the efficient and successful implementation and support of complex data engineering solutions for Client end-users.
Demonstrate a solid understanding of industry-standard implementation methodologies using data engineering technologies, tools, and processes.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics.
Create Extraction, Transformation, Load scripts (ETL) for data from a wide variety of data sources using SQL, cloud, and ‘big data’ technologies.
Create, update, and maintain system documentation.
Develop new, or build against existing APIs, for data access or landing data as output for further downstream consumption in the appropriate target data store.
Perform special projects and initiatives as assigned.
Job Type: Full-time
Pay: $65.00 - $100.00 per hour
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
writing SQL: 5 years (Required)
copying, manipulating, and automating manual data operations: 5 years (Required)
Informatica Power Center, IICS, SSIS, or similar ETL tools: 5 years (Preferred)
Amazon Web Services or Microsoft Azure cloud computing: 5 years (Preferred)
In-depth knowledge of SQL and other database solutions: 5 years (Preferred)
data warehousing ie. Snowflake, Redshift, etc.: 5 years (Preferred)
modeling database schemas for large datasets: 5 years (Preferred)
developing cloud-ready applications: 5 years (Preferred)
working with languages like Python, Java, and Perl: 5 years (Preferred)
developing Microsoft PowerBI solution: 4 years (Preferred)
development with Informatica PowerCenter and B2B Data: 4 years (Preferred)
Oracle 10g/11g, SQL Server and/or a database appliance: 4 years (Preferred)
Big Data Engineering: 5 years (Preferred)
knowledge of metadata-driven enterprise reporting platforms: 4 years (Preferred)
Work Location: In person
Show Less
Report",$65.00 - $100.00 Per hour,Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Catalytic Data Science,Data Engineer,"Boston, MA","Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
Show Less
Report",$87T - $1L,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$1 to $5 million (USD)
Exusia,Spark Data Engineer - Consultant / Sr Consultant,"Chicago, IL","Department: Sales and Delivery Team - Empower
Industry: Information Technology & Services, Computer Software, Management Consulting
Location: Chicago
Experience Range: 5+ years
Basic Qualification: Bachelor of Engineering or Equivalent
Travel Requirements: Not required
Website: www.exusia.com

As a Senior Spark/Data Engineer, you will apply your expertise to assist Exusia’s clients with the development of applications within both cloud and on-premise environments. In this role, the candidate will be designing, coding, testing, automating, debugging, documenting, and supporting all types of applications consistent with established specifications and business requirements.

Criteria for the Role

Experience implementing high-performance design patterns for ingestion, integration, aggregation and distribution of large volume data sets within tight SLA windows
Develop high-quality programming deliverable in a self-contained, module format
Deliver data solutions across highly demanding client environments, and provide hands-on guidance to other team members
Collaborate with product development teams and senior designers to develop architectural requirements, ensuring client satisfaction with the given product
Manage build phase quality assurance code to confirm fulfillment of business/technical requirements and adherence to coding standards
Resolve difficult design and development issues
Identify areas of improvement, and review application performance
Utilize leadership skills and initiative to stay on top of emerging technologies, and rapidly ramp up on new platforms/frameworks
Exhibit excellent communication and documentation skills, especially when interacting with client stakeholders

QUALIFICATIONS:
Must Have -

Bachelor’s/Master’s degree in Computer Science or Engineering with 5+ years of experience in software development and/or data engineering
Experience in Apache Spark using Scala and/or PySpark
Experience in Apache Airflow using Python
Experience with columnar data storage formats such as Parquet
Experience with Apache Kafka messaging services
Experience in Linux-based operating systems such as RedHat
Experience with Kubernetes, Docker, and AWX in order to build, deliver, and scale containerized apps
Experience with Amazon Web Services
Understanding of CI/CD tools
Knowledge of SQL with any RDBMS
Familiarity with scheduling and orchestration

Nice-to-Have Skills

Experience in Java and other object-oriented programming languages
Experience implementing batch, micro-batch & streaming data processing architecture
Experience in PostgreSQL
Experience with ElasticSearch
AWS or Azure certification
Experience operating under an Agile Scrum delivery methodology
Ability to solution with a wide variety of open-source technologies
Ability to compare tools & technologies, and present a knowledgeable recommendation

About Exusia
Exusia (http://exusia.com/) is a global technology consulting company that empowers its clients to gain a competitive edge by accelerating business objectives and providing strategy and solutions in data management and analytics. The company has established its leadership position by solving some of the world's largest and most complex data problems in the financial, healthcare, telecommunications and high technology industries.

Exusia’s mission is to transform the world through the innovative use of information.

Exusia was recognized by Inc. 5000 and by Crain’s publications as one of the fastest growing privately held companies in the world. Since the company’s founding in 2012, Exusia has experienced an impressive seven years of revenue growth and has expanded its operations in the Americas, Asia, Africa and UK. Exusia has recently also been recognized by publications such as the CIO Review, Industry Era, Insight Success and the CIO Bulletin for the company’s innovation in IT Services, the Telecommunications and Healthcare industries and its entrepreneurship. The company is headquartered in Miami city of Florida, United States with development centers in Pune, Hyderabad, Bengaluru and Gurugram, India.

Interested applicants should apply by forwarding their CV to: elevate@exusia.com
Show Less
Report",$87T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $50 million (USD)
Samjose Infotech,"Data Engineer, Hybrid, Local Texas only","Plano, TX","Apply data and systems engineering principles to develop code spanning the data lifecycle including ingest, transform, consume end to end from source to consumption for operational and analytical workloads that minimize complexity and maximize business value.
Work as part of an agile scrum team to deliver business value.
Participate in design sessions to understand customers' functional needs.
Work with solution architect and development team to build quick prototypes leveraging existing or new architecture.
Provide end-to-end flow for a data process, map technical solutions to the process.
Develop and deploy code in continuous development pipelines leveraging off-the-shelf and open-source components of Enterprise Data Warehouse, ETL, and Data Management processes adhering to the solution architecture.
Perform software analysis, code analysis, requirements analysis, release analysis and deployment.
Job Type: Contract
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Preferred)
Education:
Bachelor's (Preferred)
Experience:
distributed, analytical, cloud-based, open-source tech: 10 years (Preferred)
building software for Data Ingestion/Data Movement ETL: 10 years (Preferred)
AWS, Snowflake, MongoDB, Postgres: 10 years (Preferred)
Python and/or Java, and/or SnowSQL: 10 years (Preferred)
Github, Gradle, Maven, Jenkins: 10 years (Preferred)
NiFi, Kafka: 10 years (Preferred)
building data pipeline framework: 10 years (Preferred)
Work Location: In person
Show Less
Report",$75T - $1L,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
AgileEngine,Senior Big Data Engineer,Remote,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Analyze, develop and implement database designs, data models and logical data specifications
Review and improve existing databases/data warehouse designs
Contribute to systems architecture analysis and designProvide comprehensive consultation to infrastructure administrators and business analysts in resolving issues
Perform data modeling studies and develop detailed data models
Work with Business Analysts and staff to establish and maintain consistent data element definitions
Participate in the development and maintenance of corporate data architecture, data management standards and conventions, data dictionaries and data element naming standards
Research and evaluate alternative solutions and recommend the most efficient and cost effective data related solutions for improved data integrity
Performance tune ETL jobs and data models
Migrate ETL code from development to production environments
Assist in the design and development of BI dashboards
Perform DW training as needed
Must haves
Experience in AWS cloud services.
Working in big data projects with 4+ years of experience
Strong working experience Apache Spark
Good experience programming language Scala or Java or both
The benefits of joining us
Professional growth: Accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: We match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: Join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: Tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote
Show Less
Report",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
Spark Foundry,"Manager, Data Engineer & ETL Processing","Chicago, IL","Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795
Show Less
Report",#N/A,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2017,Unknown / Non-Applicable
"Alianza, Inc.",Data Engineer,"Pleasant Grove, UT","Data Engineer
Alianza is looking for an experienced and results driven Data Engineer. The successful candidate will be the technical engine of the data team, building Python applications to ingest streaming and extracted data and persist to cloud storage. Will use Python and SQL with AWS cloud technologies to automate the generation and delivery of reports. Will utilize CI/CD technologies to fully automate the release of all compute and storage components to the cloud. Work with our data architect and Java developers to design creative, high-quality, data-oriented insights and dashboards. Significant focus of the position will be on streaming data pipelines, distributed datalake architectures, and AWS services. Question the status quo. Write clean, testable, resilient code. Make things go fast and have fun doing it!
Key Duties and Responsibilities:
Participate in the process of designing, data engineering, and developing data services (Streaming, ETL/ELT, Real-time analytics, Reporting) using Python, SQL and AWS services
Adhere to modern methodologies for designing, coding, and testing
Build connected, fully automated data systems and pipeline
Work effectively with remote teams in various remote time zones
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources into usable format
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Identify and implement automatable tasks and reusable frameworks
Participate in sprint planning meetings and provide reasonable estimations
Research and propose new process, techniques, or tools as solutions. Able to produce technical diagrams, explanations, and written documentation to promote proposed solutions
Collaborate with data team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements

Qualifications:
3+ years of Python development experience, preferably writing modules that implement part of a streaming or batch ETL system in a cloud hosted environment
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse / lake designs to stakeholders
Experience designing, building, and maintaining data processing systems
At least 3 years’ experience with modern DevOps automation ecosystems, preferably Git/GitHub/Bitbucket, Buildkite or Ansible (or similar)
Real-world experience handling large data volumes (terabytes of storage and billions of rows)
At least 3 years experience configuring, using, and performance tuning AWS cloud services:preferably S3, Glue, Athena, Kinesis, Firehose, Lambda, Cloudwatch, ECS, API Gateway, RDS (Postgres), SQS, SNS, SES
Experience using AWS Redshift (or similar)
Experience with CloudFormation or TerraForm
Ability to prioritize, learn quickly, and do high-quality work
Demonstrate understanding of modern APIs and endpoints, like REST and GraphQL
Working understanding of Agile dev methodologies, especially Scrum and Kanban
Good listener, communicator, collaborator, and documenter
Proficient with Linux and shell scripting
Experience with data warehouse, data mart, OLAP, dimensional modeling, Kimball method
Good understanding of relational and document database concepts and best practices
Know how to design a clean, performance-optimized relational data model, and reverse engineer existing databases into physical data model diagrams
Experience using C*, Spark, Kafka, KSQL, Confluent, Pulsar and/or Kinesis helpful
Automated testing experience using JUNIT or equivalent
Some experience in software engineering (front, middle, back or all three) and application architecture
Show Less
Report",$73T - $1L,201 to 500 Employees,Company - Private,Information Technology,Software Development,2009,$25 to $50 million (USD)
Denodo Technologies,Data Services Engineer,"New York, NY","Company Description

Denodo is a high growth, market leading enterprise software company backed by HGGC. We are recognized as a leader by Gartner and Forrester, and uniquely positioned to address the data fragmentation problems that exist in many enterprises.
We thrive in dynamic environments, and at the risk of sounding cliché, we work hard, and we play hard. People at Denodo are builders at heart. Our global teams are constantly interacting and working together to empower people around the world, build community and connect in meaningful ways.
Denodo's success is founded on being innovative and creative, on delivering the best solutions with the highest levels of customer satisfaction and on having a unique piece of technology. A company can only be as forward-thinking as its people, which explains why we have become the leading developer of Data Virtualization, Data Services and Cloud Data Integration technologies and solutions for the enterprise.
At Denodo, we are like a family and it is of the utmost importance to us that we help support your professional growth every step of the way.

Job Description

The Opportunity
Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization.
Your mission: to help Denodo users achieve and maintain success through accelerated adoption and productive use of Denodo solutions.
In this role you will successfully employ a combination of high technical expertise and client management skills to conduct troubleshooting and issue resolution, provide technical guidance and advice through remote or on-site consulting engagements, deliver timely and complete solutions to customer issues, and being a critical point of contact for getting things done among Denodo, partner and client teams. Our client’s rely on data in their daily operations. You will be called upon to creatively assist our customers in ensuring they are successful in their data processing endeavors utilizing our products.
Location New York, NY
Salary range: $90/yr - $130/yr - Full-time employment
Duties & Responsibilities
As a Data Services Engineer you will successfully employ a combination of high technical expertise, research and investigative know-how, trouble shooting and problem solving techniques, and communication skills between clients and internal Denodo teams to achieve your mission.
Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical discussion, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning.
Constantly learn new things and maintain an overview of modern technologies.
Provide technical consulting, training and support.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention activities to help clients and other technical specialists increase their efficiency when using Denodo products.
Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.
Train and engage clients in the product architecture, configuration, and use of the Denodo Platform.
Promote knowledge and best practices while managing deliverables and timelines.
Capable of building and/or leading the development of custom deployments based on and even beyond client’s requirements.
Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.
Develop white papers, presentations, training materials or documentation on related topics and contribute to knowledge management activities.
Participate in on-call support of Denodo products.
Be willing to travel as necessary to address or service customer needs.

Qualifications

Required Skills
BS or higher degree in Computer Science.
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Good knowledge of JDBC, XML and Web Services APIs.
Excellent verbal and written communication skills to be able to interact with technical and business counterparts.
Active listener.
Strong analytical and problem solving abilities.
Lots of curiosity. You never stop learning new things.
Creativity. We love to be surprised with innovative solutions.
Willingness to travel on occasion.
Be a team worker with positive attitude.
We Value
Experience working with GIT or other version control systems.
Experience working with Big Data and/or noSQL environments like Hadoop, mongoDB, others.
Knowledge and experience with systems and services hosted in the main cloud vendors (AWS, Azure, GCP).
Experience working with caching approaches and technologies such as JCS.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Business software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM).
Integration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, …).
Industry experience in supporting mission critical software components.
Experience in attending customer meetings and writing technical documentation.
Experience in Java software development, especially in the web and database fields.
Foreign language skills are a plus.

Additional Information

Employment Practices
Denodo is an equal opportunity employer and prohibits discrimination and harassment of any kind. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by applicable law. Denodo will provide reasonable accommodation to employees who have protected disabilities in accordance with applicable law.
We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee.
Show Less
Report",$80T - $1L,201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1999,$5 to $25 million (USD)
Edgesource,Remote Data Engineer,"Alexandria, VA","EOE Statement
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law.
Category
Information Technology
Description
Experience:
**Fully Remote** Must be US Citizen and able to obtain a TS/SCI
Develop standardized data architecture that includes data structure and transfer protocols to facilitate sensor integration and dynamically share information to improve situational awareness.
Develop a standardized architecture that supports a centralized data repository that advances all data analytics, and AIML capabilities enhance command and control decisions.
Design and build end-to-end data pipeline solutions (esp. streaming and batch processing, machine learning model training and updating).
Develop strategies for data acquisitions, archive recovery, and implementation of a database.
Define, design, and build dimensional databases.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Works closely with all engineering teams to develop strategy for long term data platform architecture.

Qualifications / Skills:
SQL/MySQL/PostgreSQL
Microservice deployment, cloud certification (AWS architect)
Education, Experience, and Licensing Requirements:
BS or MS degree in Computer Science or a related technical field
2-4+ years of experience as a data engineer
U.S. Citizenship with ability to obtain government security clearance

Position Requirements

Full-Time/Part-Time: Full-Time

Shift: -not applicable-

Position: Data Architect

Number of Openings: 1

Req Number: INF-20-00012

Open Date: 8/24/2020
Location: Mountain View
About the Organization: Edgesource Corporation is a leading small business providing information technology and business consulting services to the federal government. We are a stable, growing business offering excellent growth potential for business minded professionals.
Show Less
Report",$73T - $1L,51 to 200 Employees,Company - Public,Information Technology,Information Technology Support Services,#N/A,$5 to $25 million (USD)
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Freemind solutions,Big Data Engineer with Spark and Python,Remote,"Required Skillset
· 5-10 years of experience as a Big Data Developer
· In-depth knowledge of Big Data technologies - Spark, HDFS, Hive, Kudu, Impala · Solid programming experience in Python
· Production experience in core Hadoop technologies including HDFS, Hive and YARN
· Strong working knowledge of SQL and the ability to write, debug, and optimize distributed SQL queries
· Excellent communication skills; previous experience working with internal or external customers
· Strong analytical abilities; ability to translate business requirements and use cases into a Hadoop solution, including ingestion of many data sources, ETL processing, data access, and consumption, as well as custom analytics
· Effective analysis of new and existing applications and platforms
· Experience working with Data Governance tools like Apache Sentry, Kerberos, Atlas, Ranger
· Experience working with streaming data with technologies like Kafka, Spark streaming
· Strong understanding of big data performance tuning
· Experience handling different kinds of structured and unstructured data formats (Parquet/Delta Lake/Avro/XML/JSON/YAML/CSV/Zip/Xlsx/Text etc.)
· Well versed with Software Development Life Cycle Methodologies and Practices · Clear communication and documentation of technical specifications
· Spark Certification is a huge plus Responsibilities
· Integrate data from a variety of data sources (data warehouse, data marts) utilizing on-prem or cloud-based data structures (Azure/AWS); determine new and existing data sources
· Develop, implement and optimize streaming, data lake, and analytics big data solutions
· Create and execute testing strategies including unit, integration, and full end-to-end tests of data pipelines
· Recommend Kudu, HBase, HDFS, and relational databases based on their strengths
· Utilize ETL processes to build data repositories; integrate data into Hadoop data lake using Sqoop (batch ingest), Kafka (streaming), Spark, Hive or Impala (transformation)
· Adapt and learn new technologies in a quickly changing field
· Be creative; evaluate and recommend big data technologies to solve problems and create solutions Recommend and implement best tools to ensure optimized data performance; perform Data Analysis utilizing Spark, Hive, and Impala
Job Type: Contract
Job Type: Part-time
Pay: $60.00 - $65.00 per hour
Experience:
spark: 5 years (Preferred)
python: 5 years (Preferred)
databricks: 3 years (Preferred)
Work Location: Remote
Show Less
Report",$60.00 - $65.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
princeton it services,Data Engineer lead,"Boston, MA","Data Engineer Lead
Job Description:
Position: Data Engineer Lead
Location: Raleigh, NC or Boston, MA
Job Length: Long term
Position Type: C2C/W2
Qualifications:
9+ years Experience in Alation, Collibra, Snowflake
9+ years Experience in Java , Spring boot , spark , Scala.
Stays current with technology trends in order to provide best options for solutions • Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently.
Responsibilities:
Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data management and Stewardship using Collibra.Alation
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required :Alation, Collibra, Snowflake
Job Type: Contract
Salary: From $65.00 per hour
Schedule:
8 hour shift
Experience:
collibra: 5 years (Preferred)
snowflake: 5 years (Preferred)
aliation: 4 years (Preferred)
Work Location: On the road
Speak with the employer
+91 6093006906
Show Less
Report",$65.00 Per hour,1 to 50 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2008,$1 to $5 million (USD)
APLOMB Technologies,Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Innova Solutions Inc.,Data Engineer/Data Analyst,"Richmond, VA","Position Summary
The person will have a mix of highly technical data quality controls development, data analysis and reporting responsibilities to include writing complex SQL queries, some python code analysis, extensive data analysis, building DQ controls metrics reports, defining tech data controls strategy, working with metadata, architecture and development teams on the resolution to DQ controls issues
Primary Skill
MySQL
Secondary Skill
Tertiary Skill
Required Skills
SQL and DQ Controls Experience
DQ Controls Development and Strategy Skills
Business Analyst
Data Architecture
Desired Skills
Python, metadata, business intelligence reporting, BA
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richmond, VA 23173: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
MySQL: 5 years (Required)
Data Quality: 5 years (Required)
Work Location: Hybrid remote in Richmond, VA 23173
Show Less
Report",$60.00 - $65.00 Per hour,10000+ Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1998,$2 to $5 billion (USD)
Datalere,Data Engineer (Mid and Senior)---Remote Work Possible,Remote,"Data Engineer, Mid to Senior Level
Datalere team members lead by example, focus on customer needs and have a thirst to learn all they can about data analytics. Successful candidates are self-starters and never shy away from challenges.
We need team members that excel when working directly with clients to meet their goals. They understand the client's needs and requirements and build a collaborative environment to ensure a successful project delivery.
Data Engineers analyze and develop on-premises and/or cloud data and ETL solutions to solve the client's challenges. They enjoy the challenges of consulting and thrive to knock the socks off of clients
Please note that this role is vendor agnostic in regards to what ETL tools are used, so having multi vendor experience would be ideal.
Please also note that this role is open to remote candidates that live relatively close to Denver, to include neighboring states.
Responsibilities:
Hands-on development and serve as technical expert on projects
Develop data solutions leveraging traditional and cloud product offerings from leading vendors
Develop data models to meet client needs
Develop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL
Develop ETL/ELT processes and patterns to efficiently move data
Create data visualizations, dashboards and reports as needed
Data migrations and conversions to the cloud and to cloud data warehouse services
Develop and scope requirements
Travel as needed (currently less than 5%)
Maintain effective communication with team and customers
Qualifications
2+ years designing and developing data analytics solutions
2+ years with databases such as SQL Server, Oracle, MySQL
2+ years data warehouse, dimensional modeling design and architecture
Knowledge of distributed databases such as Redshift, Snowflake, Azure Synapse, and BigQuery
A passion to learn and improve your skills to deliver the best possible solutions to customers
Experience with cloud based data services offered by Azure, AWS and Google
Experience with data visualization tools such as Power BI and Tableau
Previous consulting experience preferred
Degree in computer science, information technology, engineering or business
Must be authorized to work in the US. We are unable to sponsor H-1B visas at this time.
About Us
At Datalere, we work with our clients to transform their enterprise through the use of modern compute technologies and proven deployment processes providing cost effective durable solutions for the competitive world.
If you are seeking new challenges, interested in staying up to date with the latest releases and can deliver uncompromised service to our customers, then we'd like to hear from you. If you are interested and meet the above qualifications, please submit your resume and cover letter indicating your interest to join our team.


Benefits as follows:
Participation in the Datalere bonus program;
Eligibility to participate in the company 401k program after 90 days of employment;
15 days PTO after 90 days of employment, subject to the terms and conditions as specified in Datalere's policies; Within the first 90 days, PTO is limited and only upon manager approval.
Paid medical benefits (Health, Dental, and Vision) $600 per month for the employee towards Health Insurance, starting on the first day of the following month of the employee start date (This year this translates to a benefit of a 100% health insurance coverage for employee only). 100% coverage for employee vision, dental, basic life insurance and short-term disability. Additional charges apply to spousal and family benefits.
8 Company-paid holidays per year as specified in Datalere's policies.
Salary is open for this position and depends on experience, with base salaries ranging from $85, 000 to $125, 000 per year
Show Less
Report",$85T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Violet Ink,Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
Arthur Grand Technologies Inc,Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L,1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
Gladly,Senior Data Engineer,"San Francisco, CA","Gladly is a Radically Personal Customer Service Platform that puts people at the center of a single, lifelong conversation. We enable companies to talk to their customers they way people talk to their friends: seamlessly across voice, email, SMS, chat, and social media.
Gladly's data products are a foundation for enabling contact center leaders to understand their team's performance and identify opportunities for their company. Because of Gladly's unique approach to customer service, the data we provide is a key differentiator, not an afterthought. Our data warehouse also gives Gladly's customer success team the insights to help customers optimize their use of the product. We create a range of metrics and datasets based on carefully designed events and data models. We are looking for a data engineer to join our small, fast-growing and high impact team.
What you'll do
Own and drive projects, as well as communicate with stakeholders on requirements, progress and delivery.
Teach. Provide technical guidance and mentorship in software engineering best practices while demonstrating these as an individual contributor.
Collaborate. Work closely with small, nimble, cross-functional teams of engineers, product managers, designers, and business teams.
Contribute. Build a best-in-class data pipeline with a few key attributes:
repeatable via infrastructure-as-code
testable, with verification of correctness
reliable and always-on
low latency (on the order of minutes)
observable.
Work with experienced colleagues who will be eager to share their knowledge, provide mentorship and help you grow your career.
Have opportunities to learn and work with technologies used at Gladly like Snowflake, dbt, Debezium, Looker, PostgreSQL, Kafka, Docker, Kubernetes, AWS, Redis, Node.js, Go, Python.
You'll be successful by
Being eager to learn Gladly's business domain and apply this knowledge in building the innovative product.
Self-organizing and prioritizing your work based on the impact to the customer.
Understanding how to balance pragmatic solutions with best practices of data engineering.
Having passion for making the most of our existing technologies and introducing the right tools for problem at hand.
Showing ownership and pride in your work by promoting data best practices and making them easy for engineering teams to adopt as well as providing ongoing maintenance and support.
We're excited about you because you have
5+ years of engineering experience including 2+ years of working with ETL pipelines, data transformation and modeling.
Strong teamwork skills. You love participating with high-performing teams of engineers.
Customer-centricity and product focus. You look at everything you create through the lens of how it improves things for the end-customer. You are comfortable communicating how various technical approaches might impact product behavior (and vice versa).
Learning mentality because nobody checks every box. You aren't intimidated by new domains or technology; you're willing to dive into documentation/videos, talk to your teammates, and experiment to become well-versed.
Willingness to work across the development stack. You're comfortable with working on data pipelines, transformations and implementing insights. You're willing to jump into Gladly backend applications on occasion.
Operational expertise. You value robust observable solutions with actionable monitoring and the importance of tooling for troubleshooting and resolving issues.
Research has shown that individuals from marginalized groups are less likely to apply to jobs where they don't meet 100% of the criteria. Gladly values diversity of experience, so if you believe you have the right skill set, we welcome you to apply - even if you don't check every box in the job description. We're committed to an inclusive workplace and would love to see if you could be the next great addition to our team.
Compensation
$156,000-$215,000 annually.
For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above.
Working at Gladly
People are not just at the heart of our product, they're at the heart of our company.
We value diverse perspectives and hire new people to enrich our mix, not keep it the same.
We believe in open communication and share in an inclusive, open culture.
We have embraced remote work and make it easy for our team to work from anywhere, but we also invest in opportunities to get the teams together in person regularly.
We learn from each other, and we help each other learn.
We provide opportunities to move between teams to learn and contribute to other cool technologies used at Gladly.
We have a strong work ethic, but value life outside of work, too.
Our focus is on people and that starts with our employees. As an employee you can count on:
Competitive salaries, stock options
Medical, Dental, Vision and Life insurance
Generous paid time off
Generous paid Parental Leave
401K
Flexible Spending Accounts
Wellness and home office stipends
Founded in 2014 by a team of repeat entrepreneurs with multiple successful exits, Gladly is reinventing customer service. By focusing on customers instead of tickets, we are disrupting a $70B market and are proud to count Crate and Barrel, Warby Parker and many other innovative brands as customers. Gladly has raised over $110M from Greylock Partners, NEA, GGV Capital, Glynn Capital and JetBlue Tech Ventures.
Gladly has made the decision to become a fully distributed company, allowing employees to live anywhere in the United States, and candidates to come from nearly any geographical region. That said, we also highly value our collaborative and creative culture and commit to meeting in real life as a company at least once per quarter when it is safe to do so.
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2014,Unknown / Non-Applicable
"AGM Tech Solutions, LLC",Data Engineer,"Alpharetta, GA","We have an excellent 6-month contract-to-hire opportunity with our Global Leader client.
Candidate must be local to Alpharetta GA (Hybrid Flexibility) - 3 days a week on-site.
Basic
Work experience with ETL, Data Modeling, and Data Architecture.
Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL.
Experience operating very large data warehouses or data lakes.
Preferred
Experience in designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Comfortable using PySpark APIs to perform advanced data transformations.
Familiarity with implementing classes with Python.
Summary
Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue
Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python.
Design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Job Type: Contract
Pay: $70.00 per hour
Benefits:
401(k)
Flexible schedule
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Machine learning: 2 years (Required)
Work Location: One location
Show Less
Report",$70.00 Per hour,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Impact Advisors LLC,Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
Talent Groups,100% REMOTE // GCP Data Engineer,Remote,"100% REMOTE
GCP Data Engineer
Full time
GCP BQ query knowledge with handson skill of Hana (DWH)
Requirement is to migrate HANA Native objects to GCP BQ
DWH handon with details surrogate keys, SCD and various types, referential integrity etc
SQL (Joins, tables, ranks)
Python (file handling (open, read, remove etc), database connections, lists, data type conversions
GCP experience (BigQuery, Pub Sub, GCS), it is a plus with migration knowledge
Job Types: Full-time, Contract
Pay: $140,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Speak with the employer
+91 847-474-3706
Show Less
Report",$1L - $2L,Unknown,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",#N/A,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
YT Global Network,Data Engineer- Remote,Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$90.00 - $120.00 Per hour,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
APLOMB Technologies,Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Impact Advisors LLC,Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
"AGM Tech Solutions, LLC",Data Engineer,"Alpharetta, GA","We have an excellent 6-month contract-to-hire opportunity with our Global Leader client.
Candidate must be local to Alpharetta GA (Hybrid Flexibility) - 3 days a week on-site.
Basic
Work experience with ETL, Data Modeling, and Data Architecture.
Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL.
Experience operating very large data warehouses or data lakes.
Preferred
Experience in designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Comfortable using PySpark APIs to perform advanced data transformations.
Familiarity with implementing classes with Python.
Summary
Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue
Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python.
Design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Job Type: Contract
Pay: $70.00 per hour
Benefits:
401(k)
Flexible schedule
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Machine learning: 2 years (Required)
Work Location: One location
Show Less
Report",$70.00 Per hour,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Violet Ink,Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
"Alianza, Inc.",Data Engineer,"Pleasant Grove, UT","Data Engineer
Alianza is looking for an experienced and results driven Data Engineer. The successful candidate will be the technical engine of the data team, building Python applications to ingest streaming and extracted data and persist to cloud storage. Will use Python and SQL with AWS cloud technologies to automate the generation and delivery of reports. Will utilize CI/CD technologies to fully automate the release of all compute and storage components to the cloud. Work with our data architect and Java developers to design creative, high-quality, data-oriented insights and dashboards. Significant focus of the position will be on streaming data pipelines, distributed datalake architectures, and AWS services. Question the status quo. Write clean, testable, resilient code. Make things go fast and have fun doing it!
Key Duties and Responsibilities:
Participate in the process of designing, data engineering, and developing data services (Streaming, ETL/ELT, Real-time analytics, Reporting) using Python, SQL and AWS services
Adhere to modern methodologies for designing, coding, and testing
Build connected, fully automated data systems and pipeline
Work effectively with remote teams in various remote time zones
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources into usable format
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Identify and implement automatable tasks and reusable frameworks
Participate in sprint planning meetings and provide reasonable estimations
Research and propose new process, techniques, or tools as solutions. Able to produce technical diagrams, explanations, and written documentation to promote proposed solutions
Collaborate with data team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements

Qualifications:
3+ years of Python development experience, preferably writing modules that implement part of a streaming or batch ETL system in a cloud hosted environment
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse / lake designs to stakeholders
Experience designing, building, and maintaining data processing systems
At least 3 years’ experience with modern DevOps automation ecosystems, preferably Git/GitHub/Bitbucket, Buildkite or Ansible (or similar)
Real-world experience handling large data volumes (terabytes of storage and billions of rows)
At least 3 years experience configuring, using, and performance tuning AWS cloud services:preferably S3, Glue, Athena, Kinesis, Firehose, Lambda, Cloudwatch, ECS, API Gateway, RDS (Postgres), SQS, SNS, SES
Experience using AWS Redshift (or similar)
Experience with CloudFormation or TerraForm
Ability to prioritize, learn quickly, and do high-quality work
Demonstrate understanding of modern APIs and endpoints, like REST and GraphQL
Working understanding of Agile dev methodologies, especially Scrum and Kanban
Good listener, communicator, collaborator, and documenter
Proficient with Linux and shell scripting
Experience with data warehouse, data mart, OLAP, dimensional modeling, Kimball method
Good understanding of relational and document database concepts and best practices
Know how to design a clean, performance-optimized relational data model, and reverse engineer existing databases into physical data model diagrams
Experience using C*, Spark, Kafka, KSQL, Confluent, Pulsar and/or Kinesis helpful
Automated testing experience using JUNIT or equivalent
Some experience in software engineering (front, middle, back or all three) and application architecture
Show Less
Report",$73T - $1L,201 to 500 Employees,Company - Private,Information Technology,Software Development,2009,$25 to $50 million (USD)
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
"Bluemont Technology & Research, Inc.",Data Engineer,"Norfolk, VA","NATO Data Engineer
Requirements:
Ts/sci or secret clearance
High proficiency level in English language
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets is required.
Description:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.
Job Type: Full-time
Pay: $90,000.00 - $130,000.00 per year
Experience level:
10 years
11+ years
4 years
5 years
6 years
7 years
8 years
9 years
Ability to commute/relocate:
Norfolk, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you have a secret clearance or TS/SCI?
Work Location: One location
Show Less
Report",$90T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
InfoQuest Consulting Group Inc.,Data Engineer,"Philadelphia, PA","Duration & Type: 12 months Contract with a media & communications industry client
Location: Philadelphia, PA
No. of Positions: Multiple
Responsibilities:
Develop solutions to big data problems utilizing common tools found in the ecosystem.
Develop solutions to real-time and offline event collecting from various systems.
Develop, maintain, and perform analysis within a real-time architecture supporting large amounts of data from various sources.
Analyze massive amounts of data and help drive prototype ideas for new tools and products.
Design, build and support APIs and services that are exposed to other internal teams
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Required:
5+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in one or more languages: Python, Scala/Java, Spark, Batch, Streaming, ML
Experience with Python unit testing and code coverage frameworks
Experienced in NoSQL / SQL, Microservice, RESTful API development
Strong Experience with AWS Core such as Kinesis, Lambda, API Gateway, CloudFormation, CloudWatch
Experienced with one of the Analytics tools – Presto / Athena, QuickSight, Tableau
Strong Experience with Container technologies and Real-time Streaming (such as Kafka, Kinesis)
Preferred:
Test-driven development/test automation, continuous integration, and deployment automation experience
Experience with Performance tuning at scale
Experience working on big data platforms in the cloud or on traditional Hadoop platforms
Experience working in agile/iterative development and delivery environments
Enjoy working with data – data analysis, data quality, reporting, and visualization
Great design and problem solving skills, with a strong bias for architecting at scale
Excellent communication skills
Experience in software development of large-scale distributed systems
For consideration, please send resume to career@infoquestgroup.com
Show Less
Report",$89T - $1L,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Together Credit Union,Data Engineer,Remote,"Position:
Data Engineer (Remote- Nationwide)
Position Summary:
As a Data Engineer at Together Credit Union, you will be responsible for developing and enhancing the various real time flow pipelines as well as enabling sophisticated data analysis from data in our data lake, while also maintaining strict high performance and throughput requirements. You will also work closely with Enterprise Architecture, Business Analysts and Security experts to bring new ideas in data exploration and bi analytics to fruition as deliverables that will enable new ways of furthering the member experience.
Job Description:
Role and Responsibilities:
Work with architects, business analysts, and other technical resources to understand data designs and implement them.
Coordinates, plans, and implements data solutions to meet needs of the business.
Create near real time pipeline with Spark jobs using PySpark scripting.
Build required infrastructure for extract load and transform (ELT) operations from various sources of data to Redshift Serverless.
Develop complex SQL.
Implement JDBC solutions with encryption in transit.
Execute pipelines and Spark jobs in AWS EMR Serverless.
Document pipelines solutions.
Understand the role of an Active Meta Data Repository in driving pipeline solutions.
Create shared and reusable code which can be used across PySpark scripting.
Schedule pipelines which execute in a cohesive manner to ensure timely data delivery and ingestion.
Demonstrate strong technical acumen when representing the data team to the business.
Appropriately escalate and mitigate risks for projects and initiatives to leadership.
Experience, Qualifications, and Skills:
Bachelor’s Degree in Computer Science, Management Information Systems, Information Technology, or related field preferred.
3+ years’ experience in Information Technology.
Expert level Python scripting skills and/or two plus years of PySpark development required.
Minimum of 2 years’ experience with AWS Cloud experience preferred.
Familiarity with additional cloud platforms would be a plus.
3 years of SQL experience required.
Understand relational databases and SQL technologies especially massively parallel solutions like AWS Redshift Serverless or Redshift.
2 years’ experience with ELT required.
2 years’ experience with AWS Redshift preferred.
Previous experience with Snowflake, Synaps, and BigQuery would be a plus.
2 years of AWS EMR experiences preferred.
Previous experience with Job scheduling software.
Possesses and applies a high degree of subject matter expertise; continually strives to build on this knowledge to produce results that meet customer needs and enterprise goals and objectives.
Outstanding customer service skills and demonstrated ability to interact with anticipated audiences in a courteous, service-oriented manner.
Excellent organizational, multi-tasking, and time-management skills.
Collaborative team player, capable of working well with others, but also autonomously with little direction.
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Finance,Investment & Asset Management,1939,$25 to $50 million (USD)
Titan Healthcare Management Solutions,Data Engineer,Remote,"Titan Health is currently hiring for a Data Engineer.
Under the general direction of the Technology Solutions Manager, the Data Engineer is responsible for implementation, configuration, maintenance, and performance of business-critical data infrastructure to deliver data enablement at scale, and power our revenue cycle applications. This role will work with enterprise and client leaders to translate business and functional requirements into technical specifications and solutions within the data architecture strategy.
Essential Job Duties/Responsibilities
Implement efficient and scalable pipelines integrating data from multiple sources to common data models.
Convert raw data into usable information for client and enterprise organizations.
Within an Agile team design, develop, test, implement, and support technical solutions that support full-stack development tools and technologies.
Support data science, data enrichment, research, and data analysis as well as making data operationally able to be consumed by products and services.
Collaborate with product managers, software engineers, security & compliance, and data scientists to enable them with robust data delivery solutions that drive powerful experiences.
Identify and debug issues with code and suggest changes and/or improvements.
Perform unit tests and conduct reviews with the team to ensure code is rigorously designed, elegantly coded, and effectively tuned for performance.
Utilize available technologies to collect and map data to find cost savings and optimization opportunities.
Support and drive a proactive culture of security and compliance.
Bring an agile and engineering mindset to address complex problems, identify opportunities and craft creative solutions.
Leverage best practice coding and engineering standards to support growth and flexibility.
Coordinate with teams across the organization to address incident, change and release management needs/requirements.
Provide input to risk management; report risks as they are identified and participate in prioritization/follow up.
Stay current with emerging technologies and advancements within existing technologies.
Positively and deliberately engage with colleagues – external and internal – to foster collaborative and productive relationships.
Cultivate great teams and lead in alignment with Titan values.
Comply with and hold with utmost regard all compliance requirements to protect patient privacy and confidentiality.
Stay curious, kind and contribute positively to the Titan culture.
Minimum Qualifications
Bachelor’s Degree in Information Technology, Computer Science, Mathematics or related field is preferred but not required.
Experience with Agile methodologies.
Experience working in HIPAA, HITRUST or other advanced compliance environments.
Experience leading the lifecycle management and integrations of enterprise data.
Ability to clearly articulate technology concepts to business leaders and engineers.
Strong understanding of IT Service Management practices.
Strong analytical, problem-solving, and critical thinking skills with excellent attention to detail.
Direct experience with Azure SQL.
Experience with Azure Data Factory preferred.
Prior experience in the facilitating conversations to translate business requirements into the technical requirements needed to develop solutions.
Excellent oral and written communication skills.
Comprehensive knowledge of Microsoft Office applications.
Knowledge of Hospital CMS or billing data structures preferred.
Titan Health offers a robust Health and Welfare benefits program, along with Paid Time Off, 401k plan with company match, and remote working environment.
H9CslyVeWp
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
FlexIT Inc,Full Stack Data Engineer,"Beaverton, OR","APLA is building capabilities around the company's data foundation to build data sources that are needed for reporting and analytics
The type of engineer were looking for is a Full Stack Data Engineer
Knowledge of data visualization engineering as well as consumption and view build engineering
Show Less
Report",$87T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Concurrency,Data Engineer,United States,"Who We Are
We are change agents. We are inspired technologists. We are unlike any other technology consulting firm. Our team fearlessly challenges the status quo, relentlessly pursues what’s next and pushes the limits of what’s possible. A Microsoft Gold Partner and multiple time Partner of the Year award recipient, Concurrency is renowned for its ability to turn unmatched technology expertise into client outcomes. Have we inspired the technologist in you? Come be a change agent at Concurrency.
Who We’re Looking For
We’re excited to add a Data Engineer to our Data & AI team. In this role, you’ll work with a team of customer-focused professionals who are committed to defining technical strategy, architecting, designing, and delivering end-to-end digital transformation. you'll demonstrate strong technical competence and business acumen through engaging in senior-level technology decision-making discussions related to agility, business value, data warehousing, and cloud-oriented data solutions. You’ll empower other consultants by sharing subject matter expertise in large enterprise implementations, as well as overseeing the delivery of large, complex, and strategic projects for enterprise customers.
Position Responsibilities
Data Engineers for various and unanticipated worksites throughout the U.S. (HQ: Brookfield, WI).
Lead requirements and design sessions with customers and internal teams.
Author functional requirements and technical design documentation.
Build, automate, and modify ADF pipelines.
Create or modify ELT/ETL procedures and scripts in T-SQL.
Create or modify Python, Scala, and SQL programs.
Develop Power BI Tabular Models, Reports and Dashboards.
Work with the solution team to help set standard architectures, processes, and best practices.
Technical Environment: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas).
POSITION REQUIREMENTS:
Bachelor’s degree in Computer Science, Management Information Systems, or a related field plus 3 years of experience in the job offered or in data analytics required.
Required skills: Data Analysis, Data Migration, Data Mining, Machine Learning, Data Modeling, ETL, Power BI, MS Azure ML, Azure SQL Database, SQL Server, R Studio, Python (NumPy, Pandas). 100% telecommuting permitted.
Concurrency takes pride in bringing a different mindset to consulting—that takes a diversity of thought, collaboration and resilience. We are an innovation-obsessed yet a fun and progressive place to work. We offer flexible work schedules, competitive compensation, and great benefits for our people and their families.
In addition, all employees are eligible for several rewards and recognition programs, excellent training programs, and bonus opportunities to encourage our people to be the best versions of themselves in and out of work.
Show Less
Report",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1989,$25 to $50 million (USD)
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
nLeague Services Inc.,Power BI Data Visualization Engineer,"Nashville, TN","Programmer / Developer
Location: 710 James Robertson Parkway Nashville, Tennessee 37243 (Hybrid)
Duration: 06+ Months
Client: TN DOE
Job ID: 60392
**** MUST BE LOCAL****
Job Description:
The Tennessee Department of Education is hiring a Power BI Data Visualization Engineer to develop standardized reports and dashboards for the Finance team using the Microsoft Power BI application suite.
This position will be responsible for designing the data models, designing the reports, developing the data connections, transforming the data, and delivering the application solution.
The candidate should have the following skills, knowledge, and competencies:
Responsibilities:
Elicit requirements through customer focused discovery meetings.
Provide Power BI training for super users.
Building Analysis Services reporting models.
Ensure proper definition and understanding of the scope of each data reporting request.
Facilitate proper requirements/user story definition is maintained across workstreams.
Adhere to Agile/Scrum project methodologies and participate in supporting meetings/ceremonies.
Provide accurate, status updates to all team members and stakeholders.
Ensure all project artifacts are properly created and stored adhering to TDOE standards.
Assist in the design and delivery of analytic services and reporting and data models.
Develop sophisticated visual reports, dashboards and KPI scorecards using Microsoft PowerBI.
Design and develop complex queries and data transformations for Business Intelligence.
Work with the IT team to ensure data reporting development standards and best practices are adhered to.
Build, test and verify advanced level calculations on data sets.
Develop tabular and multidimensional models that are compatible with data warehouse standards.
Develop and manage custom DAX queries in Power BI desktop.
Required Skills:
2- or 4-year Degree in relations to Computer Science, Data Analysis, Visualization, Media Arts and Sciences, or equivalent with 3-6 years of relevant experience
5+ years working in data development and data visualization environment.
Knowledge of graphics and visualization tools such as Xcel, PowerBI, Visio, Tableau etc.
3+ years of experience using Microsoft’s PowerBI
3+ years writing complex DAX based scripts for the purpose of advanced data reporting.
Strong SQL query experience.
Familiar with current web standards such as HTML, CSS, JQuery or other scripting code libraries,
Experience in developing various types of visualization products is a plus.
Ability to work on multiple projects at one time.
Strong interpersonal and collaboration skills – strong oral, written communication and presentations shills.
Exceptional analytical and problem-solving skills.
Experience in using advance level calculations on the data set.
Skills
CSS
HTML DOM, HTML
SQL, XML
Microsoft Office
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Experience level:
6 years
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Nashville, TN 37243: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$55.00 - $60.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2001,Less than $1 million (USD)
Catalytic Data Science,Data Engineer,"Boston, MA","Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
Show Less
Report",$87T - $1L,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$1 to $5 million (USD)
Govini,Data Engineer,"Pittsburgh, PA","Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.
Show Less
Report",$75T - $1L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable
Stytch,Data Engineer,"New York, NY","What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!
Show Less
Report",$2L - $3L,1 to 50 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2020,Unknown / Non-Applicable
Neurable. Inc.,Neural Data Engineer,"Boston, MA","Neurable is seeking an experienced Neural Data Engineer to join our team and help build wearable devices powered by neurotechnology. Your goal will be to bring cutting-edge data management and AI/ML solutions to bear in the field of neurotechnology.
This position will be responsible for developing new tools to improve our neuroinformatics database and data archival pipeline. You will also work closely with our data science and data collection teams in order to deploy AI/ML models, clean and refine neural data streams, and develop processes for monitoring and visualizing data.
If you enjoy creative problem solving in a fast-paced startup environment. If you can find yourself getting lost tinkering. If you are curious by nature, consider yourself a technologist or someone who wants to affect change, you will fit in well with our team.
This is an opportunity to join a high-impact company, a world class team, and pioneer new technology that will change the way people interact with computers. We want you to have full creative latitude and know that this is your company, not just a job.
What You Will Do:
Maintain and enhance pipelines for filtering, denoising, featurizing, and modeling EEG data
Develop and maintain state-of-the-art methods for data archival and management
Work closely with our experimental team to optimize the quality of data and data labels that are being collected
Develop dashboards for continuous monitoring of data quality
Develop new methods for denoising and preprocessing EEG data in conjunction with other modalities including accelerometer data
Support server configuration (web and application) and deployment
Lead data engineering efforts, including database and API design, data extraction/transformation/load, and data aggregation/integration
Containerize and deploy software and workflows on local high performance computing platforms and cloud computing infrastructure (AWS)
Communicate with internal teams and external stakeholders
Define experiments, provide scientific guidance, and help the engineering team throughout R&D and Product life cycles
Manage and contribute to grant applications, studies, and projects
The Ideal Candidate Will Possess:
PhD or Master’s in Computer Science, Engineering, Cognitive or Computational Neuroscience, Physical Sciences, or Applied Mathematics/Statistics
Excellent programming skills in Python, Bash, or MATLAB
Experienced with version control, CI/CD, unit testing, and issue/release management
Knowledgeable with API deployment and containerization
The ability to get new applications up and running, and to overcome hurdles as they arise, is particularly helpful
Experience in brain-computer interfaces or neuroengineering is preferred but not necessary
Enthusiastic for building BCI technology and hungry to learn
Basic experience with signal processing, machine learning, and deep learning frameworks (e.g., PyTorch, Tensorflow) is a plus
Prior experience and continued interest in mentorship and technical development of junior data engineers
Ability to solve problems that have not been solved, in other words, a willingness to explore the unknown and ability to make progress
Ability to embrace uncertainty when working on challenging research questions
Knowledge of perceptual/behavioral evaluations and physiological measurement is desired
Hardware and software experience with multimodal data acquisition, instrumentation, and human-machine interfaces is a plus
Compensation and Benefits:
Competitive salary and equity
High quality health insurance (100% company paid)
401(k) with employer matching contributions
Generous PTO
Pet friendly office, fun team activities, and homemade waffles every Wednesday!
We are not able to provide a visa or sponsorship for this position. All candidates must be authorized to work in the USA.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
ShorePoint,Data Engineer,"Herndon, VA","Who we are:

ShorePoint is a fast-growing, industry recognized, and award-winning cybersecurity services firm with a focus on high-profile, high-threat, private and public-sector customers who demand experience and proven security models to protect their data. ShorePoint subscribes to a “work hard, play hard” mentality and celebrates individual and company successes. We are passionate about our mission and going above and beyond to deliver for our customers. We are equally passionate about an environment that supports creativity, accountability, diversity, inclusion, and a focus on giving back to our community.

The perks:

As recognized members of the Cyber Elite, we work together in partnership to defend our nation’s critical infrastructure while building meaningful and exciting career development opportunities in a culture tailored to the individual technical and professional growth. We are committed to the belief that our team members do their best work when they are happy and well cared for. In support of this philosophy, we offer a comprehensive benefits package, including major carriers for health care providers. Highlighted benefits offered: 18 days of PTO, 11 holidays, 80% of insurance premium covered, 401k, continued education, certification maintenance and reimbursement, etc.

Who we’re looking for:

We are seeking a Data Engineer who has experience providing support in a dynamic, fast-paced environment within the public sector. The Data is responsible for expanding and optimizing data and data architecture, as well as optimizing data flow and collection for cross-functional teams. Additional position activities include supporting software developers, database architects, data analysts and other program teammates to achieve data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects within the federal market. This is a unique opportunity to join an exciting company where you will have a voice and be an active participant in driving growth and shaping our companies’ culture.

What you’ll be doing:

Extensive experience as a data engineer or comparable discipline in a government environment
Building infrastructure required for extraction, transformation, and loading of data from a wide variety of data sources
Advanced applied knowledge and experience working with relational databases, query authoring, as well as working familiarity with a variety of databases
Experience building and optimizing data pipelines, architectures and data sets
Administers Security Data Integration Platform
Conducts parsing/normalization of all data feeds
Documents and automates parsing of tools/versions within inventory catalog
Investigates/correlates new data feeds for inventory and data source updates
Provides site-specific data platform technical reach-back and guidance for site administrators
Experience with Agile management and associated tools
Self-starting and able to drive projects to completion in a fast-moving environment
Solid communications skills, both written and verbal
Able to create, discuss and explain technical documentation
Ability to function effectively as part of a high-performance team

What you need to know:

Experience troubleshooting issues related to data connections and/or data sources
Familiar with NISTIR 8112
Multi-cloud (AWS, Azure, Google, and/or other SaaS providers) and on-premises data integration experience
Experience with one or more queuing technologies

Must have’s:

Must be a U.S. Citizen and have an ability to obtain and maintain a clearance
Bachelor’s Degree in a related technical discipline, or the equivalent combination of education, technical certifications or training, or work experience
Minimum of 5+ years of experience sourcing, connecting, and assembling large, complex data sets

Where it’s done:

Herndon, VA and Remote
Show Less
Report",$95T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Norwin,Data Engineer,"South San Francisco, CA","Title: Data Engineer or Data Scientist
Location: South San Francisco, CA 94080(Candidate need to be local and be onsite 3 days a week)
Duration: 6+months(W2 Contract)
Client: Genentech
Description
The systems specialist role will be part of the Workplace Digital Insights team which supports making key data-driven decisions for the Real Estate and Workplace Effectiveness(RE&WE) organization. We help this organization’s forward planning strategies for South San Francisco ranging from developing real estate strategies, to campus planning, building investments, occupancy planning and move management.
Our team designs, builds, integrates and operationalizes technology applications that are customized to meet those organizational business needs. We are responsible for the end to end lifecycle of key applications and data streams that feed our Analytics team and Business Operations teams with high quality, well defined data. These data and data streams are critical to those applications and reporting platforms. Important business data originates from real time sensors, Wi-Fi, other databases, and people, then pipelines into storage, validation and then it is combined with other data in cloud storage, processed by software applications, and ends in visualization and decision support.
This specialized role will have to use a wide range of skills to understand business requests, identify the problem and break it into their business process, functional and system components. A clear and critical thinker is required to then bring new potential solutions to fruition, manage and analyze existing pipelines and data.
This role presents a unique opportunity to work with systems developed and maintained by the group as well as within corporate IT environments, multiple vendor systems, our AWS data lake, and all the pipelines in between. This includes working with data from various Internet of Things (IOT) networks and a great opportunity to collaborate and develop new streams and strategies on our horizon. The position must be filled with someone who has sharp critical thinking, combined with excellent project management and coordination skills, having a strong ability to work with both people representing the business people and those supporting roles within the IT organization. Technical skills are also extremely important, especially skills that support understanding SQL queries, database concepts, API’s, AWS/Cloud services, and developing Tableau as well. You’ll be responsible for understanding how real time data from sensors integrates with corporate systems, combined to cloud based data lake, funnels into data science analytics and will end in visualizations.
Responsibilities
● Understands and writes SQL code in Oracle and Redshift databases to support existing data structures, joins, views. Write and optimize new SQL to support new storage needs, reporting needs, and better performance and efficiency
● Makes decisions and solves problems using sound, inclusive reasoning and judgment. Gathers and analyzes information to fully understand a problem and proactively anticipates needs and prioritize action steps.
● Partners and communicates well with internal stakeholders, business partners, and Global IT to understand, define and help deliver technology needs
● Implements vetted strategies developed by the business to achieve technical roadmaps
● Manages project delivery end-to-end, in other cases helping our IT partner stay on project delivery milestones
● Wrangling structured, unstructured and poorly structured data into appropriate data structures.
● Auditing data in databases and/or reports with a goal of improving data quality from systems or data owners
● Identify opportunities to further build out our IoT strategy
● Develops new and supports existing Tableau dashboard platform, including how to improve sql queries and data management for optimal performance in reporting
Experience/Skills/Abilities:
● At least 3 years of experience working as a Systems Integrator, Data Engineer, Software Engineer or similar position demonstrating the ability to design and implement automation, data modeling, data wrangling, data analysis, and data vision solutions to complex problems, processes, and scenarios. Familiarity with common data structures and languages.
● BS Computer Science, Computer/Electrical Engineering, or Math Degree or relevant experience.
● Experience with IoT, cloud computing, distributed data systems
● Strong capabilities in SQL and Tableau, Excel/Google Suite
● Understands AWS platform (Amazon Redshift, S3, EC2, Glue jobs, etc.)
● Friendly and approachable, with strong communication and presentation skills
● Desire to keep current with a challenging and evolving environment
● Team focused and self-motivated. Able to work as part of a coordinated team, yet independently when necessary
● Proven abilities to take initiative and to be innovative; have an analytical mind with a problem-solving aptitude
● Demonstrated ability in leading technology projects
● Demonstrated experience in bridging business requirements and technical development
● Strong communications skills maintaining ties to product developers and stakeholders
● Demonstrated experience with software lifecycle management
Desirable Experience (but not required):
● Experience working with statistical teams and/or data scientists
● Tools/Programming Experience: Python, R, web services, other languages(e.g. Java, C++, scripting languages)
● AWS Certifications and working experience
● Experience with LEAN/KANBAN/SCRUM development methodologies is desirable
Job Type: Contract
Salary: Up to $90.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 4 years (Required)
SQL: 4 years (Required)
Tableau: 2 years (Required)
AWS: 2 years (Preferred)
Work Location: One location
Show Less
Report",$90.00 Per hour,1 to 50 Employees,Company - Private,Manufacturing,Machinery Manufacturing,#N/A,Unknown / Non-Applicable
Konnectingtree,Data Engineer,Remote,"Greetings from KonnectingTree!
We are looking for a Data Engineer for one of our clients. This is a remote position with an Implementation partner. AWS Certification Mandatory.
Data Engineer with AWS Experience
Experience with PySpark/Spark
Experience in Python
Able to work independently
Able to work with the business team directly
Interested candidates kindly share your updated resume with mythili.saravanan@konenctingtree.com. Please reach me at 952-679-2916.
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Required)
Python: 5 years (Required)
PySpark: 5 years (Required)
Work Location: Remote
Speak with the employer
+91 952-679-2916
Show Less
Report",$45.00 - $50.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
infinity quest,DATA ENGINEER,"Seattle, WA","At least 3 years of Data Engineer experience is required preferably in a cloud Environment.
You should have at least 4 years of coding experience in python/java/ Scala and open source packages with at least 2 years of experience with Databases(SQL/NOSQL etc).
Experience with large scale Distributed databases like redshift/Snowflake is a big Plus.
You should have Experience with different aspects of data systems including database design, data modeling, performance optimization, SQL etc.
Some Experience with building data pipelines and Orchestration(Airflow ,ADF,glue etc) is required.
Strong communication skills (able to explain concepts to non-technical audiences as well as peers)
Self-starter who is highly organized, communicative, quick learner, and team-oriented
Technology Requirements:
Python/Java or Scala , SQL and Airflow. Cloud experience AWS/Azure
Daily tasks:
Developing, executing, monitoring and troubleshooting Data pipelines and workflows in our cloud environment.
Work on Data Lake/DW/DQ and other framework related items
Team and cross functional collaboration as needed.
Preferred background/prior work experience:
3 years of DE expertise building data pipelines and working in a DW/Data lake Cloud based environment
Job Type: Contract
Salary: $65.00 per hour
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
Day shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road
Show Less
Report",$65.00 Per hour,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Violet Ink,Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
APLOMB Technologies,Data Engineer,"Princeton, NJ","We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$70T - $75T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"AGM Tech Solutions, LLC",Data Engineer,"Alpharetta, GA","We have an excellent 6-month contract-to-hire opportunity with our Global Leader client.
Candidate must be local to Alpharetta GA (Hybrid Flexibility) - 3 days a week on-site.
Basic
Work experience with ETL, Data Modeling, and Data Architecture.
Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL.
Experience operating very large data warehouses or data lakes.
Preferred
Experience in designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Comfortable using PySpark APIs to perform advanced data transformations.
Familiarity with implementing classes with Python.
Summary
Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue
Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python.
Design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Job Type: Contract
Pay: $70.00 per hour
Benefits:
401(k)
Flexible schedule
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Machine learning: 2 years (Required)
Work Location: One location
Show Less
Report",$70.00 Per hour,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Plaxonic,Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Impact Advisors LLC,Data Engineer,United States,"Healthcare Data Engineer

Work You’ll Do:

As a Healthcare Data Engineer, you will work closely with a multidisciplinary Agile team to build high-quality data pipelines driving analytic solutions. Utilizing your deep understanding of data architecture, data engineering, data analysis, reporting, and basic understanding of data science, the solutions you create will generate insights from the organization’s connected data which will enable the advancement of data-driven decision-making capabilities within the enterprise. You will utilize your strong problem-solving skills, ability to work as part of a technical, cross-functional analytics team, and desire to solve complex data problems to deliver the insights which enable analytics strategies.

About Impact Advisors:

We deliver Best in KLAS advisory, implementation and optimization services to healthcare organizations. At Impact Advisors, we are committed to exceeding our clients’ expectations. We are a nationally recognized partner to many of the nation’s top healthcare organizations. Our commitment to patient-centered, value-driven outcomes has earned us some of the industry’s most prestigious awards. Please visit our website at www.impact-advisors.com for additional information.

Your Responsibilities:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals
Solve complex data problems to deliver insights that help business to achieve goals
Create data products for analytics and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data & analytics professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics

Your Expertise:
Bachelor’s degree preferred; Computer Science, MIS, or Engineering preferred
Certification in one or more of the following Epic Systems modules: Cogito, Caboodle, Clarity, Reporting Workbench
5 years of experience working in data engineering or architecture role, 7+ preferred (3 years preferred for Jr. role)
Expertise in SQL and data analysis and experience with at least one programming language (Python preferred)
Significant experience developing and maintaining data warehouses in big data solutions (e.g., Snowflake, SAP Hana, Oracle, SQL Server, Teradata, etc.)
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies
Worked with BI tools such as Tableau, Power BI, Looker
Deep knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.
Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka
Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Experience in using data base connections, SSIS, API, ODBC, etc.
Healthcare experience preferred but not required.

Our People and Culture:

We believe in a caring, fun, honest and autonomous work environment and we recognize that our dedication to our associates drives our success. Our mission to create a Positive Impact fuels our associates to innovate and deliver high value services to our clients.

In healthcare, many of the greatest ideas and discoveries come from a diverse mix of minds, backgrounds and experiences, and we are committed to cultivating an inclusive work environment. Impact Advisors provides equal opportunities to all employees and applicants for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, genetic disposition, neurodiversity, disability, veteran status, or any other protected category under federal, state and local law
Show Less
Report",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $50 million (USD)
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
"Bluemont Technology & Research, Inc.",Data Engineer,"Norfolk, VA","NATO Data Engineer
Requirements:
Ts/sci or secret clearance
High proficiency level in English language
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets is required.
Description:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.
Job Type: Full-time
Pay: $90,000.00 - $130,000.00 per year
Experience level:
10 years
11+ years
4 years
5 years
6 years
7 years
8 years
9 years
Ability to commute/relocate:
Norfolk, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you have a secret clearance or TS/SCI?
Work Location: One location
Show Less
Report",$90T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Alianza, Inc.",Data Engineer,"Pleasant Grove, UT","Data Engineer
Alianza is looking for an experienced and results driven Data Engineer. The successful candidate will be the technical engine of the data team, building Python applications to ingest streaming and extracted data and persist to cloud storage. Will use Python and SQL with AWS cloud technologies to automate the generation and delivery of reports. Will utilize CI/CD technologies to fully automate the release of all compute and storage components to the cloud. Work with our data architect and Java developers to design creative, high-quality, data-oriented insights and dashboards. Significant focus of the position will be on streaming data pipelines, distributed datalake architectures, and AWS services. Question the status quo. Write clean, testable, resilient code. Make things go fast and have fun doing it!
Key Duties and Responsibilities:
Participate in the process of designing, data engineering, and developing data services (Streaming, ETL/ELT, Real-time analytics, Reporting) using Python, SQL and AWS services
Adhere to modern methodologies for designing, coding, and testing
Build connected, fully automated data systems and pipeline
Work effectively with remote teams in various remote time zones
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources into usable format
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Identify and implement automatable tasks and reusable frameworks
Participate in sprint planning meetings and provide reasonable estimations
Research and propose new process, techniques, or tools as solutions. Able to produce technical diagrams, explanations, and written documentation to promote proposed solutions
Collaborate with data team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements

Qualifications:
3+ years of Python development experience, preferably writing modules that implement part of a streaming or batch ETL system in a cloud hosted environment
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse / lake designs to stakeholders
Experience designing, building, and maintaining data processing systems
At least 3 years’ experience with modern DevOps automation ecosystems, preferably Git/GitHub/Bitbucket, Buildkite or Ansible (or similar)
Real-world experience handling large data volumes (terabytes of storage and billions of rows)
At least 3 years experience configuring, using, and performance tuning AWS cloud services:preferably S3, Glue, Athena, Kinesis, Firehose, Lambda, Cloudwatch, ECS, API Gateway, RDS (Postgres), SQS, SNS, SES
Experience using AWS Redshift (or similar)
Experience with CloudFormation or TerraForm
Ability to prioritize, learn quickly, and do high-quality work
Demonstrate understanding of modern APIs and endpoints, like REST and GraphQL
Working understanding of Agile dev methodologies, especially Scrum and Kanban
Good listener, communicator, collaborator, and documenter
Proficient with Linux and shell scripting
Experience with data warehouse, data mart, OLAP, dimensional modeling, Kimball method
Good understanding of relational and document database concepts and best practices
Know how to design a clean, performance-optimized relational data model, and reverse engineer existing databases into physical data model diagrams
Experience using C*, Spark, Kafka, KSQL, Confluent, Pulsar and/or Kinesis helpful
Automated testing experience using JUNIT or equivalent
Some experience in software engineering (front, middle, back or all three) and application architecture
Show Less
Report",$73T - $1L,201 to 500 Employees,Company - Private,Information Technology,Software Development,2009,$25 to $50 million (USD)
Gladly,Senior Data Engineer,"San Francisco, CA","Gladly is a Radically Personal Customer Service Platform that puts people at the center of a single, lifelong conversation. We enable companies to talk to their customers they way people talk to their friends: seamlessly across voice, email, SMS, chat, and social media.
Gladly's data products are a foundation for enabling contact center leaders to understand their team's performance and identify opportunities for their company. Because of Gladly's unique approach to customer service, the data we provide is a key differentiator, not an afterthought. Our data warehouse also gives Gladly's customer success team the insights to help customers optimize their use of the product. We create a range of metrics and datasets based on carefully designed events and data models. We are looking for a data engineer to join our small, fast-growing and high impact team.
What you'll do
Own and drive projects, as well as communicate with stakeholders on requirements, progress and delivery.
Teach. Provide technical guidance and mentorship in software engineering best practices while demonstrating these as an individual contributor.
Collaborate. Work closely with small, nimble, cross-functional teams of engineers, product managers, designers, and business teams.
Contribute. Build a best-in-class data pipeline with a few key attributes:
repeatable via infrastructure-as-code
testable, with verification of correctness
reliable and always-on
low latency (on the order of minutes)
observable.
Work with experienced colleagues who will be eager to share their knowledge, provide mentorship and help you grow your career.
Have opportunities to learn and work with technologies used at Gladly like Snowflake, dbt, Debezium, Looker, PostgreSQL, Kafka, Docker, Kubernetes, AWS, Redis, Node.js, Go, Python.
You'll be successful by
Being eager to learn Gladly's business domain and apply this knowledge in building the innovative product.
Self-organizing and prioritizing your work based on the impact to the customer.
Understanding how to balance pragmatic solutions with best practices of data engineering.
Having passion for making the most of our existing technologies and introducing the right tools for problem at hand.
Showing ownership and pride in your work by promoting data best practices and making them easy for engineering teams to adopt as well as providing ongoing maintenance and support.
We're excited about you because you have
5+ years of engineering experience including 2+ years of working with ETL pipelines, data transformation and modeling.
Strong teamwork skills. You love participating with high-performing teams of engineers.
Customer-centricity and product focus. You look at everything you create through the lens of how it improves things for the end-customer. You are comfortable communicating how various technical approaches might impact product behavior (and vice versa).
Learning mentality because nobody checks every box. You aren't intimidated by new domains or technology; you're willing to dive into documentation/videos, talk to your teammates, and experiment to become well-versed.
Willingness to work across the development stack. You're comfortable with working on data pipelines, transformations and implementing insights. You're willing to jump into Gladly backend applications on occasion.
Operational expertise. You value robust observable solutions with actionable monitoring and the importance of tooling for troubleshooting and resolving issues.
Research has shown that individuals from marginalized groups are less likely to apply to jobs where they don't meet 100% of the criteria. Gladly values diversity of experience, so if you believe you have the right skill set, we welcome you to apply - even if you don't check every box in the job description. We're committed to an inclusive workplace and would love to see if you could be the next great addition to our team.
Compensation
$156,000-$215,000 annually.
For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above.
Working at Gladly
People are not just at the heart of our product, they're at the heart of our company.
We value diverse perspectives and hire new people to enrich our mix, not keep it the same.
We believe in open communication and share in an inclusive, open culture.
We have embraced remote work and make it easy for our team to work from anywhere, but we also invest in opportunities to get the teams together in person regularly.
We learn from each other, and we help each other learn.
We provide opportunities to move between teams to learn and contribute to other cool technologies used at Gladly.
We have a strong work ethic, but value life outside of work, too.
Our focus is on people and that starts with our employees. As an employee you can count on:
Competitive salaries, stock options
Medical, Dental, Vision and Life insurance
Generous paid time off
Generous paid Parental Leave
401K
Flexible Spending Accounts
Wellness and home office stipends
Founded in 2014 by a team of repeat entrepreneurs with multiple successful exits, Gladly is reinventing customer service. By focusing on customers instead of tickets, we are disrupting a $70B market and are proud to count Crate and Barrel, Warby Parker and many other innovative brands as customers. Gladly has raised over $110M from Greylock Partners, NEA, GGV Capital, Glynn Capital and JetBlue Tech Ventures.
Gladly has made the decision to become a fully distributed company, allowing employees to live anywhere in the United States, and candidates to come from nearly any geographical region. That said, we also highly value our collaborative and creative culture and commit to meeting in real life as a company at least once per quarter when it is safe to do so.
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2014,Unknown / Non-Applicable
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
InfoQuest Consulting Group Inc.,Data Engineer,"Philadelphia, PA","Duration & Type: 12 months Contract with a media & communications industry client
Location: Philadelphia, PA
No. of Positions: Multiple
Responsibilities:
Develop solutions to big data problems utilizing common tools found in the ecosystem.
Develop solutions to real-time and offline event collecting from various systems.
Develop, maintain, and perform analysis within a real-time architecture supporting large amounts of data from various sources.
Analyze massive amounts of data and help drive prototype ideas for new tools and products.
Design, build and support APIs and services that are exposed to other internal teams
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Required:
5+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in one or more languages: Python, Scala/Java, Spark, Batch, Streaming, ML
Experience with Python unit testing and code coverage frameworks
Experienced in NoSQL / SQL, Microservice, RESTful API development
Strong Experience with AWS Core such as Kinesis, Lambda, API Gateway, CloudFormation, CloudWatch
Experienced with one of the Analytics tools – Presto / Athena, QuickSight, Tableau
Strong Experience with Container technologies and Real-time Streaming (such as Kafka, Kinesis)
Preferred:
Test-driven development/test automation, continuous integration, and deployment automation experience
Experience with Performance tuning at scale
Experience working on big data platforms in the cloud or on traditional Hadoop platforms
Experience working in agile/iterative development and delivery environments
Enjoy working with data – data analysis, data quality, reporting, and visualization
Great design and problem solving skills, with a strong bias for architecting at scale
Excellent communication skills
Experience in software development of large-scale distributed systems
For consideration, please send resume to career@infoquestgroup.com
Show Less
Report",$89T - $1L,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Royal Credit Union,Data Engineer,"Eau Claire, WI","Team Members are at the core of who we are; living our core values of making a difference, doing the right thing, making it easy, being caring, being friendly, nice, and respectful, and taking ownership. Perhaps Royal is the change you’re looking for. Royal Credit Union is looking for caring, energetic team members who want to create a positive impact in the lives of our Members.

Is this you? If so, let us know by submitting an application!

Description:
The Data Engineer will work in a team that is building the Business Intelligence platform at Royal. This includes the use of various methods to analyze and transform raw data into useful cloud-based data systems. Involving increasingly complex level of data and statistical analysis in support of creating and shaping data systems, pipelines, and other business-related activities. Resolutions may require more in-depth quantitative analysis with non-routine solutions. Support line of business for all quantitative needs to include leading mid-size initiatives.

Work Schedule:
This is a full-time, exempt level position. Hours will generally be Monday through Friday, 8:30am – 5:00pm with occasional evening and weekend hours, as needed.

Required Experience/Education/Skills:
Bachelor’s Degree in software engineering, data engineering, data science, information systems, computer science, mathematics, machine learning, or related field of study or the equivalent in relevant work experience.
Four (4) years of experience in data engineering, data science, or software engineering experience, including knowledge of extensive data ecosystem.
Preferred Experience/Education/Skills:
Experienced in implementing Data and Advanced Analytic solutions, or related experience in the Cloud (AWS, Azure, etc.).
Experience implementing an end-to-end Cloud native platform for Datawarehouse (Snowflake, Databricks, Big Query, Redshift, etc.).
Experience with Data connectivity methodologies such as APIs (Rest/Soap), ODBC/JDBC, HTTP web hooks, Json, etc.
Intermediate experience with data management principles.
We will be communicating with you by email and/or text on the status of your application. Please monitor your email inbox and junk folder for status updates and additional instructions.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, protected veteran status or status as an individual with disability.
Show Less
Report",$92T - $1L,501 to 1000 Employees,Company - Private,Finance,Banking & Lending,1964,$2 to $5 billion (USD)
Open Systems Technologies Corporation,Junior Data Engineer,"Arlington, VA","Open Systems Technologies Corporation is looking for a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform.

Responsibilities

As a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members.

Required Skills:
Clearance: Secret
4+ years of experience with SQL
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
4+ years of experience with a modern programming language such as Python or Java
4 years of experience working in a big data and cloud environment
Experience with distributed computer understanding and experience with SQL, Spark, ETL.
Documented experience with AWS, EC2, S3, and/or RDS
Preferred Skills:
2 years of experience working in an agile development environment
Ability to quickly learn technical concepts and communicate with multiple functional groups
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes
BENEFITS

Our company OST has been operating since 1996 and have various contracts with Government agencies. We offer a comprehensive benefit package that includes 3 weeks paid time off, 2 weeks Holiday pay, medical/dental coverage, STD, LTD, Life Insurance, ADD, 401k with up to 4% match, and end of year profit sharing paid out in 401k.
Show Less
Report",$69T - $1L,Unknown,Company - Private,Information Technology,Information Technology Support Services,1996,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
infinity quest,DATA ENGINEER,"Seattle, WA","At least 3 years of Data Engineer experience is required preferably in a cloud Environment.
You should have at least 4 years of coding experience in python/java/ Scala and open source packages with at least 2 years of experience with Databases(SQL/NOSQL etc).
Experience with large scale Distributed databases like redshift/Snowflake is a big Plus.
You should have Experience with different aspects of data systems including database design, data modeling, performance optimization, SQL etc.
Some Experience with building data pipelines and Orchestration(Airflow ,ADF,glue etc) is required.
Strong communication skills (able to explain concepts to non-technical audiences as well as peers)
Self-starter who is highly organized, communicative, quick learner, and team-oriented
Technology Requirements:
Python/Java or Scala , SQL and Airflow. Cloud experience AWS/Azure
Daily tasks:
Developing, executing, monitoring and troubleshooting Data pipelines and workflows in our cloud environment.
Work on Data Lake/DW/DQ and other framework related items
Team and cross functional collaboration as needed.
Preferred background/prior work experience:
3 years of DE expertise building data pipelines and working in a DW/Data lake Cloud based environment
Job Type: Contract
Salary: $65.00 per hour
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
Day shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road
Show Less
Report",$65.00 Per hour,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Gladly,Senior Data Engineer,"San Francisco, CA","Gladly is a Radically Personal Customer Service Platform that puts people at the center of a single, lifelong conversation. We enable companies to talk to their customers they way people talk to their friends: seamlessly across voice, email, SMS, chat, and social media.
Gladly's data products are a foundation for enabling contact center leaders to understand their team's performance and identify opportunities for their company. Because of Gladly's unique approach to customer service, the data we provide is a key differentiator, not an afterthought. Our data warehouse also gives Gladly's customer success team the insights to help customers optimize their use of the product. We create a range of metrics and datasets based on carefully designed events and data models. We are looking for a data engineer to join our small, fast-growing and high impact team.
What you'll do
Own and drive projects, as well as communicate with stakeholders on requirements, progress and delivery.
Teach. Provide technical guidance and mentorship in software engineering best practices while demonstrating these as an individual contributor.
Collaborate. Work closely with small, nimble, cross-functional teams of engineers, product managers, designers, and business teams.
Contribute. Build a best-in-class data pipeline with a few key attributes:
repeatable via infrastructure-as-code
testable, with verification of correctness
reliable and always-on
low latency (on the order of minutes)
observable.
Work with experienced colleagues who will be eager to share their knowledge, provide mentorship and help you grow your career.
Have opportunities to learn and work with technologies used at Gladly like Snowflake, dbt, Debezium, Looker, PostgreSQL, Kafka, Docker, Kubernetes, AWS, Redis, Node.js, Go, Python.
You'll be successful by
Being eager to learn Gladly's business domain and apply this knowledge in building the innovative product.
Self-organizing and prioritizing your work based on the impact to the customer.
Understanding how to balance pragmatic solutions with best practices of data engineering.
Having passion for making the most of our existing technologies and introducing the right tools for problem at hand.
Showing ownership and pride in your work by promoting data best practices and making them easy for engineering teams to adopt as well as providing ongoing maintenance and support.
We're excited about you because you have
5+ years of engineering experience including 2+ years of working with ETL pipelines, data transformation and modeling.
Strong teamwork skills. You love participating with high-performing teams of engineers.
Customer-centricity and product focus. You look at everything you create through the lens of how it improves things for the end-customer. You are comfortable communicating how various technical approaches might impact product behavior (and vice versa).
Learning mentality because nobody checks every box. You aren't intimidated by new domains or technology; you're willing to dive into documentation/videos, talk to your teammates, and experiment to become well-versed.
Willingness to work across the development stack. You're comfortable with working on data pipelines, transformations and implementing insights. You're willing to jump into Gladly backend applications on occasion.
Operational expertise. You value robust observable solutions with actionable monitoring and the importance of tooling for troubleshooting and resolving issues.
Research has shown that individuals from marginalized groups are less likely to apply to jobs where they don't meet 100% of the criteria. Gladly values diversity of experience, so if you believe you have the right skill set, we welcome you to apply - even if you don't check every box in the job description. We're committed to an inclusive workplace and would love to see if you could be the next great addition to our team.
Compensation
$156,000-$215,000 annually.
For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above.
Working at Gladly
People are not just at the heart of our product, they're at the heart of our company.
We value diverse perspectives and hire new people to enrich our mix, not keep it the same.
We believe in open communication and share in an inclusive, open culture.
We have embraced remote work and make it easy for our team to work from anywhere, but we also invest in opportunities to get the teams together in person regularly.
We learn from each other, and we help each other learn.
We provide opportunities to move between teams to learn and contribute to other cool technologies used at Gladly.
We have a strong work ethic, but value life outside of work, too.
Our focus is on people and that starts with our employees. As an employee you can count on:
Competitive salaries, stock options
Medical, Dental, Vision and Life insurance
Generous paid time off
Generous paid Parental Leave
401K
Flexible Spending Accounts
Wellness and home office stipends
Founded in 2014 by a team of repeat entrepreneurs with multiple successful exits, Gladly is reinventing customer service. By focusing on customers instead of tickets, we are disrupting a $70B market and are proud to count Crate and Barrel, Warby Parker and many other innovative brands as customers. Gladly has raised over $110M from Greylock Partners, NEA, GGV Capital, Glynn Capital and JetBlue Tech Ventures.
Gladly has made the decision to become a fully distributed company, allowing employees to live anywhere in the United States, and candidates to come from nearly any geographical region. That said, we also highly value our collaborative and creative culture and commit to meeting in real life as a company at least once per quarter when it is safe to do so.
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2014,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Arthur Grand Technologies Inc,Azure Data Engineer,"Mount Laurel, NJ","Role: Senior/Lead Azure Data Engineer – On Prem (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC
Experience: 8-12+ Years
Azure Data Engineer
Job Description:
Must Have:
More than 12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have:
Any relevant certifications
Thanks
Saranya Ponmudi | Technical Recruiter
Arthur Grand Technologies Inc
44355 Premier Plaza, Suite 110, Ashburn, VA 20147
T: +1 614-500-8416/ +1 703-219-8023
Job Types: Full-time, Contract
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
Mt. Laurel, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 4 years (Preferred)
Azure: 5 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Show Less
Report",$91T - $1L,1 to 50 Employees,Company - Private,Information Technology,Software Development,2012,$1 to $5 million (USD)
princeton it services,Data Engineer lead,"Boston, MA","Data Engineer Lead
Job Description:
Position: Data Engineer Lead
Location: Raleigh, NC or Boston, MA
Job Length: Long term
Position Type: C2C/W2
Qualifications:
9+ years Experience in Alation, Collibra, Snowflake
9+ years Experience in Java , Spring boot , spark , Scala.
Stays current with technology trends in order to provide best options for solutions • Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently.
Responsibilities:
Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data management and Stewardship using Collibra.Alation
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required :Alation, Collibra, Snowflake
Job Type: Contract
Salary: From $65.00 per hour
Schedule:
8 hour shift
Experience:
collibra: 5 years (Preferred)
snowflake: 5 years (Preferred)
aliation: 4 years (Preferred)
Work Location: On the road
Speak with the employer
+91 6093006906
Show Less
Report",$65.00 Per hour,1 to 50 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2008,$1 to $5 million (USD)
Transcarent,Senior Data Engineer,"San Francisco, CA","Who we are
Healthcare is more confusing, more costly, and more complex than ever. Transcarent is a health and care experience company on a mission to empower Members to stay healthy by providing them with unbiased information, trusted guidance, and easy access to high value care where and when they need it. You will be part of a world-class team, supported by top tier investors like 7wireVentures and General Catalyst, and founded by a mission-driven team committed to transforming the health and care experience for all. We closed on our Series C funding in January 2022, raising our total funding to $298 million and enabling us to respond to the demand for rapid expansion of our offering.
Transcarent is committed to growing and empowering a diverse and inclusive community within our company. We believe that a team with diverse lived experiences, working together will strengthen our organization, and our ability to deliver ""not just better but different"" experiences for our members.
What we look for in our teammates
We are looking for teammates to join us in building our company, culture, and Member experience who:
Put people first, and make decisions with the Member's best interests in mind
Are active learners, constantly looking to improve and grow
Are driven by our mission to measurably improve health and care each day
Bring the energy needed to transform health and care, and move and adapt rapidly
Are laser focused on delivering results for Members, and proactively problem solving to get there
We are looking for data engineers excited to create a single source of truth that will power the decisions for the company for other developers, products, clinical teams, data analysts, data scientists, member experience, and more. They will need to be able to ingest various sorts of application and health data, leverage our Snowflake data warehouse to load it, and transform the data into various data sets to offer it self-serve in a secure, compliant manner. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

In this role, you will...
Be a data champion and seek to empower others to leverage the data to its full potential.
Create and maintain optimal data pipeline architecture with high observability and robust operational characteristics.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal data extraction, transformation, and loading using SQL, python, and dbt from various sources.
Work with stakeholders, including the Executive, Product, Clinical, Data, and Design teams, to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
About You
You are entrepreneurial and mission-driven and can present your ideas with clarity and confidence.
You are a high-agency person. You refuse to accept undue constraints and the status quo and will not rest until you figure things out.
Advanced expertise in python and dbt for data pipelines
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing big data pipelines, architectures, and data sets. A definite plus with healthcare experience
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Good to have healthcare domain experience.
Tech Stack
The following is a list of software/tools that would be nice to have but not required:
Experience with cloud-based data warehouse: Snowflake
Experience with relational SQL and NoSQL databases
Experience with object-oriented/object function scripting languages: Golang, Python, Java, C++, Scala, etc.
Experience with big data tools: Spark, Kafka, etc.
Experience with data pipeline and workflow management tools like Airflow
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Total Rewards
As a remote position, the salary range for this role is $140,000 – $170,000.
Individual compensation packages are based on a few different factors unique to each candidate, including primary work location and an evaluation of a candidate's skills, experience, market demands, and internal equity.
Salary is just one component of Transcarent's total package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock options.
Our benefits and perks programs include, but are not limited to:
Competitive medical, dental, and vision coverage
Competitive 401(k) Plan with a generous company match
Flexible Time Off/Paid Time Off, 12 paid holidays
Protection Plans including Life Insurance, Disability Insurance, and Supplemental Insurance
Mental Health and Wellness benefits
Location
You must be authorized to work in the United States. Depending on the position we may have a preference to a specific location, but are generally open to remote work anywhere in the US.
Transcarent is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. If you are a person with a disability and require assistance during the application process, please don't hesitate to reach out!
Research shows that candidates from underrepresented backgrounds often don't apply unless they meet 100% of the job criteria. While we have worked to consolidate the minimum qualifications for each role, we aren't looking for someone who checks each box on a page; we're looking for active learners and people who care about disrupting the current health and care with their unique experiences.
Show Less
Report",$1L - $2L,201 to 500 Employees,Company - Private,Healthcare,Healthcare Services & Hospitals,2020,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Plaxonic,Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Visvak Solutions,Azure Data Engineer,Remote,"JD:
Overall 7+ years of experience
has good understanding on Azure storage Gen2
hands on experience with Azure stack (minimum 5 years)
o Azure Databricks
o Azure Data Factory
o Azure DevOps
proficient coding experience using Spark(Scala/Python), T-SQL
Understanding around the services related to Azure Analytics, Azure SQL, Azure function app, logic app
prior ETL development experience using industry tool e.g. informatica/SSIS/Talend etc.
proficient in a source code control system
good to have knowledge in Kafka streaming Azure Infrastructure
Job Type: Full-time
Salary: $39.86 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: Remote
Show Less
Report",$39.86 Per hour,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
MetroSys,Data Migration Engineer,Remote,"Job Description
Context
We have a great project and need Migration Engineers to assist with migration of an enterprise client to a new data center. This new IT production environment is based in Redhat Linux, IBM AIX, Sun Solaris, UNIX and HP UX. We are searching for an engineer to support the migration by migrating servers, storage and databases to the new environment. We use a strict step-by-step plan (factory plan) to efficiently execute the migration.

Competence
Bachelor of Science / Master's degree
Minimal 3-5 years of relevant work experience within an enterprise environments
Advanced knowledge of Redhat Linux and UNIX
Advanced knowledge of IBM AIX, Solaris, and HP UX
Some knowledge of IBM XIV, Pure Storage, HPE Nimble and 3PAR storage preferred
Experience with databases (Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
The candidate has a hands-on mindset, a strong customer- and problem-solving orientation, shows fast results, and has demonstrated good communication skills, especially in an international IT organization. To achieve the project goals, the candidate is able to liaise directly with all stakeholders. The candidate has a clear focus on results and quality, and is eager to develop quickly as a project leader, serving customers.

Activities
Intake / analysis of applications for migration
Creation or update of migration run books
Migration of VMware instances to the new platform
Creation of storage disks, virtual storage devices
Reconfiguration of servers, including storage & network
1RfnvbOr2v
Show Less
Report",$50.00 - $70.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
#N/A,Data Engineer - Remote,"Phoenix, AZ","At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing *your life's best work.(sm)*
**
You’ll enjoy the flexibility to work remotely from anywhere within the U.S. as you take on some tough challenges.*
Primary Responsibilities:*
*
Accountable for data engineering lifecycle including research, proof of concepts, design, development, test, deployment and maintenance
Design, develop, implement and run cross-domain, modular, optimized, flexible, scalable, secure, reliable and quality data solutions that transform data for meaningful analyses and analytics while ensuring operability
Design, develop, implement and run data solutions that improve data efficiency, reliability and quality, and are performant by design
Layer in instrumentation in the development process so that data pipelines can be monitored. Measurements are used to detect internal problems before they result into user visible outages or data quality issues
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking
Tech debt reduction/ Tech transformation including Open-source adoption, Cloud adoption, HCP assessment and adoption
Contribution to our industry community and strive to reuse and share components wherever possible across the organization
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Identifies solutions to non-standard requests and problems
Solves moderately complex problems and/or conducts moderately complex analyses
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.*
*Required Qualifications:*
*
Undergraduate degree or equivalent experience
3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python
3+ years of experience in Data Engineering, Coding ETL and building data pipelines
1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc.
1+ years of experience writing data engineering code in Databricks
Preferred Qualifications:*
*
Cloud experience (Azure/AWS/GCP)
Snowflake experience
Proficient in building relationship with stakeholder and maintaining it during the course of the project/program
Proficient in working with cross functional teams
Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.*
*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.
California, Colorado, Connecticut, Nevada, New York City, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New York City, or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Job Type: Full-time
Application Question(s):
Do you have a Bachelor’s Degree or equivalent underwriting work experience?
Do you have 3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python?
Do you have 3+ years of experience in Data Engineering, Coding ETL and buidling data pipelines?
Do you have 1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc. ?
Do you have 1+ years of experience writing data engineering code in Databricks?
To apply to this job, click Easy Apply
Show Less
Report",$73T - $1L,10000+ Employees,Company - Public,Healthcare,Healthcare Services & Hospitals,1977,$10+ billion (USD)
Plaxonic Technologies,Sr. Data Engineer,"Dallas, TX","Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Salary: $140,000.00 - $150,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Pyspark: 8 years (Required)
Azure: 8 years (Required)
Work Location: On the road
Speak with the employer
+91 (727) 241- 5640
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Edgesource,Remote Data Engineer,"Alexandria, VA","EOE Statement
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law.
Category
Information Technology
Description
Experience:
**Fully Remote** Must be US Citizen and able to obtain a TS/SCI
Develop standardized data architecture that includes data structure and transfer protocols to facilitate sensor integration and dynamically share information to improve situational awareness.
Develop a standardized architecture that supports a centralized data repository that advances all data analytics, and AIML capabilities enhance command and control decisions.
Design and build end-to-end data pipeline solutions (esp. streaming and batch processing, machine learning model training and updating).
Develop strategies for data acquisitions, archive recovery, and implementation of a database.
Define, design, and build dimensional databases.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Works closely with all engineering teams to develop strategy for long term data platform architecture.

Qualifications / Skills:
SQL/MySQL/PostgreSQL
Microservice deployment, cloud certification (AWS architect)
Education, Experience, and Licensing Requirements:
BS or MS degree in Computer Science or a related technical field
2-4+ years of experience as a data engineer
U.S. Citizenship with ability to obtain government security clearance

Position Requirements

Full-Time/Part-Time: Full-Time

Shift: -not applicable-

Position: Data Architect

Number of Openings: 1

Req Number: INF-20-00012

Open Date: 8/24/2020
Location: Mountain View
About the Organization: Edgesource Corporation is a leading small business providing information technology and business consulting services to the federal government. We are a stable, growing business offering excellent growth potential for business minded professionals.
Show Less
Report",$73T - $1L,51 to 200 Employees,Company - Public,Information Technology,Information Technology Support Services,#N/A,$5 to $25 million (USD)
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Plaxonic,Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
AgriCapture,Senior Data Engineer,"Nashville, TN","Job Title: Senior Data Engineer
Reports to: Director of Technology
Location: Nashville, TN
Start Date: May 1, 2023
Position Summary:
AgriCapture is a mission-driven company that certifies Climate-Friendly practices on farms, ranchlands and grasslands and quantifies associated emissions reductions, enabling producers to generate revenue for their sustainable management practices. By certifying agricultural products as Climate-Friendly and developing carbon credits, we serve corporations who are reducing and offsetting their GHG emissions while empowering consumers to consciously select Climate-Friendly products.
As the Senior Data Engineer, you will play a central role in developing a variety of proprietary systems and data pipelines that will enable the company to balance robust and cost effective, best-of-breed services to support climate friendly agricultural practice adoption and carbon credit issuance. In this role, you will work close to the business and IT leadership in the design and development of agile data architectures that evolve as new trends emerge. Your expertise will be a vital piece to the company and its mission and greater purpose. This will be a dynamic, fast-paced position providing a unique opportunity to be a part of a growing company that is poised to have a positive environmental impact.
Objectives of the Role
Build data tools and systems that scale and leverage AgriCapture’s core competency and competitive advantage
Apply conceptual knowledge of business processes and technology to solve complex business process and procedural problems
Serves as a technical advisor and a subject matter expert to internal and external staff who perform development and IT related functions
Work with Product and Business Analysis in transforming business requirements into actions that create value
Proven history to acquire, scale and lead with data
Responsibilities
Proficient working with large, complex data sets, with data lake and warehouse in cloud environments
Uses industry best practice, proactively analyze existing software architecture and new development to improve data quality
Develop and maintain data models for data lake house solutions
Work with Business Analysts to validate processes of test / use cases and then optimizes data load jobs to improve performance and automate
Proficient in creating, maintaining, and auditing ETL processes using Cloud technologies
Work with the analysts developing the requirements of the data warehouse solution
Provide clear analysis and written documentation including unit and quality assurance test plans for the development of newly designed applications and redesigns, data modeling and all associated tasks
Create solutions to improve the performance and availability of self-service analytics
Lead project efforts, ensuring project requirements and timelines are met and may guide, mentor, and oversee the work of other technical staff
Skills and Qualifications
4+ years of experience building production data pipelines in cloud environments
Experience with multiple file types including Apache Parquet, Avro
4+ years of experience in programming in Python, PowerShell, Bash, T-SQL
Experience with version control repositories.
Skilled at writing, testing, debugging new and existing code based on program area knowledge, conceptual and technical design specifications
Proficiency with scheduling and automation of ETL processes and file processing Proficiency with business intelligence products
Benefits
100% of employee medical premiums company paid
Employer HSA contribution
Coverage for Dental, Vision, Disability, and Life Insurance
Identity Theft and Prepaid Legal coverage options available
Competitive Pay
Time away: Flexible PTO and paid holidays
401k with company match
Allowance for office equipment
Monthly happy hours, weekly lunch catering and office snacks and drinks
AgriCapture is committed to creating a diverse environment and is proud to be an equal-opportunity employer. AgriCapture recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Job Type: Full-time
Ability to commute/relocate:
Nashville, TN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person
Show Less
Report",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
Gladly,Senior Data Engineer,"San Francisco, CA","Gladly is a Radically Personal Customer Service Platform that puts people at the center of a single, lifelong conversation. We enable companies to talk to their customers they way people talk to their friends: seamlessly across voice, email, SMS, chat, and social media.
Gladly's data products are a foundation for enabling contact center leaders to understand their team's performance and identify opportunities for their company. Because of Gladly's unique approach to customer service, the data we provide is a key differentiator, not an afterthought. Our data warehouse also gives Gladly's customer success team the insights to help customers optimize their use of the product. We create a range of metrics and datasets based on carefully designed events and data models. We are looking for a data engineer to join our small, fast-growing and high impact team.
What you'll do
Own and drive projects, as well as communicate with stakeholders on requirements, progress and delivery.
Teach. Provide technical guidance and mentorship in software engineering best practices while demonstrating these as an individual contributor.
Collaborate. Work closely with small, nimble, cross-functional teams of engineers, product managers, designers, and business teams.
Contribute. Build a best-in-class data pipeline with a few key attributes:
repeatable via infrastructure-as-code
testable, with verification of correctness
reliable and always-on
low latency (on the order of minutes)
observable.
Work with experienced colleagues who will be eager to share their knowledge, provide mentorship and help you grow your career.
Have opportunities to learn and work with technologies used at Gladly like Snowflake, dbt, Debezium, Looker, PostgreSQL, Kafka, Docker, Kubernetes, AWS, Redis, Node.js, Go, Python.
You'll be successful by
Being eager to learn Gladly's business domain and apply this knowledge in building the innovative product.
Self-organizing and prioritizing your work based on the impact to the customer.
Understanding how to balance pragmatic solutions with best practices of data engineering.
Having passion for making the most of our existing technologies and introducing the right tools for problem at hand.
Showing ownership and pride in your work by promoting data best practices and making them easy for engineering teams to adopt as well as providing ongoing maintenance and support.
We're excited about you because you have
5+ years of engineering experience including 2+ years of working with ETL pipelines, data transformation and modeling.
Strong teamwork skills. You love participating with high-performing teams of engineers.
Customer-centricity and product focus. You look at everything you create through the lens of how it improves things for the end-customer. You are comfortable communicating how various technical approaches might impact product behavior (and vice versa).
Learning mentality because nobody checks every box. You aren't intimidated by new domains or technology; you're willing to dive into documentation/videos, talk to your teammates, and experiment to become well-versed.
Willingness to work across the development stack. You're comfortable with working on data pipelines, transformations and implementing insights. You're willing to jump into Gladly backend applications on occasion.
Operational expertise. You value robust observable solutions with actionable monitoring and the importance of tooling for troubleshooting and resolving issues.
Research has shown that individuals from marginalized groups are less likely to apply to jobs where they don't meet 100% of the criteria. Gladly values diversity of experience, so if you believe you have the right skill set, we welcome you to apply - even if you don't check every box in the job description. We're committed to an inclusive workplace and would love to see if you could be the next great addition to our team.
Compensation
$156,000-$215,000 annually.
For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above.
Working at Gladly
People are not just at the heart of our product, they're at the heart of our company.
We value diverse perspectives and hire new people to enrich our mix, not keep it the same.
We believe in open communication and share in an inclusive, open culture.
We have embraced remote work and make it easy for our team to work from anywhere, but we also invest in opportunities to get the teams together in person regularly.
We learn from each other, and we help each other learn.
We provide opportunities to move between teams to learn and contribute to other cool technologies used at Gladly.
We have a strong work ethic, but value life outside of work, too.
Our focus is on people and that starts with our employees. As an employee you can count on:
Competitive salaries, stock options
Medical, Dental, Vision and Life insurance
Generous paid time off
Generous paid Parental Leave
401K
Flexible Spending Accounts
Wellness and home office stipends
Founded in 2014 by a team of repeat entrepreneurs with multiple successful exits, Gladly is reinventing customer service. By focusing on customers instead of tickets, we are disrupting a $70B market and are proud to count Crate and Barrel, Warby Parker and many other innovative brands as customers. Gladly has raised over $110M from Greylock Partners, NEA, GGV Capital, Glynn Capital and JetBlue Tech Ventures.
Gladly has made the decision to become a fully distributed company, allowing employees to live anywhere in the United States, and candidates to come from nearly any geographical region. That said, we also highly value our collaborative and creative culture and commit to meeting in real life as a company at least once per quarter when it is safe to do so.
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2014,Unknown / Non-Applicable
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Visvak Solutions,Azure Data Engineer,Remote,"JD:
Overall 7+ years of experience
has good understanding on Azure storage Gen2
hands on experience with Azure stack (minimum 5 years)
o Azure Databricks
o Azure Data Factory
o Azure DevOps
proficient coding experience using Spark(Scala/Python), T-SQL
Understanding around the services related to Azure Analytics, Azure SQL, Azure function app, logic app
prior ETL development experience using industry tool e.g. informatica/SSIS/Talend etc.
proficient in a source code control system
good to have knowledge in Kafka streaming Azure Infrastructure
Job Type: Full-time
Salary: $39.86 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: Remote
Show Less
Report",$39.86 Per hour,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Stash,Senior Data Engineer,"New York, NY","Want to help everyday Americans build wealth? Financial inequality is increasing and too many people are getting left behind. At Stash, we believe in the power of simplifying investing, making it easy and affordable for everyday Americans to build wealth and achieve their financial goals.
We're one of the fastest growing fintechs in the U.S. and have had another record-breaking year. In 2021 we almost doubled our headcount and valuation. Our personal finance app makes investing easy and affordable; this year 6 million customers set aside more than $3 billion with Stash.
Prioritizing People is one of our core values and has been key to a healthy work-life balance and a great sense of fulfillment and inclusion. We employ a true people first - hybrid model. Live and work where you feel the most productive, whether that is in our home, in an office, or a combination of both. Anywhere in the US or UK.
Let's solve complex problems and tackle wealth inequality.
We look for people who will help raise the bar for our entire engineering organization in terms of tech prowess, passion for collaboration and desire to mentor and educate fellow team members. We look for strategic thinkers and creative problem solvers with a bias for execution and we'll expect you to contribute code as well as product/feature ideas from the get-go.
Our team has built an amazing modern data platform and we would like to add many advancements such as real time streaming, many tools around data governance. As a Data Engineer, you will be responsible for enhancing our data infrastructure to take it to the next level, in collaboration with the team members. You will also be an active contributor in the ongoing maintenance of the existing pipelines. Stash is a data-driven organization and data infrastructure is a critical part of our overall infrastructure. You will have the opportunity to make an impact in the companies' OKRs by coordinating with data science, marketing teams and backend teams by aligning with their data needs. We work with the latest technologies in the big data space and are seeking folks who would like to do the same.
Tech stack (evolving):
Spark, Scala, Python, Kafka, AWS EMR, Hive, Redshift, Lambda, SNS, SQS, S3, Looker, DynamoDB, CircleCI, Terraform.
What you'll do:
Contribute to the design/architecture new initiatives such as real time streaming pipelines, tooling around data governance, build job orchestration abstractions to manage resources on AWS
Collaborate with the team to build tools for data science/marketing teams
Design integration pipelines for new data sources and improve existing pipelines to perform efficiently at scale
Provide technical guidance to the team
Leverage best practices in continuous integration and deployment to our cloud-based infrastructure
Optimize data access and consumption for our business and product colleagues
Who you are:
4+ years of professional experience working in data warehousing, data architecture, and/or data engineering environments, especially using spark, hadoop, hive etc with solid understanding of streaming pipelines.
At least 1+ years of experience in streaming pipeline development
Proficiency in at least one high-level programming language Scala
Good understanding of databases
You have built large-scale data products and understand the tradeoffs made when building these features
You have a deep understanding of system design, data structures, and algorithms
You have an excellent knowledge of distributed computing frameworks such as Hadoop MapReduce, Spark.
You have a strong knowledge of following AWS infrastructure - EMR, S3, Redshift
You have strong understanding of data quality, governance
You are a team player, self-driven, highly motivated individual who loves to learn new things
Gold stars:
Experience in Machine Learning infrastructure
Experience in Search Engines
#LI-MN1
#LI-REMOTE
At Stash it is our mission to help everyday Americans invest and build wealth. That includes people of all races, genders, and abilities, so it is important to us to acknowledge and address the issues of inequality in financial services head on.
Diversity and inclusion are essential to living our values, promoting innovation, and building the best products. Our success is directly related to our employees and we believe that our team should reflect the diversity of the customers that we serve. As an Equal Opportunity Employer, Stash is committed to building an inclusive environment for people of all backgrounds.
If you require any reasonable accommodations to make your application process more accessible please reach out to recruiting@stash.com.
Invest in Yourself:
Equity & Stash Accounts [Invest, Retire, Custodial, Bank]
Flexible PTO
Learning & Development Fund
Work from Home Space Stipends
Parental Leave [Primary & Secondary]
Recognition:
Comparably's Best Company for Diversity, Women, Culture, and more! (2022)
BuiltIn's Best Places to Work (2019, 2020, 2021, 2022)
Forbes Fintech 50 (2019, 2020, 2021)
Best Digital Bank, Finovate Awards (2020)
Tearsheet Challenge Awards, Best Banking Card Product - Stock-Back® Card, 2020
LendIt Fintech Innovator of the Year (2019 & 2020)
Salary Range: $135k - $202k
The base salary range represents the reasonably anticipated low and high end of the salary range for this position. Actual salaries will vary and will be based on various factors, such as the candidate's qualifications, skills, experience and competencies, as well as internal equity and alignment with market data for companies of our size and industry.
**No recruiters, please**
Show Less
Report",$1L - $2L,201 to 500 Employees,Company - Private,Finance,Investment & Asset Management,2015,Unknown / Non-Applicable
Mashvisor Inc.,Data Science Engineer,Remote,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?
Show Less
Report",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,2015,Unknown / Non-Applicable
TalentMovers,Azure Data Engineer,"Plano, TX","Title : Azure Data Engineer
Location : Plano, TX (Initially Remote)
FTE Only
Salary : on FTE
Candidate Profile / Qualification
Education: B.E. / B. Tech / MCA / MCS
A Minimum of a Bachelor’s degree in Computer Science or related software engineering discipline, or equivalent
4-6 years of experience managing Cloud Ops, Data factory, SQL Queries, Data Lake, Data Bricks etc.
Strong ETL pipeline development experience with Azure Ecosystem like Azure Data Factory. Build simple to complex pipelines, activities, Datasets & data flows.
Experience with message ingestion systems like Kafka / Azure Event Hub.
Experience with NoSQL databases and design for NoSQL (Cosmos DB, Mongo DB) data structures.
Experience in data modelling and Proficient in SQL developer skills in writing stored procedures, functions, transformations etc.
Knowledge/Experience in Python will be an added advantage.
Knowledge in Service provisioning, scripted provisioning, blueprint development for data service deployment etc.
Excellent oral and written communication skills.
Candidate should possess a strong work ethic, good interpersonal and communication skills and a high energy level
Thank You,
Saif
Associate Recruiter
TalentMovers, Inc.
Cell # (650)-844-9900
Job Type: Full-time
Salary: $94,578.54 - $171,140.71 per year
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 7033495365
Show Less
Report",$95T - $2L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
#N/A,Data Engineer - Remote,"Phoenix, AZ","At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing *your life's best work.(sm)*
**
You’ll enjoy the flexibility to work remotely from anywhere within the U.S. as you take on some tough challenges.*
Primary Responsibilities:*
*
Accountable for data engineering lifecycle including research, proof of concepts, design, development, test, deployment and maintenance
Design, develop, implement and run cross-domain, modular, optimized, flexible, scalable, secure, reliable and quality data solutions that transform data for meaningful analyses and analytics while ensuring operability
Design, develop, implement and run data solutions that improve data efficiency, reliability and quality, and are performant by design
Layer in instrumentation in the development process so that data pipelines can be monitored. Measurements are used to detect internal problems before they result into user visible outages or data quality issues
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking
Tech debt reduction/ Tech transformation including Open-source adoption, Cloud adoption, HCP assessment and adoption
Contribution to our industry community and strive to reuse and share components wherever possible across the organization
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Identifies solutions to non-standard requests and problems
Solves moderately complex problems and/or conducts moderately complex analyses
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.*
*Required Qualifications:*
*
Undergraduate degree or equivalent experience
3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python
3+ years of experience in Data Engineering, Coding ETL and building data pipelines
1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc.
1+ years of experience writing data engineering code in Databricks
Preferred Qualifications:*
*
Cloud experience (Azure/AWS/GCP)
Snowflake experience
Proficient in building relationship with stakeholder and maintaining it during the course of the project/program
Proficient in working with cross functional teams
Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.*
*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.
California, Colorado, Connecticut, Nevada, New York City, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New York City, or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Job Type: Full-time
Application Question(s):
Do you have a Bachelor’s Degree or equivalent underwriting work experience?
Do you have 3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python?
Do you have 3+ years of experience in Data Engineering, Coding ETL and buidling data pipelines?
Do you have 1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc. ?
Do you have 1+ years of experience writing data engineering code in Databricks?
Apply Now: click Easy Apply
Show Less
Report",$73T - $1L,10000+ Employees,Company - Public,Healthcare,Healthcare Services & Hospitals,1977,$10+ billion (USD)
Transcarent,Senior Data Engineer,"San Francisco, CA","Who we are
Healthcare is more confusing, more costly, and more complex than ever. Transcarent is a health and care experience company on a mission to empower Members to stay healthy by providing them with unbiased information, trusted guidance, and easy access to high value care where and when they need it. You will be part of a world-class team, supported by top tier investors like 7wireVentures and General Catalyst, and founded by a mission-driven team committed to transforming the health and care experience for all. We closed on our Series C funding in January 2022, raising our total funding to $298 million and enabling us to respond to the demand for rapid expansion of our offering.
Transcarent is committed to growing and empowering a diverse and inclusive community within our company. We believe that a team with diverse lived experiences, working together will strengthen our organization, and our ability to deliver ""not just better but different"" experiences for our members.
What we look for in our teammates
We are looking for teammates to join us in building our company, culture, and Member experience who:
Put people first, and make decisions with the Member's best interests in mind
Are active learners, constantly looking to improve and grow
Are driven by our mission to measurably improve health and care each day
Bring the energy needed to transform health and care, and move and adapt rapidly
Are laser focused on delivering results for Members, and proactively problem solving to get there
We are looking for data engineers excited to create a single source of truth that will power the decisions for the company for other developers, products, clinical teams, data analysts, data scientists, member experience, and more. They will need to be able to ingest various sorts of application and health data, leverage our Snowflake data warehouse to load it, and transform the data into various data sets to offer it self-serve in a secure, compliant manner. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

In this role, you will...
Be a data champion and seek to empower others to leverage the data to its full potential.
Create and maintain optimal data pipeline architecture with high observability and robust operational characteristics.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal data extraction, transformation, and loading using SQL, python, and dbt from various sources.
Work with stakeholders, including the Executive, Product, Clinical, Data, and Design teams, to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
About You
You are entrepreneurial and mission-driven and can present your ideas with clarity and confidence.
You are a high-agency person. You refuse to accept undue constraints and the status quo and will not rest until you figure things out.
Advanced expertise in python and dbt for data pipelines
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing big data pipelines, architectures, and data sets. A definite plus with healthcare experience
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Good to have healthcare domain experience.
Tech Stack
The following is a list of software/tools that would be nice to have but not required:
Experience with cloud-based data warehouse: Snowflake
Experience with relational SQL and NoSQL databases
Experience with object-oriented/object function scripting languages: Golang, Python, Java, C++, Scala, etc.
Experience with big data tools: Spark, Kafka, etc.
Experience with data pipeline and workflow management tools like Airflow
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Total Rewards
As a remote position, the salary range for this role is $140,000 – $170,000.
Individual compensation packages are based on a few different factors unique to each candidate, including primary work location and an evaluation of a candidate's skills, experience, market demands, and internal equity.
Salary is just one component of Transcarent's total package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock options.
Our benefits and perks programs include, but are not limited to:
Competitive medical, dental, and vision coverage
Competitive 401(k) Plan with a generous company match
Flexible Time Off/Paid Time Off, 12 paid holidays
Protection Plans including Life Insurance, Disability Insurance, and Supplemental Insurance
Mental Health and Wellness benefits
Location
You must be authorized to work in the United States. Depending on the position we may have a preference to a specific location, but are generally open to remote work anywhere in the US.
Transcarent is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. If you are a person with a disability and require assistance during the application process, please don't hesitate to reach out!
Research shows that candidates from underrepresented backgrounds often don't apply unless they meet 100% of the job criteria. While we have worked to consolidate the minimum qualifications for each role, we aren't looking for someone who checks each box on a page; we're looking for active learners and people who care about disrupting the current health and care with their unique experiences.
Show Less
Report",$1L - $2L,201 to 500 Employees,Company - Private,Healthcare,Healthcare Services & Hospitals,2020,Unknown / Non-Applicable
etrailer.com,Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L,501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
YT Global Network,Data Engineer- Remote,Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$90.00 - $120.00 Per hour,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Stratford Solutions Inc.,Senior Data Engineer,Remote,"Job title: Senior Data Engineer

Job Location: REMOTE (EST ZONE) M-F 9-5 (35/hrs a week)

Job type: 8 Month Contract

Pay Rate: $100-$125/hr

SCOPE OF SERVICES

Seeking a Data Engineer role to ensure the efficient and successful implementation and support of complex data engineering solutions for City agencies. This resource should demonstrate a solid understanding of industry-standard implementation methodologies using data engineering technologies, tools, and processes.

TASKS:
? Create and maintain optimal data pipeline architecture that is coherent and scalable, based on best practices of integrating data into a consolidated repository.

? Perform the technical design, development, and component testing of repository changes.

? Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics.

? Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, cloud, and big data technologies.

? Develop ETLs to move data securely from source to target systems.

? Create, update, and maintain system documentation.

? Develop new or build against existing APIs for data access or landing data as output for further downstream consumption in the appropriate target data store.

? Perform special projects and initiatives as assigned.

MANDATORY SKILLS/EXPERIENCE Note: Candidates who do not have the mandatory skills will not be considered

8+ years of experience in writing SQL.
8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl
DESIRABLE SKILLS/EXPERIENCE:
Hands on experience developing Microsoft PowerBI solutions.
5+ years hands-on experience in development with the suite of tools from Informatica PowerCenter and B2B Data Transformation.
Experience using Oracle 10g/11g, SQL Server and/or a database appliance.
Knowledge of metadata-driven enterprise reporting platforms.
Show Less
Report",$100.00 - $125.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Fiable Consulting,Data and Knowledge Engineer,Remote,"REQUIRED
Python, Data Pipelines/ETL, Apache NiFi, Databases, SQL, Big Data Analytics ( Snowflake, Databricks, Spark)
Desired:
Enterprise Knowledge Graph (Stardog, RDF, OWL, SPARQL)
Job Types: Full-time, Contract
Salary: From $56,153.57 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$56T,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Trf Technologies,Contract Senior Data Integration BA/Engineer,"New York, NY","JOB TITLE : Contract Senior Data Integration BA/Engineer with Financial Services Experience
MUST be local to New York City. Work 3 days in office/2 days remote
Major Responsibilities:
BIG PLUS: Experience with Axiom or other Regulatory Reporting
This position is 30-40% Business Analyst/60-70% SQL/ETL Engineer
Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.
Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
Qualifications:
10+ years - Enterprise Data Management
10+ years - SQL Server based development of large datasets
5+ years with Data Architecture
3+ years’ experience in Finance / Banking industry – some understanding of Securities and
Banking products and their data footprints.
2+ years Python coding experience
Proficient with Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
Capable of discussing enterprise level services independent of technology stack
Experience with Cloud based data architectures, messaging, analytics
Superior communication skills
Cloud certification(s)
Any experience with Regulatory Reporting is a Plus
Education:
Minimally a BA degree within an engineering and/or computer science discipline
Master’s degree strongly preferred
Job Types: Full-time, Contract
Pay: $100.00 - $120.00 per hour
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
New York, NY: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Required)
Experience:
Axiom or other Regulatory Reporting: 1 year (Required)
SQL: 1 year (Required)
Enterprise Data Management: 10 years (Required)
Finance / Banking industry: 3 years (Required)
Work Location: In person
Show Less
Report",$100.00 - $120.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
"Bluemont Technology & Research, Inc.",Data Engineer,"Norfolk, VA","NATO Data Engineer
Requirements:
Ts/sci or secret clearance
High proficiency level in English language
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets is required.
Description:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.
Job Type: Full-time
Pay: $90,000.00 - $130,000.00 per year
Experience level:
10 years
11+ years
4 years
5 years
6 years
7 years
8 years
9 years
Ability to commute/relocate:
Norfolk, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you have a secret clearance or TS/SCI?
Work Location: One location
Show Less
Report",$90T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gladly,Senior Data Engineer,"San Francisco, CA","Gladly is a Radically Personal Customer Service Platform that puts people at the center of a single, lifelong conversation. We enable companies to talk to their customers they way people talk to their friends: seamlessly across voice, email, SMS, chat, and social media.
Gladly's data products are a foundation for enabling contact center leaders to understand their team's performance and identify opportunities for their company. Because of Gladly's unique approach to customer service, the data we provide is a key differentiator, not an afterthought. Our data warehouse also gives Gladly's customer success team the insights to help customers optimize their use of the product. We create a range of metrics and datasets based on carefully designed events and data models. We are looking for a data engineer to join our small, fast-growing and high impact team.
What you'll do
Own and drive projects, as well as communicate with stakeholders on requirements, progress and delivery.
Teach. Provide technical guidance and mentorship in software engineering best practices while demonstrating these as an individual contributor.
Collaborate. Work closely with small, nimble, cross-functional teams of engineers, product managers, designers, and business teams.
Contribute. Build a best-in-class data pipeline with a few key attributes:
repeatable via infrastructure-as-code
testable, with verification of correctness
reliable and always-on
low latency (on the order of minutes)
observable.
Work with experienced colleagues who will be eager to share their knowledge, provide mentorship and help you grow your career.
Have opportunities to learn and work with technologies used at Gladly like Snowflake, dbt, Debezium, Looker, PostgreSQL, Kafka, Docker, Kubernetes, AWS, Redis, Node.js, Go, Python.
You'll be successful by
Being eager to learn Gladly's business domain and apply this knowledge in building the innovative product.
Self-organizing and prioritizing your work based on the impact to the customer.
Understanding how to balance pragmatic solutions with best practices of data engineering.
Having passion for making the most of our existing technologies and introducing the right tools for problem at hand.
Showing ownership and pride in your work by promoting data best practices and making them easy for engineering teams to adopt as well as providing ongoing maintenance and support.
We're excited about you because you have
5+ years of engineering experience including 2+ years of working with ETL pipelines, data transformation and modeling.
Strong teamwork skills. You love participating with high-performing teams of engineers.
Customer-centricity and product focus. You look at everything you create through the lens of how it improves things for the end-customer. You are comfortable communicating how various technical approaches might impact product behavior (and vice versa).
Learning mentality because nobody checks every box. You aren't intimidated by new domains or technology; you're willing to dive into documentation/videos, talk to your teammates, and experiment to become well-versed.
Willingness to work across the development stack. You're comfortable with working on data pipelines, transformations and implementing insights. You're willing to jump into Gladly backend applications on occasion.
Operational expertise. You value robust observable solutions with actionable monitoring and the importance of tooling for troubleshooting and resolving issues.
Research has shown that individuals from marginalized groups are less likely to apply to jobs where they don't meet 100% of the criteria. Gladly values diversity of experience, so if you believe you have the right skill set, we welcome you to apply - even if you don't check every box in the job description. We're committed to an inclusive workplace and would love to see if you could be the next great addition to our team.
Compensation
$156,000-$215,000 annually.
For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above.
Working at Gladly
People are not just at the heart of our product, they're at the heart of our company.
We value diverse perspectives and hire new people to enrich our mix, not keep it the same.
We believe in open communication and share in an inclusive, open culture.
We have embraced remote work and make it easy for our team to work from anywhere, but we also invest in opportunities to get the teams together in person regularly.
We learn from each other, and we help each other learn.
We provide opportunities to move between teams to learn and contribute to other cool technologies used at Gladly.
We have a strong work ethic, but value life outside of work, too.
Our focus is on people and that starts with our employees. As an employee you can count on:
Competitive salaries, stock options
Medical, Dental, Vision and Life insurance
Generous paid time off
Generous paid Parental Leave
401K
Flexible Spending Accounts
Wellness and home office stipends
Founded in 2014 by a team of repeat entrepreneurs with multiple successful exits, Gladly is reinventing customer service. By focusing on customers instead of tickets, we are disrupting a $70B market and are proud to count Crate and Barrel, Warby Parker and many other innovative brands as customers. Gladly has raised over $110M from Greylock Partners, NEA, GGV Capital, Glynn Capital and JetBlue Tech Ventures.
Gladly has made the decision to become a fully distributed company, allowing employees to live anywhere in the United States, and candidates to come from nearly any geographical region. That said, we also highly value our collaborative and creative culture and commit to meeting in real life as a company at least once per quarter when it is safe to do so.
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2014,Unknown / Non-Applicable
ASA,Data Integration Engineer(Banking),"New York, NY","Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.
Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
SKILLS
10+ years - Enterprise Data Management
10+ years - SQL Server based development of large datasets
5+ years with Data Architecture
3+ years experience in Finance / Banking industry some understanding of Securities and
Banking products and their data footprints.
2+ years Python coding experience
Proficient with Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
Capable of discussing enterprise level services independent of technology stack
Experience with Cloud based data architectures, messaging, analytics
Superior communication skills
Cloud certification(s)
Note
Any experience with Regulatory Reporting is a Plus
Job Type: Contract
Salary: $100.00 - $103.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10020: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL Server: 10 years (Preferred)
python: 2 years (Preferred)
Data management: 10 years (Preferred)
Work Location: One location
Show Less
Report",$100.00 - $103.00 Per hour,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1999,$25 to $50 million (USD)
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Plaxonic,Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Visvak Solutions,Azure Data Engineer,Remote,"JD:
Overall 7+ years of experience
has good understanding on Azure storage Gen2
hands on experience with Azure stack (minimum 5 years)
o Azure Databricks
o Azure Data Factory
o Azure DevOps
proficient coding experience using Spark(Scala/Python), T-SQL
Understanding around the services related to Azure Analytics, Azure SQL, Azure function app, logic app
prior ETL development experience using industry tool e.g. informatica/SSIS/Talend etc.
proficient in a source code control system
good to have knowledge in Kafka streaming Azure Infrastructure
Job Type: Full-time
Salary: $39.86 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: Remote
Show Less
Report",$39.86 Per hour,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Stratford Solutions Inc.,Senior Data Engineer,Remote,"Job title: Senior Data Engineer

Job Location: REMOTE (EST ZONE) M-F 9-5 (35/hrs a week)

Job type: 8 Month Contract

Pay Rate: $100-$125/hr

SCOPE OF SERVICES

Seeking a Data Engineer role to ensure the efficient and successful implementation and support of complex data engineering solutions for City agencies. This resource should demonstrate a solid understanding of industry-standard implementation methodologies using data engineering technologies, tools, and processes.

TASKS:
? Create and maintain optimal data pipeline architecture that is coherent and scalable, based on best practices of integrating data into a consolidated repository.

? Perform the technical design, development, and component testing of repository changes.

? Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics.

? Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, cloud, and big data technologies.

? Develop ETLs to move data securely from source to target systems.

? Create, update, and maintain system documentation.

? Develop new or build against existing APIs for data access or landing data as output for further downstream consumption in the appropriate target data store.

? Perform special projects and initiatives as assigned.

MANDATORY SKILLS/EXPERIENCE Note: Candidates who do not have the mandatory skills will not be considered

8+ years of experience in writing SQL.
8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl
DESIRABLE SKILLS/EXPERIENCE:
Hands on experience developing Microsoft PowerBI solutions.
5+ years hands-on experience in development with the suite of tools from Informatica PowerCenter and B2B Data Transformation.
Experience using Oracle 10g/11g, SQL Server and/or a database appliance.
Knowledge of metadata-driven enterprise reporting platforms.
Show Less
Report",$100.00 - $125.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
#N/A,Data Engineer - Remote,"Phoenix, AZ","At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing *your life's best work.(sm)*
**
You’ll enjoy the flexibility to work remotely from anywhere within the U.S. as you take on some tough challenges.*
Primary Responsibilities:*
*
Accountable for data engineering lifecycle including research, proof of concepts, design, development, test, deployment and maintenance
Design, develop, implement and run cross-domain, modular, optimized, flexible, scalable, secure, reliable and quality data solutions that transform data for meaningful analyses and analytics while ensuring operability
Design, develop, implement and run data solutions that improve data efficiency, reliability and quality, and are performant by design
Layer in instrumentation in the development process so that data pipelines can be monitored. Measurements are used to detect internal problems before they result into user visible outages or data quality issues
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking
Tech debt reduction/ Tech transformation including Open-source adoption, Cloud adoption, HCP assessment and adoption
Contribution to our industry community and strive to reuse and share components wherever possible across the organization
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Identifies solutions to non-standard requests and problems
Solves moderately complex problems and/or conducts moderately complex analyses
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.*
*Required Qualifications:*
*
Undergraduate degree or equivalent experience
3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python
3+ years of experience in Data Engineering, Coding ETL and building data pipelines
1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc.
1+ years of experience writing data engineering code in Databricks
Preferred Qualifications:*
*
Cloud experience (Azure/AWS/GCP)
Snowflake experience
Proficient in building relationship with stakeholder and maintaining it during the course of the project/program
Proficient in working with cross functional teams
Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.*
*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.
California, Colorado, Connecticut, Nevada, New York City, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New York City, or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Job Type: Full-time
Application Question(s):
Do you have a Bachelor’s Degree or equivalent underwriting work experience?
Do you have 3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python?
Do you have 3+ years of experience in Data Engineering, Coding ETL and buidling data pipelines?
Do you have 1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc. ?
Do you have 1+ years of experience writing data engineering code in Databricks?
Apply Now: click Easy Apply
Show Less
Report",$73T - $1L,10000+ Employees,Company - Public,Healthcare,Healthcare Services & Hospitals,1977,$10+ billion (USD)
Synchrony Corp,Senior Data Engineer,Remote,"Job Title : Senior Data Engineer
Duration : Contract on W2
Location : Remote
This is a W2 opportunity.
IV Process: EQP & Technical IV with client
Tech (Qualification) Notes:
Skillset (top 3-5)
Must haves:
Python: Must be VERY strong here & able to write strong code. Doing OOP & designing pipeline
SQL: Writing advanced queries
Cloud: GCP or Azure
Knowledge of DevOps (CI/CD, Jenkins etc)
Database experience: Ideally Snowflake or Redshift
Scripting: Shell or Bash
Data Pipelines & Ingestions
Nice to haves:
Kafka is a HUGE nice to have. This will get use a quick win
Official JD:
Strong in SQL and Python, 2+ years with both (preferably)
Experience building automated data pipelines
Experience performing data analysis and data exploration
Experience working in an agile delivery environment
Strong critical thinking, communication, and problem solving skills
Prefered:
Previous healthcare experience and domain knowledge
Exposure/understanding DevOps best practice and CICD (i.e. Jenkins)
Exposure/understanding of containerization (i.e. Kubernetes, Docker)
Job Type: Contract
Salary: From $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Data Engineer: 10 years (Required)
SQL: 6 years (Required)
Python: 6 years (Required)
Work Location: Remote
Show Less
Report",$70.00 Per hour,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
ConnectiveRx,Sr. Data Engineer,"Hanover, NJ","ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.
Show Less
Report",$96T - $1L,1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable
YT Global Network,Data Engineer- Remote,Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$90.00 - $120.00 Per hour,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
AgileEngine,Senior Big Data Engineer,Remote,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Analyze, develop and implement database designs, data models and logical data specifications
Review and improve existing databases/data warehouse designs
Contribute to systems architecture analysis and designProvide comprehensive consultation to infrastructure administrators and business analysts in resolving issues
Perform data modeling studies and develop detailed data models
Work with Business Analysts and staff to establish and maintain consistent data element definitions
Participate in the development and maintenance of corporate data architecture, data management standards and conventions, data dictionaries and data element naming standards
Research and evaluate alternative solutions and recommend the most efficient and cost effective data related solutions for improved data integrity
Performance tune ETL jobs and data models
Migrate ETL code from development to production environments
Assist in the design and development of BI dashboards
Perform DW training as needed
Must haves
Experience in AWS cloud services.
Working in big data projects with 4+ years of experience
Strong working experience Apache Spark
Good experience programming language Scala or Java or both
The benefits of joining us
Professional growth: Accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: We match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: Join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: Tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote
Show Less
Report",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
infinity quest,DATA ENGINEER,"Seattle, WA","At least 3 years of Data Engineer experience is required preferably in a cloud Environment.
You should have at least 4 years of coding experience in python/java/ Scala and open source packages with at least 2 years of experience with Databases(SQL/NOSQL etc).
Experience with large scale Distributed databases like redshift/Snowflake is a big Plus.
You should have Experience with different aspects of data systems including database design, data modeling, performance optimization, SQL etc.
Some Experience with building data pipelines and Orchestration(Airflow ,ADF,glue etc) is required.
Strong communication skills (able to explain concepts to non-technical audiences as well as peers)
Self-starter who is highly organized, communicative, quick learner, and team-oriented
Technology Requirements:
Python/Java or Scala , SQL and Airflow. Cloud experience AWS/Azure
Daily tasks:
Developing, executing, monitoring and troubleshooting Data pipelines and workflows in our cloud environment.
Work on Data Lake/DW/DQ and other framework related items
Team and cross functional collaboration as needed.
Preferred background/prior work experience:
3 years of DE expertise building data pipelines and working in a DW/Data lake Cloud based environment
Job Type: Contract
Salary: $65.00 per hour
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
Day shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road
Show Less
Report",$65.00 Per hour,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
"Bluemont Technology & Research, Inc.",Data Engineer,"Norfolk, VA","NATO Data Engineer
Requirements:
Ts/sci or secret clearance
High proficiency level in English language
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets is required.
Description:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.
Job Type: Full-time
Pay: $90,000.00 - $130,000.00 per year
Experience level:
10 years
11+ years
4 years
5 years
6 years
7 years
8 years
9 years
Ability to commute/relocate:
Norfolk, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you have a secret clearance or TS/SCI?
Work Location: One location
Show Less
Report",$90T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Visvak Solutions,Azure Data Engineer,Remote,"JD:
Overall 7+ years of experience
has good understanding on Azure storage Gen2
hands on experience with Azure stack (minimum 5 years)
o Azure Databricks
o Azure Data Factory
o Azure DevOps
proficient coding experience using Spark(Scala/Python), T-SQL
Understanding around the services related to Azure Analytics, Azure SQL, Azure function app, logic app
prior ETL development experience using industry tool e.g. informatica/SSIS/Talend etc.
proficient in a source code control system
good to have knowledge in Kafka streaming Azure Infrastructure
Job Type: Full-time
Salary: $39.86 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: Remote
Show Less
Report",$39.86 Per hour,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Humu,Senior Data Engineer,"Mountain View, CA","What you'll get to work on
Humu's Nudge Engine® deploys thousands of customized nudges—small, personal steps—throughout organizations to empower every employee, manager, team, and leader as a change agent. Over time, our nudges grow increasingly aware of the timing, messaging, and motivational techniques that inspire individual employees towards action.
As a member of Humu's Data Engineering team, you will be responsible for building and maintaining systems focused on expanding and optimizing data pipelines to compute insights and analyses at scale. Some of the team's major ownership areas include:
A data pipeline tool for helping transform ingested HR data into a consistent and clean format.
The automated processes for deciding which nudges to send to users, scheduling them for delivery, and delivering them through a multitude of formats (email, text messages, etc).
The logic and systems to make data available to a variety of cross functional teams at Humu.

Where you fit in
We are committed to change the working world for the better by bringing greater meaning and happiness into everyone's working lives, everywhere. We are passionate about our mission, and excited to grow our school of fish with people who want to do the same - and people who will bring in their different perspectives to help us continue to shape our team and product. If this is you, we encourage you swim into our candidate pool!

The details
Role and responsibilities:
As a member of our Product Team, you will ensure optimal data delivery architecture is consistent throughout ongoing projects.
Create and optimize our data pipeline architecture
Build data access platform for our data science and tech teams
Manage and optimize customer ingestions for the Humu product
Add and improve logging and monitoring
Willingness to occasionally implement UI for internal data tools.
Qualifications:
4 - 99 years of experience managing data pipelines
Driven engineer that is motivated to build a great product and great codebase in a fast-paced environment
Strong communication skills with a growth and learning mindset
Understanding of object-oriented languages (Java, Python, etc.)
Familiarity with cloud based platforms - GCP (Preferred), AWS, Azure
Experience with large-scale databases (preference towards non-relational unstructured databases).
Usage or understanding of complex data pipelines.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.

Salary range: $160,000 - $220,000
Only open to candidates in Seattle, WA or SF Bay Area, CA
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017,Unknown / Non-Applicable
Stratford Solutions Inc.,Senior Data Engineer,Remote,"Job title: Senior Data Engineer

Job Location: REMOTE (EST ZONE) M-F 9-5 (35/hrs a week)

Job type: 8 Month Contract

Pay Rate: $100-$125/hr

SCOPE OF SERVICES

Seeking a Data Engineer role to ensure the efficient and successful implementation and support of complex data engineering solutions for City agencies. This resource should demonstrate a solid understanding of industry-standard implementation methodologies using data engineering technologies, tools, and processes.

TASKS:
? Create and maintain optimal data pipeline architecture that is coherent and scalable, based on best practices of integrating data into a consolidated repository.

? Perform the technical design, development, and component testing of repository changes.

? Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics.

? Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, cloud, and big data technologies.

? Develop ETLs to move data securely from source to target systems.

? Create, update, and maintain system documentation.

? Develop new or build against existing APIs for data access or landing data as output for further downstream consumption in the appropriate target data store.

? Perform special projects and initiatives as assigned.

MANDATORY SKILLS/EXPERIENCE Note: Candidates who do not have the mandatory skills will not be considered

8+ years of experience in writing SQL.
8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl
DESIRABLE SKILLS/EXPERIENCE:
Hands on experience developing Microsoft PowerBI solutions.
5+ years hands-on experience in development with the suite of tools from Informatica PowerCenter and B2B Data Transformation.
Experience using Oracle 10g/11g, SQL Server and/or a database appliance.
Knowledge of metadata-driven enterprise reporting platforms.
Show Less
Report",$100.00 - $125.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Talent Group,Sr. Data Engineer- BHS,Remote,"Job Description :
Must have Behavioral Health Services Experience .
5+ Yearsof experience building data warehouses and data marts On-prem and on Cloud infrastructure.
Hands-on experience with data analysis / profiling and SQL Scripting.
Experience FSE with Java & Angular, 3-5 years’ experience.
Job Type: Full-time
Salary: Up to $110,000.00 per year
Benefits:
401(k)
401(k) matching
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L,1 to 50 Employees,Company - Public,Media & Communication,Broadcast Media,#N/A,Less than $1 million (USD)
I28 Technologies,Jr.Data Engineer - Python / Hadoop / PySpark,"Honolulu, HI","Banking domain knowledge
knowledge in software development and delivery.
knowledge with Neo4j, GraphQL
Good aptitude, knowledge problem-solving abilities, and analytical skills, ability to take ownership as appropriate
Should be able to do coding, debugging, performance tuning, and deploying the apps to the Production environment.
knowledge working in Agile Methodology
Ability to learn and help the team learn new technologies quickly
Take up the complete planning design implementation/ UAT delivery of the project
knowledge communication and coordination skills
Job Type: Full-time
Salary: $55,000.00 - $60,000.00 per year
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: One location
Show Less
Report",$55T - $60T,1 to 50 Employees,Company - Private,Finance,Accounting & Tax,#N/A,Less than $1 million (USD)
Freemind solutions,Big Data Engineer with Spark and Python,Remote,"Required Skillset
· 5-10 years of experience as a Big Data Developer
· In-depth knowledge of Big Data technologies - Spark, HDFS, Hive, Kudu, Impala · Solid programming experience in Python
· Production experience in core Hadoop technologies including HDFS, Hive and YARN
· Strong working knowledge of SQL and the ability to write, debug, and optimize distributed SQL queries
· Excellent communication skills; previous experience working with internal or external customers
· Strong analytical abilities; ability to translate business requirements and use cases into a Hadoop solution, including ingestion of many data sources, ETL processing, data access, and consumption, as well as custom analytics
· Effective analysis of new and existing applications and platforms
· Experience working with Data Governance tools like Apache Sentry, Kerberos, Atlas, Ranger
· Experience working with streaming data with technologies like Kafka, Spark streaming
· Strong understanding of big data performance tuning
· Experience handling different kinds of structured and unstructured data formats (Parquet/Delta Lake/Avro/XML/JSON/YAML/CSV/Zip/Xlsx/Text etc.)
· Well versed with Software Development Life Cycle Methodologies and Practices · Clear communication and documentation of technical specifications
· Spark Certification is a huge plus Responsibilities
· Integrate data from a variety of data sources (data warehouse, data marts) utilizing on-prem or cloud-based data structures (Azure/AWS); determine new and existing data sources
· Develop, implement and optimize streaming, data lake, and analytics big data solutions
· Create and execute testing strategies including unit, integration, and full end-to-end tests of data pipelines
· Recommend Kudu, HBase, HDFS, and relational databases based on their strengths
· Utilize ETL processes to build data repositories; integrate data into Hadoop data lake using Sqoop (batch ingest), Kafka (streaming), Spark, Hive or Impala (transformation)
· Adapt and learn new technologies in a quickly changing field
· Be creative; evaluate and recommend big data technologies to solve problems and create solutions Recommend and implement best tools to ensure optimized data performance; perform Data Analysis utilizing Spark, Hive, and Impala
Job Type: Contract
Job Type: Part-time
Pay: $60.00 - $65.00 per hour
Experience:
spark: 5 years (Preferred)
python: 5 years (Preferred)
databricks: 3 years (Preferred)
Work Location: Remote
Show Less
Report",$60.00 - $65.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Synchrony Corp,Senior Data Engineer,Remote,"Job Title : Senior Data Engineer
Duration : Contract on W2
Location : Remote
This is a W2 opportunity.
IV Process: EQP & Technical IV with client
Tech (Qualification) Notes:
Skillset (top 3-5)
Must haves:
Python: Must be VERY strong here & able to write strong code. Doing OOP & designing pipeline
SQL: Writing advanced queries
Cloud: GCP or Azure
Knowledge of DevOps (CI/CD, Jenkins etc)
Database experience: Ideally Snowflake or Redshift
Scripting: Shell or Bash
Data Pipelines & Ingestions
Nice to haves:
Kafka is a HUGE nice to have. This will get use a quick win
Official JD:
Strong in SQL and Python, 2+ years with both (preferably)
Experience building automated data pipelines
Experience performing data analysis and data exploration
Experience working in an agile delivery environment
Strong critical thinking, communication, and problem solving skills
Prefered:
Previous healthcare experience and domain knowledge
Exposure/understanding DevOps best practice and CICD (i.e. Jenkins)
Exposure/understanding of containerization (i.e. Kubernetes, Docker)
Job Type: Contract
Salary: From $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Data Engineer: 10 years (Required)
SQL: 6 years (Required)
Python: 6 years (Required)
Work Location: Remote
Show Less
Report",$70.00 Per hour,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
etrailer.com,Data Engineer/Data Scientist,Remote,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow
Show Less
Report",$1L - $2L,501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
Pura,Data Engineer (Mid/Jr),"Pleasant Grove, UT","Data Engineer (Mid/Jr Level)
Pura has been revolutionizing the smart home experience for the past several years. We obsess over providing world-class experiences for our customers, partners, and vendors. We pride ourselves on maintaining a high standard of quality and innovation with our products, and continuous growth and development for our people.
We are looking for a Data Engineer to help business users and analysts throughout the organization access the data they need to operate and grow the business.
What you’ll own:
In this high-impact role, you will:
Work closely with the Data Science team to design and develop scalable data pipelines for processing and analyzing large volumes of data
Build and maintain ETL processes using Python, SQL, Apache Airflow, and other technologies
Develop and deploy data processing jobs on AWS or GCP using Docker and Kubernetes
Write API wrappers to integrate with various external data sources and third-party tools
Implement and maintain best practices for data security, data quality, and data governance
Collaborate with other cross-functional teams to ensure data is available, reliable, and accessible to support business decisions
Write clean, readable, and maintainable code and ensure code is thoroughly tested and documented

Qualifications:
Bachelor's degree in Computer Science, Software Engineering, or related field
1-3 years of experience in data engineering or a related field
Proficiency in Python, SQL, Apache Airflow, and Docker
Experience with AWS or GCP and some Kubernetes experience
Strong analytical and problem-solving skills
Excellent communication and collaboration skills
Ability to work independently and as part of a team
Passion for writing clean, readable code and ensuring code quality

If you are passionate about data engineering and want to join a fast-paced, dynamic team that is making a real impact, we encourage you to apply today!.
Pura’s Story
At Pura, we’re pairing smart tech with premium fragrance to create a perfectly personalized and customized scenting experience for the individual. We partner with brands like Disney, Capri Blue, and Anthropologie to bring original and well-loved fragrances to homes in a modern, convenient, and safe way. We know we’ve only just begun to unlock the possibility of scent, and we’re excited for the opportunities that lie ahead.
We’re quickly turning heads and getting noticed. We raised a seed round of 4.4M in February of 2020, was recognized by Inc. Magazine as a 2021 Best Workplace, won the Silicon Slopes Hall of Fame & Awards Advertising category in 2022, and we’re currently the 6th-fastest growing company in Utah. Check out our Instagram @pura and TikTok @trypura channels for a look into the excited, engaged community we’re building. We pride ourselves on being a human brand and in creating a culture worth talking about, and we have big goals for the future.
Join the Pura Team!
All candidates are subject to a background check.
Pura provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

PSRu221guI
Show Less
Report",$39T - $58T,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gladly,Senior Data Engineer,"San Francisco, CA","Gladly is a Radically Personal Customer Service Platform that puts people at the center of a single, lifelong conversation. We enable companies to talk to their customers they way people talk to their friends: seamlessly across voice, email, SMS, chat, and social media.
Gladly's data products are a foundation for enabling contact center leaders to understand their team's performance and identify opportunities for their company. Because of Gladly's unique approach to customer service, the data we provide is a key differentiator, not an afterthought. Our data warehouse also gives Gladly's customer success team the insights to help customers optimize their use of the product. We create a range of metrics and datasets based on carefully designed events and data models. We are looking for a data engineer to join our small, fast-growing and high impact team.
What you'll do
Own and drive projects, as well as communicate with stakeholders on requirements, progress and delivery.
Teach. Provide technical guidance and mentorship in software engineering best practices while demonstrating these as an individual contributor.
Collaborate. Work closely with small, nimble, cross-functional teams of engineers, product managers, designers, and business teams.
Contribute. Build a best-in-class data pipeline with a few key attributes:
repeatable via infrastructure-as-code
testable, with verification of correctness
reliable and always-on
low latency (on the order of minutes)
observable.
Work with experienced colleagues who will be eager to share their knowledge, provide mentorship and help you grow your career.
Have opportunities to learn and work with technologies used at Gladly like Snowflake, dbt, Debezium, Looker, PostgreSQL, Kafka, Docker, Kubernetes, AWS, Redis, Node.js, Go, Python.
You'll be successful by
Being eager to learn Gladly's business domain and apply this knowledge in building the innovative product.
Self-organizing and prioritizing your work based on the impact to the customer.
Understanding how to balance pragmatic solutions with best practices of data engineering.
Having passion for making the most of our existing technologies and introducing the right tools for problem at hand.
Showing ownership and pride in your work by promoting data best practices and making them easy for engineering teams to adopt as well as providing ongoing maintenance and support.
We're excited about you because you have
5+ years of engineering experience including 2+ years of working with ETL pipelines, data transformation and modeling.
Strong teamwork skills. You love participating with high-performing teams of engineers.
Customer-centricity and product focus. You look at everything you create through the lens of how it improves things for the end-customer. You are comfortable communicating how various technical approaches might impact product behavior (and vice versa).
Learning mentality because nobody checks every box. You aren't intimidated by new domains or technology; you're willing to dive into documentation/videos, talk to your teammates, and experiment to become well-versed.
Willingness to work across the development stack. You're comfortable with working on data pipelines, transformations and implementing insights. You're willing to jump into Gladly backend applications on occasion.
Operational expertise. You value robust observable solutions with actionable monitoring and the importance of tooling for troubleshooting and resolving issues.
Research has shown that individuals from marginalized groups are less likely to apply to jobs where they don't meet 100% of the criteria. Gladly values diversity of experience, so if you believe you have the right skill set, we welcome you to apply - even if you don't check every box in the job description. We're committed to an inclusive workplace and would love to see if you could be the next great addition to our team.
Compensation
$156,000-$215,000 annually.
For cash compensation, we set standard ranges for all U.S.-based roles based on function, level, and geographic location, benchmarked against similar stage growth companies. In order to be compliant with local legislation, as well as to provide greater transparency to candidates, we share salary ranges on all job postings regardless of desired hiring location. Final offer amounts are determined by multiple factors, including geographic location as well as candidate experience and expertise, and may vary from the amounts listed above.
Working at Gladly
People are not just at the heart of our product, they're at the heart of our company.
We value diverse perspectives and hire new people to enrich our mix, not keep it the same.
We believe in open communication and share in an inclusive, open culture.
We have embraced remote work and make it easy for our team to work from anywhere, but we also invest in opportunities to get the teams together in person regularly.
We learn from each other, and we help each other learn.
We provide opportunities to move between teams to learn and contribute to other cool technologies used at Gladly.
We have a strong work ethic, but value life outside of work, too.
Our focus is on people and that starts with our employees. As an employee you can count on:
Competitive salaries, stock options
Medical, Dental, Vision and Life insurance
Generous paid time off
Generous paid Parental Leave
401K
Flexible Spending Accounts
Wellness and home office stipends
Founded in 2014 by a team of repeat entrepreneurs with multiple successful exits, Gladly is reinventing customer service. By focusing on customers instead of tickets, we are disrupting a $70B market and are proud to count Crate and Barrel, Warby Parker and many other innovative brands as customers. Gladly has raised over $110M from Greylock Partners, NEA, GGV Capital, Glynn Capital and JetBlue Tech Ventures.
Gladly has made the decision to become a fully distributed company, allowing employees to live anywhere in the United States, and candidates to come from nearly any geographical region. That said, we also highly value our collaborative and creative culture and commit to meeting in real life as a company at least once per quarter when it is safe to do so.
Show Less
Report",$2L - $2L,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2014,Unknown / Non-Applicable
YT Global Network,Data Engineer- Remote,Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$90.00 - $120.00 Per hour,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Jacobs Levy Equity Management,Quantitative Data Engineer,"Florham Park, NJ","This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer
Show Less
Report",$88T - $1L,1 to 50 Employees,Company - Private,Finance,Investment & Asset Management,#N/A,$5 to $25 million (USD)
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
infinity quest,DATA ENGINEER,"Seattle, WA","At least 3 years of Data Engineer experience is required preferably in a cloud Environment.
You should have at least 4 years of coding experience in python/java/ Scala and open source packages with at least 2 years of experience with Databases(SQL/NOSQL etc).
Experience with large scale Distributed databases like redshift/Snowflake is a big Plus.
You should have Experience with different aspects of data systems including database design, data modeling, performance optimization, SQL etc.
Some Experience with building data pipelines and Orchestration(Airflow ,ADF,glue etc) is required.
Strong communication skills (able to explain concepts to non-technical audiences as well as peers)
Self-starter who is highly organized, communicative, quick learner, and team-oriented
Technology Requirements:
Python/Java or Scala , SQL and Airflow. Cloud experience AWS/Azure
Daily tasks:
Developing, executing, monitoring and troubleshooting Data pipelines and workflows in our cloud environment.
Work on Data Lake/DW/DQ and other framework related items
Team and cross functional collaboration as needed.
Preferred background/prior work experience:
3 years of DE expertise building data pipelines and working in a DW/Data lake Cloud based environment
Job Type: Contract
Salary: $65.00 per hour
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
Day shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road
Show Less
Report",$65.00 Per hour,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Violet Ink,Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
Xiar tech inc,Senior Data Engineer,"Dallas, TX","· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location
Show Less
Report",$42.96 - $60.84 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Konnectingtree,Data Engineer,Remote,"Greetings from KonnectingTree!
We are looking for a Data Engineer for one of our clients. This is a remote position with an Implementation partner. AWS Certification Mandatory.
Data Engineer with AWS Experience
Experience with PySpark/Spark
Experience in Python
Able to work independently
Able to work with the business team directly
Interested candidates kindly share your updated resume with mythili.saravanan@konenctingtree.com. Please reach me at 952-679-2916.
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Required)
Python: 5 years (Required)
PySpark: 5 years (Required)
Work Location: Remote
Speak with the employer
+91 952-679-2916
Show Less
Report",$45.00 - $50.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Visvak Solutions,Azure Data Engineer,Remote,"JD:
Overall 7+ years of experience
has good understanding on Azure storage Gen2
hands on experience with Azure stack (minimum 5 years)
o Azure Databricks
o Azure Data Factory
o Azure DevOps
proficient coding experience using Spark(Scala/Python), T-SQL
Understanding around the services related to Azure Analytics, Azure SQL, Azure function app, logic app
prior ETL development experience using industry tool e.g. informatica/SSIS/Talend etc.
proficient in a source code control system
good to have knowledge in Kafka streaming Azure Infrastructure
Job Type: Full-time
Salary: $39.86 per hour
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: Remote
Show Less
Report",$39.86 Per hour,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Plaxonic,Azure Data Engineer,"Louisville, KY","Experience in developing applications on Microsoft Azure Platform using Features like Cloud Services, Web Role, Worker Role, Azure Web App, Azure API App, Azure Storage, Azure SQL, Azure Functions etc - Experience with Micro-services architecture - Experience in deploying Micro-services in Azure Service fabric and AKS - Hands-on experience in Databases like MS SQL and No SQL Databases - Responsible for developing application and services for and using Azure Cloud Services - Responsible for taking Technology decisions for the project - Understand business requirements and technical limitations - Participating in the complete development life cycle - Coded Unit testing achieving respective unit test coverageTalent
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Azure: 8 years (Required)
Azure Logic Apps: 5 years (Required)
Work Location: On the road
Show Less
Report",$55.00 - $60.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Stratford Solutions Inc.,Senior Data Engineer,Remote,"Job title: Senior Data Engineer

Job Location: REMOTE (EST ZONE) M-F 9-5 (35/hrs a week)

Job type: 8 Month Contract

Pay Rate: $100-$125/hr

SCOPE OF SERVICES

Seeking a Data Engineer role to ensure the efficient and successful implementation and support of complex data engineering solutions for City agencies. This resource should demonstrate a solid understanding of industry-standard implementation methodologies using data engineering technologies, tools, and processes.

TASKS:
? Create and maintain optimal data pipeline architecture that is coherent and scalable, based on best practices of integrating data into a consolidated repository.

? Perform the technical design, development, and component testing of repository changes.

? Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics.

? Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, cloud, and big data technologies.

? Develop ETLs to move data securely from source to target systems.

? Create, update, and maintain system documentation.

? Develop new or build against existing APIs for data access or landing data as output for further downstream consumption in the appropriate target data store.

? Perform special projects and initiatives as assigned.

MANDATORY SKILLS/EXPERIENCE Note: Candidates who do not have the mandatory skills will not be considered

8+ years of experience in writing SQL.
8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl
DESIRABLE SKILLS/EXPERIENCE:
Hands on experience developing Microsoft PowerBI solutions.
5+ years hands-on experience in development with the suite of tools from Informatica PowerCenter and B2B Data Transformation.
Experience using Oracle 10g/11g, SQL Server and/or a database appliance.
Knowledge of metadata-driven enterprise reporting platforms.
Show Less
Report",$100.00 - $125.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Stash,Senior Data Engineer,"New York, NY","Want to help everyday Americans build wealth? Financial inequality is increasing and too many people are getting left behind. At Stash, we believe in the power of simplifying investing, making it easy and affordable for everyday Americans to build wealth and achieve their financial goals.
We're one of the fastest growing fintechs in the U.S. and have had another record-breaking year. In 2021 we almost doubled our headcount and valuation. Our personal finance app makes investing easy and affordable; this year 6 million customers set aside more than $3 billion with Stash.
Prioritizing People is one of our core values and has been key to a healthy work-life balance and a great sense of fulfillment and inclusion. We employ a true people first - hybrid model. Live and work where you feel the most productive, whether that is in our home, in an office, or a combination of both. Anywhere in the US or UK.
Let's solve complex problems and tackle wealth inequality.
We look for people who will help raise the bar for our entire engineering organization in terms of tech prowess, passion for collaboration and desire to mentor and educate fellow team members. We look for strategic thinkers and creative problem solvers with a bias for execution and we'll expect you to contribute code as well as product/feature ideas from the get-go.
Our team has built an amazing modern data platform and we would like to add many advancements such as real time streaming, many tools around data governance. As a Data Engineer, you will be responsible for enhancing our data infrastructure to take it to the next level, in collaboration with the team members. You will also be an active contributor in the ongoing maintenance of the existing pipelines. Stash is a data-driven organization and data infrastructure is a critical part of our overall infrastructure. You will have the opportunity to make an impact in the companies' OKRs by coordinating with data science, marketing teams and backend teams by aligning with their data needs. We work with the latest technologies in the big data space and are seeking folks who would like to do the same.
Tech stack (evolving):
Spark, Scala, Python, Kafka, AWS EMR, Hive, Redshift, Lambda, SNS, SQS, S3, Looker, DynamoDB, CircleCI, Terraform.
What you'll do:
Contribute to the design/architecture new initiatives such as real time streaming pipelines, tooling around data governance, build job orchestration abstractions to manage resources on AWS
Collaborate with the team to build tools for data science/marketing teams
Design integration pipelines for new data sources and improve existing pipelines to perform efficiently at scale
Provide technical guidance to the team
Leverage best practices in continuous integration and deployment to our cloud-based infrastructure
Optimize data access and consumption for our business and product colleagues
Who you are:
4+ years of professional experience working in data warehousing, data architecture, and/or data engineering environments, especially using spark, hadoop, hive etc with solid understanding of streaming pipelines.
At least 1+ years of experience in streaming pipeline development
Proficiency in at least one high-level programming language Scala
Good understanding of databases
You have built large-scale data products and understand the tradeoffs made when building these features
You have a deep understanding of system design, data structures, and algorithms
You have an excellent knowledge of distributed computing frameworks such as Hadoop MapReduce, Spark.
You have a strong knowledge of following AWS infrastructure - EMR, S3, Redshift
You have strong understanding of data quality, governance
You are a team player, self-driven, highly motivated individual who loves to learn new things
Gold stars:
Experience in Machine Learning infrastructure
Experience in Search Engines
#LI-MN1
#LI-REMOTE
At Stash it is our mission to help everyday Americans invest and build wealth. That includes people of all races, genders, and abilities, so it is important to us to acknowledge and address the issues of inequality in financial services head on.
Diversity and inclusion are essential to living our values, promoting innovation, and building the best products. Our success is directly related to our employees and we believe that our team should reflect the diversity of the customers that we serve. As an Equal Opportunity Employer, Stash is committed to building an inclusive environment for people of all backgrounds.
If you require any reasonable accommodations to make your application process more accessible please reach out to recruiting@stash.com.
Invest in Yourself:
Equity & Stash Accounts [Invest, Retire, Custodial, Bank]
Flexible PTO
Learning & Development Fund
Work from Home Space Stipends
Parental Leave [Primary & Secondary]
Recognition:
Comparably's Best Company for Diversity, Women, Culture, and more! (2022)
BuiltIn's Best Places to Work (2019, 2020, 2021, 2022)
Forbes Fintech 50 (2019, 2020, 2021)
Best Digital Bank, Finovate Awards (2020)
Tearsheet Challenge Awards, Best Banking Card Product - Stock-Back® Card, 2020
LendIt Fintech Innovator of the Year (2019 & 2020)
Salary Range: $135k - $202k
The base salary range represents the reasonably anticipated low and high end of the salary range for this position. Actual salaries will vary and will be based on various factors, such as the candidate's qualifications, skills, experience and competencies, as well as internal equity and alignment with market data for companies of our size and industry.
**No recruiters, please**
Show Less
Report",$1L - $2L,201 to 500 Employees,Company - Private,Finance,Investment & Asset Management,2015,Unknown / Non-Applicable
Freemind solutions,Big Data Engineer with Spark and Python,Remote,"Required Skillset
· 5-10 years of experience as a Big Data Developer
· In-depth knowledge of Big Data technologies - Spark, HDFS, Hive, Kudu, Impala · Solid programming experience in Python
· Production experience in core Hadoop technologies including HDFS, Hive and YARN
· Strong working knowledge of SQL and the ability to write, debug, and optimize distributed SQL queries
· Excellent communication skills; previous experience working with internal or external customers
· Strong analytical abilities; ability to translate business requirements and use cases into a Hadoop solution, including ingestion of many data sources, ETL processing, data access, and consumption, as well as custom analytics
· Effective analysis of new and existing applications and platforms
· Experience working with Data Governance tools like Apache Sentry, Kerberos, Atlas, Ranger
· Experience working with streaming data with technologies like Kafka, Spark streaming
· Strong understanding of big data performance tuning
· Experience handling different kinds of structured and unstructured data formats (Parquet/Delta Lake/Avro/XML/JSON/YAML/CSV/Zip/Xlsx/Text etc.)
· Well versed with Software Development Life Cycle Methodologies and Practices · Clear communication and documentation of technical specifications
· Spark Certification is a huge plus Responsibilities
· Integrate data from a variety of data sources (data warehouse, data marts) utilizing on-prem or cloud-based data structures (Azure/AWS); determine new and existing data sources
· Develop, implement and optimize streaming, data lake, and analytics big data solutions
· Create and execute testing strategies including unit, integration, and full end-to-end tests of data pipelines
· Recommend Kudu, HBase, HDFS, and relational databases based on their strengths
· Utilize ETL processes to build data repositories; integrate data into Hadoop data lake using Sqoop (batch ingest), Kafka (streaming), Spark, Hive or Impala (transformation)
· Adapt and learn new technologies in a quickly changing field
· Be creative; evaluate and recommend big data technologies to solve problems and create solutions Recommend and implement best tools to ensure optimized data performance; perform Data Analysis utilizing Spark, Hive, and Impala
Job Type: Contract
Job Type: Part-time
Pay: $60.00 - $65.00 per hour
Experience:
spark: 5 years (Preferred)
python: 5 years (Preferred)
databricks: 3 years (Preferred)
Work Location: Remote
Show Less
Report",$60.00 - $65.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Edrstaffing,Python Data Engineer,"Boston, MA","Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
Show Less
Report",$90T - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Titan Healthcare Management Solutions,Data Engineer,Remote,"Titan Health is currently hiring for a Data Engineer.
Under the general direction of the Technology Solutions Manager, the Data Engineer is responsible for implementation, configuration, maintenance, and performance of business-critical data infrastructure to deliver data enablement at scale, and power our revenue cycle applications. This role will work with enterprise and client leaders to translate business and functional requirements into technical specifications and solutions within the data architecture strategy.
Essential Job Duties/Responsibilities
Implement efficient and scalable pipelines integrating data from multiple sources to common data models.
Convert raw data into usable information for client and enterprise organizations.
Within an Agile team design, develop, test, implement, and support technical solutions that support full-stack development tools and technologies.
Support data science, data enrichment, research, and data analysis as well as making data operationally able to be consumed by products and services.
Collaborate with product managers, software engineers, security & compliance, and data scientists to enable them with robust data delivery solutions that drive powerful experiences.
Identify and debug issues with code and suggest changes and/or improvements.
Perform unit tests and conduct reviews with the team to ensure code is rigorously designed, elegantly coded, and effectively tuned for performance.
Utilize available technologies to collect and map data to find cost savings and optimization opportunities.
Support and drive a proactive culture of security and compliance.
Bring an agile and engineering mindset to address complex problems, identify opportunities and craft creative solutions.
Leverage best practice coding and engineering standards to support growth and flexibility.
Coordinate with teams across the organization to address incident, change and release management needs/requirements.
Provide input to risk management; report risks as they are identified and participate in prioritization/follow up.
Stay current with emerging technologies and advancements within existing technologies.
Positively and deliberately engage with colleagues – external and internal – to foster collaborative and productive relationships.
Cultivate great teams and lead in alignment with Titan values.
Comply with and hold with utmost regard all compliance requirements to protect patient privacy and confidentiality.
Stay curious, kind and contribute positively to the Titan culture.
Minimum Qualifications
Bachelor’s Degree in Information Technology, Computer Science, Mathematics or related field is preferred but not required.
Experience with Agile methodologies.
Experience working in HIPAA, HITRUST or other advanced compliance environments.
Experience leading the lifecycle management and integrations of enterprise data.
Ability to clearly articulate technology concepts to business leaders and engineers.
Strong understanding of IT Service Management practices.
Strong analytical, problem-solving, and critical thinking skills with excellent attention to detail.
Direct experience with Azure SQL.
Experience with Azure Data Factory preferred.
Prior experience in the facilitating conversations to translate business requirements into the technical requirements needed to develop solutions.
Excellent oral and written communication skills.
Comprehensive knowledge of Microsoft Office applications.
Knowledge of Hospital CMS or billing data structures preferred.
Titan Health offers a robust Health and Welfare benefits program, along with Paid Time Off, 401k plan with company match, and remote working environment.
H9CslyVeWp
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gopuff,Principal Data Engineer,"Independence, KS","Gopuff is seeking a Principal Data Engineer to join its Data Engineering team. This individual will play a major role in shaping the team’s technical direction, designing and implementing the data architecture to enable analytics, data science, and machine learning at scale. The ideal candidate will also serve as a mentor to other data engineers, investing in the team’s development together. This position is a hands-on engineering role, with the core focus being on developing and deploying production-grade code.

#LI-Remote
Responsibilities
Takes a hands-on role at piloting and developing tools in addition to enhancing existing platforms that power Gopuff’s data teams
Architect and implement large-scale data processing systems that enable analytics, data science, and machine learning in a multi-cloud environment
Develop best practices for data collection, storage, and processing that impact company-wide data strategy across Gopuff’s data lakes and data warehouses
Partner with software and analytics engineering teams to establish data contracts to improve data quality at every stage of the data lifecycle
Participate in design and architectural review sessions with data engineers and software engineering partners
Conduct code reviews and knowledge-sharing sessions across data engineering and partner teams
Collaborate with engineering and product leadership to translate business requirements into technical solutions
Partner with engineering teams to model foundational event schemas
Qualifications
8+ years of experience in a data engineering role building end-to-end ETL/ELT pipelines
Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow
Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc.
Experience with data warehouses, data lakes, and their underlying infrastructure
Proficiency in Python, SQL, RESTful API development
Experience with cloud computing platforms such as Azure, AWS
Experience data observability and monitoring tooling such as Monte Carlo, Great Expectations, SodaSQL, Databand, etc.
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data governance, schema design, and schema evolution
Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage
Experience with Infrastructure as code tools such as Terraform
Compensation:
Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what we’d reasonably expect to pay candidates. A candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this role’s compensation package, please reach out to the designated recruiter for this role.
Remote - Salary Range (varies based on a cost of labor index for geographic area within United States): USD $152,000 - USD $241,500
Benefits
We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*

We also offer*:
Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.

At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.

And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.

Like what you’re hearing? Then join us on Team Blue.

Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show Less
Report",$1L - $2L,5001 to 10000 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2013,Unknown / Non-Applicable
#N/A,Data Engineer - Remote,"Phoenix, AZ","At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us and start doing *your life's best work.(sm)*
**
You’ll enjoy the flexibility to work remotely from anywhere within the U.S. as you take on some tough challenges.*
Primary Responsibilities:*
*
Accountable for data engineering lifecycle including research, proof of concepts, design, development, test, deployment and maintenance
Design, develop, implement and run cross-domain, modular, optimized, flexible, scalable, secure, reliable and quality data solutions that transform data for meaningful analyses and analytics while ensuring operability
Design, develop, implement and run data solutions that improve data efficiency, reliability and quality, and are performant by design
Layer in instrumentation in the development process so that data pipelines can be monitored. Measurements are used to detect internal problems before they result into user visible outages or data quality issues
Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues
Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking
Tech debt reduction/ Tech transformation including Open-source adoption, Cloud adoption, HCP assessment and adoption
Contribution to our industry community and strive to reuse and share components wherever possible across the organization
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Identifies solutions to non-standard requests and problems
Solves moderately complex problems and/or conducts moderately complex analyses
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.*
*Required Qualifications:*
*
Undergraduate degree or equivalent experience
3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python
3+ years of experience in Data Engineering, Coding ETL and building data pipelines
1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc.
1+ years of experience writing data engineering code in Databricks
Preferred Qualifications:*
*
Cloud experience (Azure/AWS/GCP)
Snowflake experience
Proficient in building relationship with stakeholder and maintaining it during the course of the project/program
Proficient in working with cross functional teams
Careers with UnitedHealthcare. Work with a Fortune 5 organization that’s serving millions of people as we transform health care with bold ideas. Bring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best. As an industry leader, our commitment to improving lives is second to none.*
*All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.
California, Colorado, Connecticut, Nevada, New York City, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New York City, or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Location: Phoenix,AZ 85002,USA, Phoenix, AZ 85002
Job Type: Full-time
Application Question(s):
Do you have a Bachelor’s Degree or equivalent underwriting work experience?
Do you have 3+ years of hands on experience writing code in developing Big Data solutions using Spark + Scala/Python?
Do you have 3+ years of experience in Data Engineering, Coding ETL and buidling data pipelines?
Do you have 1+ years of experience with CICD tools such as Jenkins, GitHub, Maven etc. ?
Do you have 1+ years of experience writing data engineering code in Databricks?
Start your job application: click Easy Apply
Show Less
Report",$73T - $1L,10000+ Employees,Company - Public,Healthcare,Healthcare Services & Hospitals,1977,$10+ billion (USD)
Amazon,Data Engineer - Flink,"Austin, TX","The successful candidate in this role will have:
Experience building and maintaining enterprise-scale (Terabyte - Exabyte) data pipelines.
Experience using modern open-source technologies and cloud services (SNS, SQS, MSK, ECS, EC2, DynamoDB, Kinesis, EMR, Kafka, Flink, Spark)
Experience with modern compression technologies (Orc, Spark)
Experience working with customers to model and onboard datasets to fit customer requirements.
Experience developing data pipeline parsers using Scala.
To follow up with any questions, please contact Ajitabh at # 408-907-2956
Job Type: Contract
Pay: $72.00 - $80.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
LinkedIn profile is required. Please share your LinkedIn profile
Experience:
Apache Flink: 1 year (Required)
Scala: 2 years (Required)
Parsing of Data: 2 years (Required)
AWS tools: 2 years (Required)
Work Location: One location
Show Less
Report",$72.00 - $80.00 Per hour,10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994,$10+ billion (USD)
Zillion Technologies,Sr. Data Engineer (ETL development),"Richmond, VA","Job Title: Sr. Data Engineer - Hybrid
Location: Richmond Virginia 23060
JOB DESCRIPTION
Position Description
The Senior Data Engineer is a senior technical role in supporting the information management architecture of the Enterprise Data Warehouse solution. The role will be actively responsible for designing the data acquisition, data staging, loading, and transformation into the Enterprise Data Warehouse. This role will be a technical expert and resource collaborating with the Data Architect, Software Engineers, Product Owners, and Project Team to develop and deliver data storage and movement solutions and to organize and oversee the loading of data into the related systems. Additionally, the Senior Data Engineer will bridge gaps related to Business Intelligence functions, supporting the analytics produced by the organization, and providing expertise tying data movement together with data consumption.
Position Accountabilities:
Design and develop complex ETL solutions using data warehouse design best practices
Analyze data requirements, data models, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projects
Create complex business intelligence reports and data visualizations using tools like Python, Tableau, and PowerBI
Analyze business requirements and outline solutions
Validate code against business and architectural requirements
Create and test prototypes
Troubleshoot applications and resolve defects
Work within an agile framework
Plan, Prioritize and Deliver Resilient, Scalable technical solutions
Communicate ideas in both technical and user friendly language
Update and maintain product documentation
Escalate issues and impediments in a timely manner
Work within established framework and processes (Agile)
Collaboratively work with Agile teams, as well as independently
Perform and coordinate unit and system integration testing when required
Participate in peer programming, mobbing, hackathons, and code reviews as required
Support and occasionally lead business intelligence efforts, data analytics efforts, and data governance/quality efforts.
Position Qualifications:
Education & Experience:
A Bachelor’s Degree or a combination of equivalent work experience
7+ years of previous experience in information technology, preferably within the financial services or other highly-regulated industry
5+ years ETL development experience
3+ years of Business Intelligence, Data Analytics, or Data Science experience
3+ years of experience in an Agile environment
Knowledge of ETL and data warehouse design
Experience using Python for data movement/manipulation
Extensive experience with data dictionaries, data analysis and relational databases
Experience with a business intelligence toolset
Preferred Qualifications:
A Master’s Degree in a technology area of study; preferably in Computer Science, MIS or Analytics.
Knowledge & Skills:
Creative problem solver with excellent communication, leadership and collaboration skills.
Advanced decision making and problem solving skills
Business acumen, knowledge and professionalism
Proactive leadership style and a self-starter and strong attention to detail
A clear and familiar understanding of the concepts and best practices of data warehouse ETL design
Demonstrable expertise with RDBMS or Big Data technology
Foundational Database administration skills; Broad DW skills including Semantic Layer, Basic BI Development and basic data modeling & data analysis
Methodical and structured approach to design and development.
Delivery focused with the ability to prioritize activities.
Flexible team player within a small, multi-disciplinary team.
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Richmond, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
ETL development: 5 years (Preferred)
Business Intelligence, Data Analytics, or Data Science: 3 years (Preferred)
Agile environment: 3 years (Preferred)
Work Location: One location
Show Less
Report",$70.00 - $80.00 Per hour,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,$5 to $25 million (USD)
Numentica LLC,AWS Principal Data Engineer,"Lehi, UT","What you will do:
Be responsible for leading the data engineering, analysis, architecture, design, and development of cloud database solutions
Be a technical team member who designs and develops data platform solutions
Uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g. data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc
Be responsible for moving, integrating, and cleansing data
Work on highly collaborative agile teams, can break down their work from stories into tasks, identify dependencies, and test and confirm acceptance criteria of work
As a Principal in our business you will:Be a self-starter, effective in breaking down large problems into smaller ones, and eager to regularly share what you learn with others within your projects and in the broader team
Supports people processes in capability / account (e.g., talent acquisition, onboarding, staffing, performance management)
Expand trusted relationships beyond primary customers and teams growing and managing professional network
What you should have:
7+ years of experience in leading data engineering work with minimum 3 years of experience as Lead / Principal Architect role for designing database systems for transactional and analytical workload
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena
Experience extracting/querying/joining large data sets at scale.
Selecting the appropriate AWS service based on data, database, or security requirements
Identifying appropriate use of AWS architectural best practices
Estimating AWS costs and identifying cost control mechanisms
Build and design secured reference architecture for all in-cloud and hybrid environments. Minimum Job Requirement:
Must be familiar working with CI tool like Jenkins.
Must have good knowledge of code deployment.
Should have solid experience using AWS core services: EC2, S3, Redshift, etc
Knowledge of SAP database systems is big plus
Must have knowledge on bigdata eco system leveraging AWS services
Must have working knowledge on AWS Glue and spark
Familiarity with other technologies such as Data Virtualization Services, Okta, Sagemaker, API Gateways, Kubernetes, Dockerization and microservices
Preferred Job Requirement:
Familiarity with open search
Familiarity with Devops
Knowledge on delta lake
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Lehi, UT: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 1 year (Preferred)
Work Location: One location
Show Less
Report",$85T - $1L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,Unknown / Non-Applicable
Barracuda Networks Inc.,Data Engineer,"Chelmsford, MA","Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote
Show Less
Report",$86T - $1L,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD)
FocuzMindz,"AWS Data Architect/ Engineer with Redshift, RDS","Alexandria, VA","We are seeking an AWS Data Engineer to join our growing team.
The qualified applicant will play a key role in the data warehouse migration as part of the Enterprise Data Analytic Services program at a federal agency located in Alexandria, VA. Hybrid work options are available.
AWS Data Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premises to cloud environment, evaluation, and optimization.
Full time opportunity
Alexandria VA
Hybrid role -onsite 2 days per week
Responsibilities:
Work on automating migration process for RDS and RedShift scripts in AWS from Dev to Production.
Performance tune RDS/RedShift SQL queries.
Maintain/resize the clusters for RDS and RedShift.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including computing, storage networking, database, management tools, security, identity, and compliance.
Good knowledge of RDS Postgres and AWS Redshift.
5+ years of experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Experience building infrastructure inside of AWS via code. Familiarity with tools such as Terraform or CloudFormation.
5+ years of experience architecting/deploying/operating solutions built on AWS.
Experience using ETL tools such as Alteryx, Snap Logic, Matillion, SAS DI Studio, Informatica, or equivalent tools.
Education Requirements:
Bachelor's degree in engineering, data science, computer science,
AWS and/or Data Science certification is a plus!
Clearance Requirements:
Ability to obtain and hold a Public Trust Clearance.
mary.a@stepstalent.com
Job Type: Contract
Pay: From $159,583.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Alexandria, VA 22301: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 9 years (Required)
AWS: 9 years (Required)
Security clearance:
Confidential (Required)
Work Location: Hybrid remote in Alexandria, VA 22301
Show Less
Report",$2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Gridiron IT,Data Engineer,"Washington, DC","Seeking a Data Engineer local to Washington, DC.
Active Top Secret/SCI Clearance Required
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Job Types: Full-time, Contract
Pay: $65.00 - $75.00 per hour
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 3 years (Preferred)
AWS: 2 years (Preferred)
ETL: 3 years (Preferred)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$65.00 - $75.00 Per hour,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Glow Networks,Data Engineer,"Dallas, TX","Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show Less
Report",$73.00 Per hour,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
Pendrick Capital Partners,Data Engineer,Remote,"Job Description
Company Overview
Pendrick Capital Partners is a leader in helping U.S. healthcare providers manage their receivables. With a core belief of practicing a patient-first mindset, Pendrick is the best-in-class revenue cycle management partner with over 10-years of experience purchasing outstanding receivables. Pendrick’s recognized compliance program offers an unparalleled degree of risk reduction for our healthcare industry partners while increasing returns on patient responsibility balances.
As a Data Scientist at Pendrick Capital Partners, you’ll help us make better and faster decisions than ever before. We use the latest in cloud, analytical, and machine learning technologies to unlock big opportunities for the company’s executives. We have big goals for the next few years, and we could use your help to design, architect, and implement solutions that meet our growing needs for rapid and cutting-edge analytics and forecasting.
This role is for you if:
You have built machine learning models through all phases of development, from design through training, evaluation, validation, and implementation and can explain your decisions in a simple and concise way to non-technical experts,
You know how to strike the right balance between sharing your expertise and listening to others’ ideas, and
You love to learn how to apply cutting-edge technologies in a way that drives value for business decisions and can leverage several technologies and languages — SQL, R,
AWS, Spark, and more — to reveal the insights hidden within huge volumes of transaction data,
The Ideal Candidate is:
A big data wrangler. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures, preferably using Spark and other open source technologies.
Technical. You’ve worked with open-source languages, you know how to develop reusable code, and you are passionate about continuing to improve. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.
Statistically-minded. You’ve built models, validated them, and monitored them post- deployment. You know how to interpret a ROC curve and partial dependence plots. You have experience with multivariate linear and nonlinear models as well as unsupervised approaches including clustering, classification, and anomaly detection.
Forward-thinking. You know how to promote a culture of technical excellence and look for opportunities to reuse robust, resilient solutions wherever possible.
Basic Qualifications:
Bachelor’s Degree plus 2 years of experience in data analytics in the workplace, or
Master’s Degree plus 1 year in data analytics in the workplace, or PhD
At least 1 year of experience in open source programming languages for large-scale data analysis (preferably R)
At least 1 year of experience with machine learning
At least 1 year of experience with relational databases
Languages: Python & SQL required. C++ and R
Preferred Qualifications:
Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics) plus 3 years of experience in data analytics, or Ph.D. in “STEM” field (Science,
Technology, Engineering, or Mathematics)
At least 1 year working in financial, healthcare, or collections services
At least 1 year of experience working with AWS
At least 2 years experience in Spark/Databricks/Scala or R
At least 2 years experience with machine learning
At least 3 years experience with SQL
Git, Docker, Serverless, Lambda, ECS, AWS CLI, Boto3
Experience with consumer finance data is a plus
For more information about Pendrick Capital Partners, please visit our website at https://www.pendrickcp.com/
Job Type: Full-time
Pay: $100,000.00 - $170,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Vision insurance
Compensation package:
Performance bonus
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
How many years of relational database experience do you have?
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
C++: 1 year (Preferred)
Work Location: Remote
Show Less
Report",$1L - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Vizva Technologies,Senior Azure Data Bricks Engineer,"Jersey City, NJ","Roles & Responsibilities
AREAS OF RESPONSIBILITY
· As a Data Engineer, you will work with multiple teams to deliver solutions on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies). In addition to building the next generation of application data platforms (not infrastructure) and/or improving recent implementations. Note: This is a data engineer from the application side. Must be able to analyze data and develop strategies for populating data lakes. This is not an infrastructure position. This person may be called upon to do complex coding using U-SQL, Scala or Python and T-SQL.
·
Generic Managerial Skills
Professional Skill Requirements
Work as part of a team to develop Cloud Data and Analytics solutions
Participate in development of cloud data warehouses, data as a service, business intelligence solutions
Data wrangling of heterogeneous data
Ability to provide solutions that are forward-thinking in data and analytics
Deliver a quality product
Coding complex U-SQL, Spark (Scala or Python). T-SQL
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)
Skills/Qualifications
Expertise in Any ETL tool i.e. (SSIS, Informatica, Data Stage)
Expertise to Implementing Data warehousing Solutions
experience as Data Engineer in Azure Big Data Environment
Programming experience in Scala or Python, SQL
Hands-on experience in Azure stack (Azure Data Lake, Azure Data Factory, Azure Databricks) -- Mandatory
Good understanding of other Azure services like Azure Data Lake Analytics & U-SQL, Azure SQL DW
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance & scale
Demonstrated analytical and problem-solving skills, particularly those that apply to a big data environment
Good Understanding of Modern Data Warehouse/Lambda Architecture, Data warehousing concepts
Proficient in a source code control system such as GIT
Knowledge of C#
Ability to code
Ability to use word to create required technical documentation
Soft Skills
Excellent written and verbal skills (English)
Flexible
Self-Starter
Team Player or individual contributor
Job Type: Contract
Pay: $52.06 - $69.28 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Bricks Engineer: 10 years (Preferred)
like Azure Data Lake Analytics & U-SQL, Azure SQL DW: 10 years (Preferred)
Scala or Python, SQL: 10 years (Preferred)
Work Location: In person
Show Less
Report",$52.06 - $69.28 Per hour,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Jconnect Infotech Inc.,Sr. Data Engineer,"Edison, NJ","Position – Senior Data Engineer
Location – Edison, NJ
Duration – Contract C2C/W2
Job Description:
Big Data (spark/kafka)
PL/SQL
Druid
GKE (Google Kubernetes Engine)
Java development experience – not into coding
Take Druid ingestion and check if everything is going well.
How queries are behaving in prod, optimize it.
Job Type: Contract
Pay: $43.82 - $66.67 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
Druid: 1 year (Required)
SQL: 5 years (Required)
Big data: 4 years (Required)
Work Location: One location
Show Less
Report",$43.82 - $66.67 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
ASA,Data Integration Engineer(Banking),"New York, NY","Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.
Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
SKILLS
10+ years - Enterprise Data Management
10+ years - SQL Server based development of large datasets
5+ years with Data Architecture
3+ years experience in Finance / Banking industry some understanding of Securities and
Banking products and their data footprints.
2+ years Python coding experience
Proficient with Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
Capable of discussing enterprise level services independent of technology stack
Experience with Cloud based data architectures, messaging, analytics
Superior communication skills
Cloud certification(s)
Note
Any experience with Regulatory Reporting is a Plus
Job Type: Contract
Salary: $100.00 - $103.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10020: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL Server: 10 years (Preferred)
python: 2 years (Preferred)
Data management: 10 years (Preferred)
Work Location: One location
Show Less
Report",$100.00 - $103.00 Per hour,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1999,$25 to $50 million (USD)
Publix,Senior Software Engineer – Data Analytics,"Lakeland, FL","Participate in Enterprise Data Warehouse and business application team projects with a focus on gathering requirements, design, development, and implementation of cloud solutions
Work with Data Asset Management team and business users to gather detailed analytics requirements
Analyze the data sources, design, and develop Databricks pipelines to ingest the data
Collaborate with the EDW team and Architecture team to structure the new data within Azure Data Lake
Collaborate with Data Asset Management team to maintain metadata


Required Qualifications:
Bachelor’s degree in Computer Science or other analytical discipline or equivalent experience
Minimum 5 years of experience designing, developing, and supporting applications in an enterprise environment
Minimum one year of experience using Azure cloud computing technologies
Minimum one year of hands-on experience using Databricks, ingesting data using pipelines, and loading delta tables
Experience with Apache Spark and Python
Experience working in a Data Lake environment handling structured and unstructured data
Experience working in a fast-paced, innovative environment
Attention to detail with the ability to produce reliable, effective solutions
Excellent communication skills
Possessing a positive attitude and ability to work in a collaborative and energetic team environment
Address: 321 S. Kentucky Ave
Show Less
Report",$1L - $2L,10000+ Employees,Company - Private,Retail & Wholesale,Grocery Stores,1930,$10+ billion (USD)
Violet Ink,Data Engineer,"Newark, NJ","Key Job Responsibilities
· Analyze data needs and objectives within the broader journey.
· Source, analyze and organize raw data, prepare data for transformation and consumption.
· Identify ways to improve data governance, reliability, efficiency, and quality.
· Build applications ensuring that the code follows latest coding practices and industry standards.
· Build using modern design patterns and architectural principles.
· Ensure developed solutions remain compliant with all applicable Prudential standards.
· Solve complex problems and provides new perspective on existing problems.
· Develop through collaboration and deliver application component solutions.
· Develop high quality, well documented, and efficient code supporting testing and automation.
· Support product owner in defining future stories and tech lead in defining technical designs.
Competencies – Knowledge, Skills, Abilities
Candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Should have experience using following software/tools:
Big data tools
Relational and NoSQL databases
Data pipeline and workflow management tools
AWS cloud services
Stream processing systems
Object oriented and scripting language
Build processes supporting data transformation, data structure, metadata, dependency, and workload management.
Successful history of manipulating, processing, and extracting value from large, disconnected structured and unstructured datasets.
Advanced working SQL knowledge and experience working with relational databases.
Experience building and optimizing data pipelines, architecture, and data sets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Strong project management and organization skills.
Experience supporting and working with agile cross functional teams in a dynamic environment
Background in financial services functions strongly desirable.
Job Type: Contract
Pay: From $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newark, NJ 07107: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
No SQL: 1 year (Required)
Work Location: Hybrid remote in Newark, NJ 07107
Show Less
Report",$60.00 Per hour,1 to 50 Employees,Company - Public,Information Technology,Information Technology Support Services,2007,Unknown / Non-Applicable
Tekrek solutions Inc,Azure Data Engineer,Remote,"Position: Azure Data Engineer
Location: Remote
Duration: 06+ 12 months + Extendable
Job Description:
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable of supporting in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Job Type: Contract
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Azure: 1 year (Required)
Synapse: 1 year (Required)
Pyspark: 1 year (Required)
Data modeling: 1 year (Required)
data design: 1 year (Required)
Work Location: Remote
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Umanist Staffing,Senior Data Engineer,"Bethesda, MD","Job Tittle - Senior Data Engineer
Work Type - Remote
Location - Bethesda, MD, US
Job Type - Full Time
Mandatory Skills –
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Antiunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Job Description –
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Job Type: Full-time
Salary: $100,000.00 - $140,000.00 per year
Benefits:
Health insurance
Life insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Are you comfortable on W2?
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 8707036327
Show Less
Report",$1L - $1L,1 to 50 Employees,Company - Private,Human Resources & Staffing,"Staffing, Recruitment & Subcontracting",2022,Unknown / Non-Applicable
Khayainfotech LLC,Sr. Data Engineer,"O Fallon, MO","Job Title: Sr. Data Engineer ( 12+ Years is a must)
Duration: Long Term Contract.
Location: St Louis, MO ( In Person 2 days Preferred, Remote Okay if candidate is exceptional)
Must Have : Strong in Scala and Spark
12+ Years experience is a must
As a Senior Data Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Spark, R, etc.
Your Role
Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering
Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information by auto-tagging images and text-to-speech conversions.
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
Participate in the development of data and analytic infrastructure for product development
Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations
Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve
Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision
Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders
Evangelize releases to users, incorporating feedback, and tracking usage to inform future development
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Work with small, cross-functional teams to define the vision, establish team culture and processes
Consistently focus on key drivers of organization value and prioritize operational activities accordingly
Escalate technical errors or bugs detected in project work
Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing.
Ideal Candidate Qualifications
Superior academic record at a leading national university in Computer Science, Data Science, Computer Engineering, Technology, or a related field or equivalent work experience
Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
At least 5 years of experience as a data engineer or machine learning engineer and with open-source tools
Prior experience in working in product development/management role
Experience in building and deploying production level data driven applications and data processing workflows/pipelines
Experience with application development frameworks (Java/Scala, Spring)
Experience with data processing and storage frameworks like Hadoop, Spark, Kafka
Experience implementing REST services with support for JSON, XML and other formats
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Experience of working in Agile teams
Good analytical skills required for writing and performance tuning complex SQL queries, debugging production issues, providing root cause, and implementing mitigation plan
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
Strong project management skills
Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting
Curiosity, creativity, and excitement for technology and innovation
Demonstrated quantitative and problem-solving abilities
Ability to multi-task and strong attention to detail
Motivation, flexibility, self-direction, and desire to thrive on small project teams
Good communication skills - both verbal and written – and strong relationship, collaboration skills, and organizational skills
The following skills will be considered as a plus
Financial Institution or a Payments experience a plus
Batch processing and workflow tools such as NiFi
Experience in developing integrated cloud applications with services like Azure, Databricks, AWS or GCP
Experience in managing/working in Agile teams
Experience developing and configuring dashboards
Job Types: Full-time, Contract
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",$80.00 - $95.00 Per hour,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2021,$1 to $5 million (USD)
ComResource,Senior Data Engineer,"Columbus, OH","ComResource is looking for a Senior Data Engineer.

The position plays a key role in developing and maintaining enterprise analytics deliverables, including but not limited to operational data stores, data integrations, and reports. The ideal candidate will be working in our mixed technology environment to deliver data products providing decision support for businesses and customers. As part of a highly collaborative team, the role will interact with technical and business resources within and outside of IT organization. The ideal candidate is a committed, creative, self-motivated, and passionate technologist who is interested in practicing current skills and learning new ones.

Responsibilities:
Partner with Business Stakeholders, Business Analysts, Data Engineers, Developers to design enterprise data warehouse components
Provide estimations, schedules, and regular and timely updates to project managers & senior management as needed
Validate proposed design for accuracy and completeness of business use cases
Develop data integration and transformation solutions to meet the input needs of the models
Develop and support batch jobs
Perform unit & regression testing
Perform code/peer reviews to ensure adherence to established design & development standards
Collaborate with development and quality assurance teams for testing and product quality improvements as needed
Produce deployment scripts, checklists, playbook & operations runbook in accordance with SDLC & change management requirements
Take measures to ensure adherence to committed service level agreements
Monitor the scheduled jobs & performance of the platform for smooth operation
Independently and with support from other developers, troubleshoot and fix issues that arise with data and/or processes
Essentials:
Bachelor’s degree in related field (prefer CS major)
10+ years of software development experience
5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS
5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization concepts
3+ years of experience in Azure using Data Factory, Databricks & ADLS
Experience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniques
Familiarity with SDLC and agile methodologies
Experience in source control tools such as TFS or Git
Experience in communicating with users, other technical teams, and management to collect requirements, identify tasks, provide estimates, and meet production deadlines
Experience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations
Understand and work in an Agile development environment
Desired:
Experience in designing & building BI Reporting solutions, preferably using Power BI
System and networking fundamentals
Knowledge/experience in Education or Aviation industry
Show Less
Report",$95T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$25 to $50 million (USD)
Avaap,Data & Analytics - Data Engineer,"Columbus, OH","Data & Analytics – Data Engineer

Avaap is looking for a Data Engineer; someone that has a deep appreciation for all things data and has the experience and skills to use data to drive tangible value. You may come from a traditional business intelligence background, or your experience may be fully immersed in the modern analytics landscape; either way, you hold a vast level of experience with key data engineering principles, techniques, tools and methodologies.

Technical Solutioning – you have the depth and skill to fully own key components/workstreams related to the conceptual development of complex technical solutions from design through deployment and operations. As a Data Engineer, you are versed in fully understanding the big picture when it comes to data engineering/data solutioning and have a keen eye for details to design, develop and deploy every component that you have been assigned. While you have strong articulation skills to describe a technical solution and can help communicate its key features and capabilities to others with ease, you prioritize your contributions by example by rolling up your sleeves and doing hands on development using a variety technologies, tools, and techniques.

Project Delivery – you have the experience to understand and appreciate that no matter how cool a technical data solution is, it is worthless if it never gets built and delivered correctly. As a Data Engineer, you are focused on developing strong work plans that align to the overall delivery approach for your team to design, develop and deploy a technical data solution. You understand the value of a work break down structure and have 10+ years of experience in developing project delivery plans related to the design and development of key pieces to large and complex data solutions. You see the value of project management techniques in whatever combination of waterfall, agile and/or a hybrid approach and can develop and execute upon project delivery plans. Your communication skills and experiences as a delivery leader are critical and you make sure to keep everyone from individual contributors on your team to your project leaders, and clients in the loop about progress, with an emphasis on communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.

People Management – let us be real, not even the smartest and most talented Data Engineer can do it by her/himself; everyone needs a team and Avaap prides itself on a team first culture. You have 10+ years of experience leading teams of consultants (and sometimes client resources) through complex and transformative delivery efforts on the workstreams you will manage. Your experience as a Data Engineer is to be a leader for your workstream and you bring the requisite people skills that establish a healthy and respectful culture on your projects and for your teammates. As a Data Engineer, you embrace being positioned as a mentor for many junior resources that may be on your projects. You positively influence less experienced, junior resources to support not only their project contributions, but also support their professional development/career roles by providing them key insights from your own working experiences.

Desired Experiences and Skills

Academic studies or equivalent experience related to Computer Science, Engineering, Technical Science with 5+ years of experience in programming and building large scale data/analytics solutions operating in production environments.
Experience in a variety of Cloud platforms, most specifically AWS, Azure, and/or Google
You have experience in Big Data/analytics/information analysis/database management/ event-driven/microservices/DevOps/ML Ops in the cloud
Deep fluency and skills with SQL.
Strong, hands-on experiences with the following data engineering technologies and languages:
Python / R / SaS / Scala / Go
Experience in distributed data computing framework such as Spark, MapReduce
Minimum Qualifications

Must have excellent verbal and written communication skills along with the ability to communicate effectively
Must be able to perform work indoors and remain stationary at a computer
Ability to work in a fast-paced and deadline-oriented environment
Passion for exceptional customer service and collaboration
Ability to work remotely or out of one of Avaap’s physical office locations
Current permanent U.S. work authorization required
Show Less
Report",$90T - $1L,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
Route,Sr. Data Engineer (Utah),"Lehi, UT","We are Route
Buying stuff online can get messy once you hit that ""order"" button. Managing dozens carrier tracking links, dealing with lost or damaged packages, and resolving issues with customer support can feel like a wild goose chase. That's why we created Route — to make the post-purchase experience for consumers like you, and the brands you love, as seamless as possible.
Route is on a mission to connect the world's commerce. Through our network of +5 million Route App users and 15,000 merchants, we're making it easier than ever for consumers to track, insure, and discover their favorite products in one place — which connects the world's coolest direct-to-consumer brands to happy repeat customers.
Since Route launched in 2018, we've been on a journey to build innovative products that empower our customers, all while fostering a people-first, values-driven company culture. We're looking for talented people across the ecommerce space to join us on the next steps of this adventure.
Don't just take our word for it! Discover what life at Route has to offer.
The Team
Route's Data Engineering team is at the cornerstone of the organization mission to mature the self-service/GitOps mantra for our processes with tools to scale and build value enhancing insights and opportunities for our most priceless asset, our data. This team has the amazing opportunity to create and innovate with an immediate impact with some of the most cutting-edge solutions in the market, as well as, embracing the open-source community, whichever we believe continues us on our exciting journey. As a member of our team, you will be working to build a best-in-class data platform, with some of the brightest engineers, to provide analytics to our customers and merchants, while developing internal software solutions for our teams.

The Opportunity
We are looking for a senior backend software engineer to join our team, focused on building a large distributed data workflow platform. We want a master builder that has an affinity for problem-solving and values the typical services' SLA and SLO expectations. You love developing with large scale orchestration, scheduling, distributed services, or even building out development tools for our engineers, data scientists, and business analysts as their audience.
We value team over ego and are passionate about simplicity, self-service, automation, and of course, data! We are traditionally a Golang shop developing in ECS in AWS, and currently playing with a plethora of tools in our stack, such as Airflow, DBT, Databricks, Snowflake and Tableau.
What you'll do
Design and optimize data structures in our data management system (i.e. S3, Snowflake, DBT, Databricks)
Drive efficiencies and reliability through design, automation, observability, and performance testing
Build and maintain scalable self-service solutions to our pipelines in our data ecosystem
Partner with Business analysts to understand their data challenges and crafting scalable solutions with Product alignment
Serve as a pragmatic data stewards as we iterate through evolving our data catalog while strengthening our data retention strategy and adoption
Contributing towards our documentation, runbook and architecture diagrams to keep them up to date as the frameworks evolve
What we are looking for
5+ years of relevant experience
Bachelor's degree in Engineering, Computer Science, Information Systems or related field, preferred
Experience in Data Modeling, Data architecture, Data Quality, ETL and Data Warehouse methodologies and technologies.
Experience with any combination of the following technologies: Databricks, Snowflake, Tableau, DBT, Airflow
3+ years with experience in one of the following: Python, Go, Spark, Scala
3+ years experience in working in AWS, Gitlab, Terraform
3+ years in SQL
1+ year designing, building and managing ETL Pipelines
Practical understanding of designing and building a Data Lake architecture
Pay Transparency
Salary for this role: $164,000 - $177,000 DOE
The cash compensation above includes base salary, and is not reflective of potential commission for employees in eligible roles, or annual bonus targets under Route's bonus plan for eligible roles. In addition to cash compensation, all Route employees are eligible to participate in Routes equity incentive plan to receive stock options per the terms of the agreement. Some roles may also be eligible for overtime pay. Individual compensation packages are based on a few different factors unique to each candidate, including their career level, skills, experience, specific geographic location qualifications and other job-related reasons.
Total Rewards:
We know our team works best when everyone feels happy, healthy, and supported. We offer to pay 100% of your health insurance premiums on a $0 deductible plan for you and your family, remote or hybrid work arrangements, unlimited PTO, 401k matching, formalized growth opportunities, learning & development, DEI programs & events, and so much more.
Equal opportunity for all:
Route is an Equal Opportunity Employer. We embrace diversity and equal opportunity in a serious way. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be.
Show Less
Report",$2L - $2L,201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,2018,Unknown / Non-Applicable
"Bluemont Technology & Research, Inc.",Data Engineer,"Norfolk, VA","NATO Data Engineer
Requirements:
Ts/sci or secret clearance
High proficiency level in English language
A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake
Ability to work with large datasets is required.
Description:
Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.
Job Type: Full-time
Pay: $90,000.00 - $130,000.00 per year
Experience level:
10 years
11+ years
4 years
5 years
6 years
7 years
8 years
9 years
Ability to commute/relocate:
Norfolk, VA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you have a secret clearance or TS/SCI?
Work Location: One location
Show Less
Report",$90T - $1L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
YT Global Network,Data Engineer- Remote,Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote
Show Less
Report",$90.00 - $120.00 Per hour,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
DataPattern,Sr. Data Engineer,"Los Angeles, CA","Responsibilities
● Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science
● Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
● Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala
● Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
● Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
● Maintain detailed documentation of your work and changes to support data quality and data governance
● Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
● Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Basic Qualifications
● 6+ years of data engineering experience developing large data pipelines
● String Python programming skills
● Strong SQL skills and ability to create queries to extract data and build performant datasets
● Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Preferred Qualifications
● Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
● Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
● Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
● Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
● Good Scripting skills, including Bash scripting and Python
● Familiar with Scrum and Agile methodologies
● You are a problem solver with strong attention to detail and excellent analytical and communication skills
Job Type: Full-time
Salary: $65.00 - $75.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: On the road
Speak with the employer
+91 9256270467
Show Less
Report",$65.00 - $75.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Gridiron IT,Data/ETL Engineer,"Springfield, VA","GridIron IT is seeking an ETL Engineer with an active Top Secret/SCI Eligible Clearance.
MUST BE LOCAL TO DC METRO AREA
In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.
RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies
Gridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.
Gridiron IT is a Women Owned Small Business (WOSB) company specializing in IT Infrastructure, Cyber & Cloud Security, Software Development, and Enterprise Support. Gridiron is an Inc. 5000 2022 recipient and Washington Business Journal Fastest Growing Companies in the Greater Washington Area for 2022. Gridiron offers a competitive benefits package to include medical, dental, vision, 401(k), life insurance, disability insurance, and pet insurance.
Job Types: Full-time, Contract
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Overtime
Experience:
ETL: 5 years (Required)
AWS: 5 years (Required)
RDS: 4 years (Required)
Security clearance:
Top Secret (Required)
Work Location: In person
Show Less
Report",$1L - $2L,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable
Stratford Solutions Inc.,Senior Data Engineer,Remote,"Job title: Senior Data Engineer

Job Location: REMOTE (EST ZONE) M-F 9-5 (35/hrs a week)

Job type: 8 Month Contract

Pay Rate: $100-$125/hr

SCOPE OF SERVICES

Seeking a Data Engineer role to ensure the efficient and successful implementation and support of complex data engineering solutions for City agencies. This resource should demonstrate a solid understanding of industry-standard implementation methodologies using data engineering technologies, tools, and processes.

TASKS:
? Create and maintain optimal data pipeline architecture that is coherent and scalable, based on best practices of integrating data into a consolidated repository.

? Perform the technical design, development, and component testing of repository changes.

? Build analytics tools that utilize the data pipeline to provide actionable insights into customer engagement and experience, operational efficiency, and other key business performance metrics.

? Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, cloud, and big data technologies.

? Develop ETLs to move data securely from source to target systems.

? Create, update, and maintain system documentation.

? Develop new or build against existing APIs for data access or landing data as output for further downstream consumption in the appropriate target data store.

? Perform special projects and initiatives as assigned.

MANDATORY SKILLS/EXPERIENCE Note: Candidates who do not have the mandatory skills will not be considered

8+ years of experience in writing SQL.
8+ years of experience in copying, transferring, manipulating, and automating data operations that were manual processes.
Experience with tools and components of data architecture such as Informatica Power Center, IICS, SSIS, or similar ETL tools.
Experience working with Amazon Web Services or Microsoft Azure cloud computing platform and services.
In-depth knowledge of SQL and other database solutions.
Experience with data warehousing (Snowflake, Redshift etc.).
Knowledge of modeling database schemas for large datasets.
Experience developing cloud-ready applications.
Experience working with programming languages like Python, Java, and Perl
DESIRABLE SKILLS/EXPERIENCE:
Hands on experience developing Microsoft PowerBI solutions.
5+ years hands-on experience in development with the suite of tools from Informatica PowerCenter and B2B Data Transformation.
Experience using Oracle 10g/11g, SQL Server and/or a database appliance.
Knowledge of metadata-driven enterprise reporting platforms.
Show Less
Report",$100.00 - $125.00 Per hour,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Cat software service,"AWS Data Engineer at Miami, FL || Onsite role","Miami, FL","Job Title : Lead Data Migration AWS Engineer
Location : Miami, FL
Duration : 12 months contract
Job Description :
Lead Engineer with 10 years of hands-on experience in Data Migration transformation programs with Oracle/MySQL, PL/SQL, SQL queries in AWS
ETL tool experience is desirable.
Experience in Telecom Domain is a must.
Best Regards,
Sam wilson| IT Recruiter
CAT Software Services INC.
PH.NO: (+1) 848-300-0290
Job Type: Contract
Salary: $60.00 - $63.00 per hour
Schedule:
No weekends
Ability to commute/relocate:
Miami, FL 33122: Reliably commute or planning to relocate before starting work (Required)
Experience:
Telecom domain: 10 years (Required)
SQL: 2 years (Required)
Work Location: One location
Speak with the employer
+91 8483000290
Show Less
Report",$60.00 - $63.00 Per hour,501 to 1000 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Konnectingtree,Data Engineer,Remote,"Greetings from KonnectingTree!
We are looking for a Data Engineer for one of our clients. This is a remote position with an Implementation partner. AWS Certification Mandatory.
Data Engineer with AWS Experience
Experience with PySpark/Spark
Experience in Python
Able to work independently
Able to work with the business team directly
Interested candidates kindly share your updated resume with mythili.saravanan@konenctingtree.com. Please reach me at 952-679-2916.
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Required)
Python: 5 years (Required)
PySpark: 5 years (Required)
Work Location: Remote
Speak with the employer
+91 952-679-2916
Show Less
Report",$45.00 - $50.00 Per hour,1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
FlexIT Inc,Data Science Engineer,"Beaverton, OR","We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform
Show Less
Report",$83T - $1L,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Ascendion,Senior Data Engineer,Remote,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote
Show Less
Report",$60.00 - $70.00 Per hour,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
Titan Healthcare Management Solutions,Data Engineer,Remote,"Titan Health is currently hiring for a Data Engineer.
Under the general direction of the Technology Solutions Manager, the Data Engineer is responsible for implementation, configuration, maintenance, and performance of business-critical data infrastructure to deliver data enablement at scale, and power our revenue cycle applications. This role will work with enterprise and client leaders to translate business and functional requirements into technical specifications and solutions within the data architecture strategy.
Essential Job Duties/Responsibilities
Implement efficient and scalable pipelines integrating data from multiple sources to common data models.
Convert raw data into usable information for client and enterprise organizations.
Within an Agile team design, develop, test, implement, and support technical solutions that support full-stack development tools and technologies.
Support data science, data enrichment, research, and data analysis as well as making data operationally able to be consumed by products and services.
Collaborate with product managers, software engineers, security & compliance, and data scientists to enable them with robust data delivery solutions that drive powerful experiences.
Identify and debug issues with code and suggest changes and/or improvements.
Perform unit tests and conduct reviews with the team to ensure code is rigorously designed, elegantly coded, and effectively tuned for performance.
Utilize available technologies to collect and map data to find cost savings and optimization opportunities.
Support and drive a proactive culture of security and compliance.
Bring an agile and engineering mindset to address complex problems, identify opportunities and craft creative solutions.
Leverage best practice coding and engineering standards to support growth and flexibility.
Coordinate with teams across the organization to address incident, change and release management needs/requirements.
Provide input to risk management; report risks as they are identified and participate in prioritization/follow up.
Stay current with emerging technologies and advancements within existing technologies.
Positively and deliberately engage with colleagues – external and internal – to foster collaborative and productive relationships.
Cultivate great teams and lead in alignment with Titan values.
Comply with and hold with utmost regard all compliance requirements to protect patient privacy and confidentiality.
Stay curious, kind and contribute positively to the Titan culture.
Minimum Qualifications
Bachelor’s Degree in Information Technology, Computer Science, Mathematics or related field is preferred but not required.
Experience with Agile methodologies.
Experience working in HIPAA, HITRUST or other advanced compliance environments.
Experience leading the lifecycle management and integrations of enterprise data.
Ability to clearly articulate technology concepts to business leaders and engineers.
Strong understanding of IT Service Management practices.
Strong analytical, problem-solving, and critical thinking skills with excellent attention to detail.
Direct experience with Azure SQL.
Experience with Azure Data Factory preferred.
Prior experience in the facilitating conversations to translate business requirements into the technical requirements needed to develop solutions.
Excellent oral and written communication skills.
Comprehensive knowledge of Microsoft Office applications.
Knowledge of Hospital CMS or billing data structures preferred.
Titan Health offers a robust Health and Welfare benefits program, along with Paid Time Off, 401k plan with company match, and remote working environment.
H9CslyVeWp
Show Less
Report",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Findability Sciences,Snowflake Data Engineer pipeline,"Houston, TX","Snowflake data engineers will be responsible for architecting and implementing very large scale data intelligence solutions around Snowflake Data Warehouse.
A solid experience and understanding of architecting, designing and operationalization of large scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must.
Developing ETL pipelines in and out of data warehouse using combination of SQL and Snowflakes Snow SQL
Writing SQL queries against Snowflake.
Developing scripts Unix, Python etc. to do Extract, Load and Transform data
Provide production support for Data Warehouse issues such data load problems, transformation translation problems
Translate requirements for BI and Reporting to Database design and reporting design
Understanding data transformation and translation requirements and which tools to leverage to get the job done
Understanding data pipelines and modern ways of automating data pipeline using cloud based
Testing and clearly document implementations, so others can easily understand the requirements, implementation, and test conditions.
Job Types: Full-time, Part-time, Contract, Temporary
Salary: $42.14 - $70.29 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Night shift
Ability to commute/relocate:
Houston, TX 77001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 3 years (Preferred)
Work Location: One location
Show Less
Report",$42.14 - $70.29 Per hour,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Edrstaffing,Python Data Engineer,"Boston, MA","Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
Show Less
Report",$90T - $2L,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
